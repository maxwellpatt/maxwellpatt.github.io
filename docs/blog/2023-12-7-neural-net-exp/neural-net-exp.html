<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Maxwell Patterson">
<meta name="dcterms.date" content="2023-05-11">

<title>Expressivity of Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cooking.html" rel="" target="">
 <span class="menu-text">Cooking</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../basketball.html" rel="" target="">
 <span class="menu-text">Basketball</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Expressivity of Neural Networks</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Maxwell Patterson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 11, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#perceptron" id="toc-perceptron" class="nav-link" data-scroll-target="#perceptron">Perceptron</a></li>
  <li><a href="#activation-function-in-play" id="toc-activation-function-in-play" class="nav-link" data-scroll-target="#activation-function-in-play">Activation Function In Play</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>The rise of neural networks as a tool has led to many technological advancements that have added significant value to society. Things such as voice and image recognition, medical diagnoses, and targeted marketing are a few examples of concepts that have seen significant improvements in performance and applica- tion from the application of neural networks. AI and deep learning unlock a whole new realm of what is possible within mathematics, as the computer is able to learn to differentiate among different representations in data that can reveal important trends and observations from some data set. In order to accomplish this, neural networks calculate, with the use of input and output data, some sort of pattern that can be applied to these situations such as voice recognition and medical diagnoses. In this project, we are tasked with exploring the na- ture in which neural networks can be applied to the approximation of functions. Overall goals of the project include developing an understanding of the training process dynamics, the ways in which the depth and width of the neural network influence the approximation, challenges and takeaways from this investigation.</p>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<section id="perceptron" class="level3">
<h3 class="anchored" data-anchor-id="perceptron">Perceptron</h3>
<p>While modern neural networks can have millions of layers, the first neural net- work had, naturally, only one layer. In July of 1958, Frank Rosenblatt revealed his prized perceptron, which set the stage for which modern day AI was built upon. Rosenblatt coined the perceptron as being the ”first machine which is capable of having an original idea” (Cornell). The perceptron is a type of single-layer neural network that is inspired from the collaborative nature of how neurons work in the brain. It operated by labeling inputs in two ways, such as left or right, or man or woman. In case of an incorrect prediction, the al- gorithm adjusts itself to improve the accuracy of future predictions. After the completion of thousands or millions of iterations, the neural networks gets more precise over time in order to obtain some valuable result. Weights and bias play a critical role in establishing the relationship between inputs and outputs in the context of perceptrons. A perceptron computes the weighted sum of the input features, <span class="math display">\[
f(x) = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b = 0
\]</span></p>
<p>x1, x2, …, xn represent the inputs, w1, w2, …, wn represent the weights associated with these inputs, and b is the bias term. The weights and bias are the keys that drive the neural network, as they are flexible controls that establish the model and allow for optimal relationships to be understood during the learning process. Bias allows for the offsetting of any constant in the data, which enables the perceptron to generate more accurate result.</p>
<p>The result that the perceptron generates, known as the decision boundary, is calculated using the input data, the associated weights in the network, and the bias term. This is done through the linear combination of these factors. The decision boundary can be defined as the set of points x such that <span class="math display">\[
f(x) = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b = 0
\]</span></p>
<p>In this case, the sign of the function f(x) determines the significance of the output. For example, a positive value could be associated to the left direction and a negative value associated to the right direction. The flexibility of the per- ceptron is dictated by the weights and biases it possesses. The model is able to learn which input features are more important than others by assigning proper weight values to each input. Coupled with the bias term allow for certain con- stant to be offset in the data, the perceptron is able to pick up certain trends in the input data. Ultimately, the perceptron’s power lies in its ability to find an adequate decision boundary through the manipulation of weights and biases, highlighting how important these concepts are in the process of neural networks.</p>
</section>
<section id="activation-function-in-play" class="level3">
<h3 class="anchored" data-anchor-id="activation-function-in-play">Activation Function In Play</h3>
<p>Activation functions allow for neural networks to learn and understand compli- cated patterns between inputs and outputs by presenting the possibility of non- linearity into the network. Non-linear networks are much more powerful than linear networks as they are able to capture much deeper and more profound correlations in data. There are many usable activation functions today: the Sigmoid function, the tanh function, Reduced Linear Unit function, or ReLU, and LeakyReLU, and Exponential Linear Unit function, or ELU to name a few. The choice of activation function depends on the specific architecture of the network. It will be looked into later in the project as to how each of these activation functions is best suited for certain types of situations. Sigmoid : a smooth, S-shaped curve that maps input values to values in the range of 0 to 1. Mathematically, the function is defined as <span class="math display">\[
f(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>While the sigmoid function is sufficiently utilized in the outputs of binary sit- uations, it can struggle when dealing with more hidden layers due to vanishing gradient type of issues. In these erroneous scenarios, the gradient approaches zero or some large, positively or negatively, constant that hinders the learning ability of the network. tanh: smooth like the sigmoid function, but maps input values to the range of -1 to 1 instead of 0 to 1. Mathematically, the function can be defined as <span class="math display">\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</span></p>
<p>The tanh function is effective in that its output is centered around zero, which can help deal with the vanishing gradient issue to a degree but still deals with the issue of blow-up values.</p>
<p><em>ReLU/LeakyReLU</em> : a piecewise linear function that returns the input value if it is positive, and zero, or a small negative value associated the input, if the value is negative. Mathematically, the function is defined as <span class="math display">\[
\text{ReLU}(x) = \max(0, x)
\]</span></p>
<p>ReLU helps deal with the vanishing gradient issue since the gradient is constant for positive input values and cannot spiral in or out. The dying ReLU problem does exist however, in which the neurons deactivate for negative inputs which makes it difficult for the network to learn and adapt over time.</p>
<p><em>ELU</em>: variation of the ReLU function that smooths the curve to the left of the x-axis. Mathematically, the function can be defined as <span class="math display">\[
\text{ELU}(x) =
\begin{cases}
x &amp; \text{if } x &gt; 0 \\
\sigma \times (e^x - 1) &amp; \text{if } x \leq 0
\end{cases}
\]</span></p>
<p>Here, σ is some positive constant. ELU helps to address the vanishing gradient issue and upholds a stronger curve. The differences in application and results obtained using these approximation functions will be discussed later.</p>
</section>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">Backpropagation</h3>
<p>Backpropagation is an essential tool that is used in the training of neural net- works, especially ones that have a multitude of layers. Backpropagation calcu- lates the loss function’s gradient with respect to the weights and biases of the neural network, which then enables the network to update it’s weights and biases in a productive manner that increases its accuracy. The process of backprop- agation contains a forward and backward direction. In the forward direction, input data is fed into the neural network, which transmits the signal through the network for it to update itself. In the backwards direction, error is calcu- lated by analyzing the difference between predicted and actual outcomes. Then, the error value is used to update the network weights via gradient descent. To accomplish this, a loss-function will be defined that calculates the error value. Since gradient descent is used to minimize the error, its derivative to the weight matrix is obtained and can be multiplied with some positive value, σ, and sub- tracted value to complete one step. σ is also known as the learning rate. It is essential to set an adequate learning rate, as one that is too high will help the model learn faster, but opens the possibility of a failure to converge so that the network does not learn anything. If the learning rate is too low, the train- ing process may take too much time. Mathematically, using W as the weight matrices, this process looks like <span class="math display">\[
W_{\text{new}} = W_{\text{old}} - \sigma \times \delta E
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>