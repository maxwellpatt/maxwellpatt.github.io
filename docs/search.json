[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A Geospatial Analysis of Houston’s Blackout Sensitivity\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nDoes Economic Freedom Improve Environmental Outcomes?\n\n\n\n\n\nA Statistical Investigation\n\n\n\n\n\n\nDec 15, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nExpressivity of Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nThe AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nThomas Fire and Air Quality Inspection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "&lt;style&gt; .collage { display: flex; flex-wrap: wrap; justify-content: center; }\n.collage img { margin: 5px; border-radius: 10px; /* Optional: for rounded corners */ max-width: 200px; /* Adjust the size as needed */ height: auto; } &lt;/style&gt;"
  },
  {
    "objectID": "about.html#my-photo-collage",
    "href": "about.html#my-photo-collage",
    "title": "About",
    "section": "My Photo Collage",
    "text": "My Photo Collage\n&lt;div class=“collage”&gt; &lt;img src=“” images/chef.png alt=“Description of image 1”&gt;  &lt;/div&gt;"
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html",
    "href": "blog/2023-12-1-stats-proj/index.html",
    "title": "Does Economic Freedom Improve Environmental Outcomes?",
    "section": "",
    "text": "When we scrutinize the relationship between capitalism and environmental impacts, we’re engaging in a complex dialogue about efficiency, freedom, and sustainability. Markets act as a powerful mechanism for efficient resource allocation. Yet market-based economies, with the intrinsic drive for growth, often neglect the environmental dimension and impact. This oversight manifests in a key shortcoming: the failure to incorporate environmental costs into the pricing of goods and services.\nThe role of freedom in global economies brings about a dualistic impact on the environment. The positive aspects are rooted in the innovative potential and efficiency that economic freedom encourages. In a capitalist system, the competitive market can be a catalyst for discovering sustainable and economical production methods. Furthermore, the wealth generated in such systems can boost both private and public investments in environmental initiatives. Regulatory efficiency also stands out as an advantage, offering the ability to quickly adapt and respond to emerging environmental challenges. Unfortunately, there are many reasons why this idealized version falls short. At the end of the day, addressing the climate crisis is a collective action dilemma, since the actions of one person are negligible in the grand scheme of things. This makes it difficult for consumers to support climate-focused goods and services in exchange for relatively cheaper alternatives.\nNow, the ugly. Capitalism often leads to aggressive resource use, resulting in issues like deforestation, loss of biodiversity, and water pollution. The tendency of capitalism to prioritize short-term profit over long-term sustainability creates a blind spot for environmental considerations. This short-sighted focus on immediate financial returns often overshadows the broader, more enduring impacts on the environment. Additionally, the burden of environmental degradation in capitalist systems is not evenly distributed. Often, it’s the less affluent communities that bear the worst of this degradation, leading to a disparity in environmental impact and quality of life. In fact, the wealthiest 10 percent of the global population are responsible for half of global emissions. (Reid 2023)\nWhile capitalism has the potential to foster innovation and generate funds that could benefit environmental conservation, resource exploitation and prioritization of short-term profits present substantial challenges to achieving true environmental sustainability.\n\n\nThere is certainly a level of subjectivity, or at least a certain amount of uncertainty, when scoring the freedom of an economy. As a result, it seems impossible to create a perfect score for economic freedom. However, this doesn’t mean people haven’t tried to create a spectrum to measure how free and open different economies are.\nThe Economic Freedom of the World: 2022 Annual Report serves as the backbone of the analysis in this exploration. The dataset has a multitude of columns, the most important of which is gives each country an economic freedom index score on a scale from 1 to 10. According to the Fraser Institute, the pillars of their scoring of economic freedom depend on “personal choice, voluntary exchange, freedom to enter markets and compete, and security of the person and privately owned property.” (Gwartney 2022) This economic freedom score is measured in five areas: size of government, legal system and property rights, sound money, freedom to trade internationally, and regulation.\nIn addition to the economic freedom index column, there are a plethora of interesting variables that can be analyzed in this dataset.\nThe report discusses how countries that are have higher levels of economic freedom outperform less free countries in indicators of well being. Countries in the top quartile of economic freedom saw an average per-caputa GDP of $48,251 in 2020, while countries in the bottom quartile for economic freedom had an average of $6,542. Furthermore, life expecancy in the top quartile was 80.4 years and 66 years in the bottom quartile in 2020 (Gwartney 2022) However, do these positive impacts of higher economic freedom also lead to better environmental outcomes? This analysis will put this question to the test.\n\n\n\nThis analysis will consider several environmental outcomes pulled from the World Bank website. These data include freshwater withdrawal as a proportion of available freshwater resources (water stress), net forest depletion as a percentage of GNI, renewable energy output as a percentage of total energy consumption, renewable energy consumption as a percentage of total energy consumption, and methane emissions in metric tons of CO2 per capita (WorldBank 2023) Combined with the economic freedom data, this will allow for the analysis to look at the relationship between economic freedom scores and related variables to environmental outcomes over time in different countries."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#introduction",
    "href": "blog/2023-12-1-stats-proj/index.html#introduction",
    "title": "Does Economic Freedom Improve Environmental Outcomes?",
    "section": "",
    "text": "When we scrutinize the relationship between capitalism and environmental impacts, we’re engaging in a complex dialogue about efficiency, freedom, and sustainability. Markets act as a powerful mechanism for efficient resource allocation. Yet market-based economies, with the intrinsic drive for growth, often neglect the environmental dimension and impact. This oversight manifests in a key shortcoming: the failure to incorporate environmental costs into the pricing of goods and services.\nThe role of freedom in global economies brings about a dualistic impact on the environment. The positive aspects are rooted in the innovative potential and efficiency that economic freedom encourages. In a capitalist system, the competitive market can be a catalyst for discovering sustainable and economical production methods. Furthermore, the wealth generated in such systems can boost both private and public investments in environmental initiatives. Regulatory efficiency also stands out as an advantage, offering the ability to quickly adapt and respond to emerging environmental challenges. Unfortunately, there are many reasons why this idealized version falls short. At the end of the day, addressing the climate crisis is a collective action dilemma, since the actions of one person are negligible in the grand scheme of things. This makes it difficult for consumers to support climate-focused goods and services in exchange for relatively cheaper alternatives.\nNow, the ugly. Capitalism often leads to aggressive resource use, resulting in issues like deforestation, loss of biodiversity, and water pollution. The tendency of capitalism to prioritize short-term profit over long-term sustainability creates a blind spot for environmental considerations. This short-sighted focus on immediate financial returns often overshadows the broader, more enduring impacts on the environment. Additionally, the burden of environmental degradation in capitalist systems is not evenly distributed. Often, it’s the less affluent communities that bear the worst of this degradation, leading to a disparity in environmental impact and quality of life. In fact, the wealthiest 10 percent of the global population are responsible for half of global emissions. (Reid 2023)\nWhile capitalism has the potential to foster innovation and generate funds that could benefit environmental conservation, resource exploitation and prioritization of short-term profits present substantial challenges to achieving true environmental sustainability.\n\n\nThere is certainly a level of subjectivity, or at least a certain amount of uncertainty, when scoring the freedom of an economy. As a result, it seems impossible to create a perfect score for economic freedom. However, this doesn’t mean people haven’t tried to create a spectrum to measure how free and open different economies are.\nThe Economic Freedom of the World: 2022 Annual Report serves as the backbone of the analysis in this exploration. The dataset has a multitude of columns, the most important of which is gives each country an economic freedom index score on a scale from 1 to 10. According to the Fraser Institute, the pillars of their scoring of economic freedom depend on “personal choice, voluntary exchange, freedom to enter markets and compete, and security of the person and privately owned property.” (Gwartney 2022) This economic freedom score is measured in five areas: size of government, legal system and property rights, sound money, freedom to trade internationally, and regulation.\nIn addition to the economic freedom index column, there are a plethora of interesting variables that can be analyzed in this dataset.\nThe report discusses how countries that are have higher levels of economic freedom outperform less free countries in indicators of well being. Countries in the top quartile of economic freedom saw an average per-caputa GDP of $48,251 in 2020, while countries in the bottom quartile for economic freedom had an average of $6,542. Furthermore, life expecancy in the top quartile was 80.4 years and 66 years in the bottom quartile in 2020 (Gwartney 2022) However, do these positive impacts of higher economic freedom also lead to better environmental outcomes? This analysis will put this question to the test.\n\n\n\nThis analysis will consider several environmental outcomes pulled from the World Bank website. These data include freshwater withdrawal as a proportion of available freshwater resources (water stress), net forest depletion as a percentage of GNI, renewable energy output as a percentage of total energy consumption, renewable energy consumption as a percentage of total energy consumption, and methane emissions in metric tons of CO2 per capita (WorldBank 2023) Combined with the economic freedom data, this will allow for the analysis to look at the relationship between economic freedom scores and related variables to environmental outcomes over time in different countries."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#analysis",
    "href": "blog/2023-12-1-stats-proj/index.html#analysis",
    "title": "Does Economic Freedom Improve Environmental Outcomes?",
    "section": "Analysis",
    "text": "Analysis\nFirst, let’s import the libraries we will need to conduct this analysis.\n\n# import libraries\nlibrary(here)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(tidyr)\nlibrary(forecast)\nlibrary(randomForest)\nlibrary(tidyverse)\n\n# clear environment for sanity\nrm(list = ls())\n\n\nData Cleaning\nNow, let’s import and clean the data containing the freedom index and other scores by country.\n\n# read in freedom data\nfreedom_raw &lt;- read.csv('data/efw_ratings.csv', header = FALSE)\n\nThis data needs a good bit of clean up. This next code chunk adjusts the column headers and converts column types appropriately.\n\n# set the 5th row as the column names\ncolnames(freedom_raw) &lt;- freedom_raw[5,]\n\n# remove the first 4 rows since they are now empty and clean names, remove columns \nfreedom &lt;- freedom_raw[-c(1:5), ] %&gt;% \n  clean_names() %&gt;% \n  subset(select = -na) %&gt;% \n  subset(select = c(-na_2, -na_3, -na_4, -na_5)) %&gt;% \n  subset(select = -world_bank_region) %&gt;% \n  subset(select = -world_bank_current_income_classification_1990_present)\n\n# convert year columns from char to num\nfreedom &lt;- freedom %&gt;%\n  mutate(across(6:ncol(freedom), as.numeric)) %&gt;% \n  mutate(economic_freedom_summary_index = as.numeric(as.character(economic_freedom_summary_index)))\n\nNow, lets move on to reading in the next dataset. The freedom data serves as the policy side of the data – now we want to append and compare environmental outcomes based on different political and economic factors.\n\n# read in esg data\nesg_wb &lt;- read.csv('data/esg_wb.csv') %&gt;% \n  clean_names()\n\nThis data also needs to be cleaned up a bit. Let’s get to work.\n\ncolumn_names &lt;- c(\"x1998_yr1998\", \"x1999_yr1999\", \"x2000_yr2000\", \n                  \"x2001_yr2001\", \"x2002_yr2002\", \"x2003_yr2003\", \n                  \"x2004_yr2004\", \"x2005_yr2005\", \"x2006_yr2006\", \n                  \"x2007_yr2007\", \"x2008_yr2008\", \"x2009_yr2009\", \n                  \"x2010_yr2010\", \"x2011_yr2011\", \"x2012_yr2012\", \n                  \"x2013_yr2013\", \"x2014_yr2014\", \"x2015_yr2015\", \n                  \"x2016_yr2016\", \"x2017_yr2017\", \"x2018_yr2018\", \n                  \"x2019_yr2019\", \"x2020_yr2020\", \"x2021_yr2021\", \n                  \"x2022_yr2022\")\n\n# Function to extract and convert the year part of a column name to numeric\nextract_year &lt;- function(column_name) {\n  year_str &lt;- substr(column_name, 2, 5)\n  as.numeric(year_str)\n}\n\nfirst &lt;- names(esg_wb)[1:4]\n\n# Apply the function to each column name\nnumeric_years &lt;- sapply(column_names, extract_year)\n\nnew_cols &lt;- c(first, numeric_years)\n\nnames(esg_wb) &lt;- new_cols\n\nesg_wb &lt;- esg_wb %&gt;% \n  mutate(across(5:ncol(.), ~ as.numeric(as.character(.))))\n\n\n# make longer so it is compatible to join with freedom data\nesg_wb_long &lt;- esg_wb %&gt;%\n  pivot_longer(\n    cols = '1998':'2022', # Specify the range of columns to pivot\n    names_to = \"Year\", # Name of the new column that will store the years\n    values_to = \"Value\" # Name of the new column that will store the corresponding values\n  ) \n\nFinally, let’s merge the datasets together by year and country name.\n\n# rename the country column in freedom dataset to match esg_wb_long\nnames(freedom)[names(freedom) == \"countries\"] &lt;- \"country_name\"\n\n# rename the year column in freedom dataset to match esg_wb_long\nnames(freedom)[names(freedom) == \"year\"] &lt;- \"Year\"\n\n# perform the join\nfreedom_esg &lt;- merge(freedom, esg_wb_long, by = c(\"Year\", \"country_name\"))\n\nfreedom_esg &lt;- freedom_esg %&gt;% \n  mutate(Year = as.numeric(as.character(Year)),\n         Value = as.numeric(as.character(Value)))\n\n# save dataset as a csv\nwrite.csv(freedom_esg, \"freedom_esg.csv\", row.names = FALSE)\n\nSweet! Now we have the dataset we will be working with in the analysis.\n\n\nData Filtering\nFor convenience, I have created a dataframe for each of the environmental indicators to make the analysis smoother.\n\nwater_stress &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Level of water stress: freshwater withdrawal as a proportion of available freshwater resources\")\n\nag_area &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Agricultural land (% of land area)\")\n\nforest_depletion &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Adjusted savings: net forest depletion (% of GNI)\")\n\nrenewable_output &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Renewable electricity output (% of total electricity output)\")\n\nrenewable_consumption &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Renewable energy consumption (% of total final energy consumption)\")\n\nmethane_emissions &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Methane emissions (metric tons of CO2 equivalent per capita)\")\n\n\n\nEmissions\nIn the first piece of analysis, let’s look at the methane emissions data. The units for which the emissions are recorded in the dataset are metric tons of CO2 equivalent per capita.\nFirst, let’s compare the average emission by economic freedom quartile from 2000 to 2020.\n\n# create summary table for mean and stdev of methane emissions for each economic freedom quartile\nmethane_emissions_quartile &lt;- methane_emissions %&gt;% \n  group_by(Year, quartile) %&gt;% \n  summarize(avg_methane = mean(Value, na.rm = TRUE),\n            std_methane = sd(Value, na.rm = TRUE)) %&gt;% \n  na.omit()\n\nmethane_emissions_plot_with_error &lt;- ggplot(methane_emissions_quartile, aes(x = Year, y = avg_methane, color = as.factor(quartile))) +\n  geom_smooth(se = TRUE) +\n  labs(title = \"Methane Emissions Over Time by Economic Freedom Quartile\",\n       x = \"Year\",\n       y = \"Average Methane Emissions (metric tons of CO2 equivalent per capita)\",\n       color = \"Economic Freedom Quartile\") +\n  theme_minimal()\n\n# Display the plot\nmethane_emissions_plot_with_error\n\n\n\n\nInteresting, so the second quartile of economically free countries has the highest methane emissions by a significant amount across the entire time period. Overall, the emissions levels of all quartiles decreased from 2000 to 2020, a positive sign in the hopes of becoming a carbon-neutral planet.\n\nmethane_col_of_interest &lt;- freedom_esg %&gt;% \n  dplyr::select(Year, quartile, country_name, economic_freedom_summary_index, data, data_3, data_6, x5d_freedom_to_enter_markets_and_compete, series_name, Value) %&gt;% \n  filter(series_name == \"Methane emissions (metric tons of CO2 equivalent per capita)\")\n\n\nmethane_lm &lt;- lm(Value ~ economic_freedom_summary_index, data = methane_col_of_interest)\n\nsummary(methane_lm)\n\n\nCall:\nlm(formula = Value ~ economic_freedom_summary_index, data = methane_col_of_interest)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6361 -1.0029 -0.7013 -0.0672 14.2312 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     1.725557   0.233174   7.400 1.75e-13 ***\neconomic_freedom_summary_index -0.004037   0.034373  -0.117    0.907    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.028 on 3029 degrees of freedom\n  (511 observations deleted due to missingness)\nMultiple R-squared:  4.554e-06, Adjusted R-squared:  -0.0003256 \nF-statistic: 0.01379 on 1 and 3029 DF,  p-value: 0.9065\n\n\nThis regression reveals that there is essentially no relationship between methane emissions and economic freedom, as shown by the exceptionally high p-value. Let’s look into a few other variables from the freedom dataset that might be more revealing.\n\n# Ensure 'Value' is numeric\nmethane_emissions$Value &lt;- as.numeric(methane_emissions$Value)\n\n# Select only numeric columns (excluding 'Value' for now)\nmethane_numeric &lt;- methane_emissions %&gt;% \n  select_if(is.numeric) %&gt;% \n  dplyr::select(-Value)\n\n# Calculate correlation of each numeric column with the 'Value' column\nmethane_correlations &lt;- sapply(methane_numeric, function(x) {\n  if(is.numeric(x)) {\n    return(cor(x, methane_emissions$Value, use = \"complete.obs\"))\n  } else {\n    return(NA)\n  }\n})\n\n# Convert to a dataframe for easier viewing\nmethane_corr_results &lt;- as.data.frame(methane_correlations)\n\n# Sort by the absolute value of correlation to find the strongest correlations\nmethane_sorted_correlations &lt;- methane_corr_results %&gt;% \n  rownames_to_column(\"series\") %&gt;% \n  arrange(desc(abs(methane_corr_results)))\n\n# View the results\nhead(methane_sorted_correlations, 10)\n\n                                           series methane_correlations\n1                              ie_state_ownership           -0.3134906\n2                                            data            0.3073977\n3                                          data_4           -0.3000107\n4             x3b_standard_deviation_of_inflation           -0.2831654\n5                      x1a_government_consumption           -0.2736432\n6                                          data_5           -0.2680756\n7                          gender_disparity_index           -0.2157412\n8  x1dii_top_marginal_income_and_payroll_tax_rate            0.2132415\n9                           x1_size_of_government           -0.2067589\n10                      x1d_top_marginal_tax_rate            0.2024452\n\n\nThis correlation analysis allows us to find the variables in the freedom dataset that are most strongly correlated with methane emissions.\n\n\nRenewable Energy Consumption\nMany countries have ambitious goals in place to become carbon-neutral in the next few decades. Are countries on the right track? Let’s look at the countries that consume the most energy: China, United States, India, and Russia. (Statista 2023)\nFirst, let’s take a look at the percentage of energy consumption that comes from renewable sources for each of these countries.\n\nrenewable_consumption_leaders &lt;- renewable_consumption %&gt;% \n  filter(country_name %in% c(\"United States\", \"Russian Federation\", \"China\", \"India\"))\n\nrenewable_consumption_leaders_plot &lt;- ggplot(renewable_consumption_leaders, aes(x = Year, y = Value, color = country_name)) +\n  geom_line()\n\nrenewable_consumption_leaders_plot\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\nRenewable Energy Output\nRenewable energy output is valuable in determining a country’s economic impact since energy that is consumed can be outputted in other countries.\n\nrenewable_output_leaders &lt;- renewable_output %&gt;% \n  filter(country_name %in% c(\"United States\", \"Russian Federation\", \"China\", \"India\"))\n\nrenewable_output_leaders_plot &lt;- ggplot(renewable_output_leaders, aes(x = Year, y = Value, color = country_name)) +\n  geom_line()\n\nrenewable_output_leaders_plot\n\nWarning: Removed 24 rows containing missing values (`geom_line()`).\n\n\n\n\n\nNext, let’s investigate if there is a significant difference in renewable energy output across each quartile of economic freedom.\n\nrenewable_output_2020_quartile &lt;- renewable_output %&gt;% \n  group_by(quartile) %&gt;% \n  summarize(mean_renewable_output = mean(Value, na.rm = TRUE),\n            std_renewable_output = sd(Value, na.rm = TRUE))\n\nrenewable_output_2020_quartile\n\n# A tibble: 5 × 3\n  quartile mean_renewable_output std_renewable_output\n     &lt;dbl&gt;                 &lt;dbl&gt;                &lt;dbl&gt;\n1        1                  32.1                 28.8\n2        2                  29.4                 33.7\n3        3                  38.5                 33.4\n4        4                  42.9                 36.7\n5       NA                  25.3                 36.4\n\n\n\nrenewable_output_2015 &lt;- renewable_output %&gt;% \n  filter(Year == 2015)\n\nanova_ro &lt;- aov(Value ~ quartile, data = renewable_output)\n\nsummary(anova_ro)\n\n              Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nquartile       1   47801   47801   42.95 6.95e-11 ***\nResiduals   2237 2489666    1113                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1303 observations deleted due to missingness\n\n\n\n\nForest Depletion\nDeforestation is a massive issue that exacerbates the climate crisis.\n\nforest_depletion_2020_quartile_summary &lt;- forest_depletion %&gt;% \n  group_by(quartile) %&gt;% \n  filter(Year == 2020) %&gt;% \n  summarize(avg_depletion = mean(Value, na.rm = TRUE),\n            std_depletion = sd(Value, na.rm = TRUE))\n\n\n# Plotting\nggplot(forest_depletion_2020_quartile_summary, aes(x = factor(quartile), y = avg_depletion)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), fill = \"skyblue\") +\n  geom_errorbar(aes(ymin = avg_depletion - std_depletion, ymax = avg_depletion + std_depletion), \n                width = 0.2, position = position_dodge(0.9)) +\n  labs(title = \"Average Forest Depletion by Quartile in 2020\",\n       x = \"Economic Freedom Quartile\",\n       y = \"Average Forest Depletion\") +\n  theme_minimal()\n\n\n\n\nThis visualization provides some strong takeaways.\nLet’s run an ANOVA test to determine the significance here.\n\nforest_depletion_2020 &lt;- forest_depletion %&gt;% \n  filter(Year == 2020) %&gt;% \n  na.omit()\n\nanova_fd &lt;- aov(Value ~ quartile, data = forest_depletion_2020)\n\nsummary(anova_fd)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nquartile     1  34.33   34.33   14.73 0.000228 ***\nResiduals   91 211.99    2.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDISCUSS RESULTS HERE\n\n\nWater Stress\n\n# Ensure 'Value' is numeric\nwater_stress$Value &lt;- as.numeric(water_stress$Value)\n\n# Select only numeric columns (excluding 'Value' for now)\nwater_stress_numeric &lt;- water_stress %&gt;% \n  select_if(is.numeric) %&gt;% \n  dplyr::select(-Value)\n\n# Calculate correlation of each numeric column with the 'Value' column\nwater_stress_correlations &lt;- sapply(water_stress_numeric, function(x) {\n  if(is.numeric(x)) {\n    return(cor(x, water_stress$Value, use = \"complete.obs\"))\n  } else {\n    return(NA)\n  }\n})\n\n# Convert to a dataframe for easier viewing\nwater_stress_correlation_results &lt;- as.data.frame(water_stress_correlations)\n\n# Sort by the absolute value of correlation to find the strongest correlations\nwater_stress_sorted_correlations &lt;- water_stress_correlation_results %&gt;% \n  rownames_to_column(\"series\") %&gt;% \n  arrange(desc(abs(water_stress_correlation_results)))\n\n# View the results\nhead(water_stress_sorted_correlations, 10)\n\n                                           series water_stress_correlations\n1                                          data_4                -0.3370701\n2                                          data_5                -0.3331397\n3                          gender_disparity_index                -0.3300405\n4  x1dii_top_marginal_income_and_payroll_tax_rate                 0.2724939\n5                              ie_state_ownership                -0.2573339\n6                       x1d_top_marginal_tax_rate                 0.2517648\n7             x3b_standard_deviation_of_inflation                -0.2345262\n8                                            data                 0.2053606\n9                      x1a_government_consumption                -0.2029093\n10                           x2h_police_and_crime                 0.1872722\n\n\nINTERPRET RESULTS\n\nwater_stress_lm &lt;- lm(Value ~ data + data_4 + data_5, data = water_stress)\n\nsummary(water_stress_lm)\n\n\nCall:\nlm(formula = Value ~ data + data_4 + data_5, data = water_stress)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-658.2  -95.7  -17.8   39.4 3425.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 235.6758    25.5546   9.222  &lt; 2e-16 ***\ndata          7.9490     0.7194  11.049  &lt; 2e-16 ***\ndata_4       -4.1606     0.6881  -6.046 1.72e-09 ***\ndata_5       -5.1079     0.7376  -6.925 5.60e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 294.1 on 2352 degrees of freedom\n  (1186 observations deleted due to missingness)\nMultiple R-squared:  0.1684,    Adjusted R-squared:  0.1674 \nF-statistic: 158.8 on 3 and 2352 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html",
    "href": "blog/2023-12-2-geo-proj/index.html",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "",
    "text": "In the heart of winter, the greater Houston region found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis in a precarious state, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#introduction",
    "href": "blog/2023-12-2-geo-proj/index.html#introduction",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "",
    "text": "In the heart of winter, the greater Houston region found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis in a precarious state, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#background",
    "href": "blog/2023-12-2-geo-proj/index.html#background",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Background",
    "text": "Background\nThe February 2021 blackout in Houston transcends being a singular, isolated power outage, instead unfolding as a critical incident that checked the robustness of urban infrastructures against extreme weather conditions. This event provides a unique opportunity to analyze the resilience of a major metropolitan area’s energy grid and its capacity to withstand unforeseen natural disasters. This particular incident in Houston, a city known for its dynamic growth and complex urban layout, becomes a focal point for examining urban resilience in the face of environmental challenges. The blackout serves as a view into understanding the repercussions of such events on a city’s operational continuity and the well-being of its residents. Houston is a good example for examining the intersection of urban infrastructure with environmental circumstances, offering insights into the areas where improvements are needed to enhance city-wide resilience. The significance of this analysis lies in its potential to influence future urban planning and emergency response strategies. By investigating the impacts of the Houston blackout, valuable lessons can be learned about the importance of designing cities that are not only efficient under normal conditions but also resilient and adaptable in the face of climate challenges. This blackout demonstrates the necessity of anticipating and preparing for extreme scenarios, ensuring that cities are not just places of progress and development, but also places of safety and stability for all civilians."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "href": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Data and Methodology",
    "text": "Data and Methodology\nThe analysis hinges on three key data sources: night light data (NASA 2023), OpenStreetMap data detailing roads and buildings (OpenStreetMaps 2023), and American Community Survey (ACS) socio-economic data. OpenStreetMap and the Landsat data from NASA are regularly-updated databases that are available to the public. I employed R, utilizing libraries such as stars, sf, and terra, to manipulate and analyze these datasets. The methodology involved a step-by-step approach, beginning with the aggregation of night light intensity data to assess blackout areas. I then integrated this with road and building data to estimate the number of homes affected and concluded with an examination of socio-economic factors, offering insights into the recovery dynamics of different communities.\nResearch Question: How did economic status influence the susceptibility and resilience of communities in Houston in the 2021 winter blackouts?\nLet’s get to coding.\nStarting off by loading in the necessary libraries.\n\n# import libraries\nlibrary(dplyr)\nlibrary(abind)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stars)\nlibrary(here)\n\n# clear environment for sanity\nrm(list = ls())\n\nNext, importing data. The first set of data is the night light data, made up of tiles, or images, that capture the light intensity emanating from the surface of the planet during nighttime. The data is stored as rasters, and the stars package is used for raster handling.\n\n# read in night lights tiles\nfeb7_h08v05 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nfeb7_h0806 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\nfeb16_h0805 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nfeb16_h0806 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\nNext, we want to combine the tiles that have the same date and create two objects. We can utilize the st_mosaic() function to combine the tiles.\n\n# combine tiles for each date to create 2 objects\nfeb7 &lt;- st_mosaic(feb7_h08v05, feb7_h0806)\nfeb16 &lt;- st_mosaic(feb16_h0805, feb16_h0806)\n\nGreat. The next step involves creating a blackout mask to identify areas in Houston that lost power during the winter storms. This is done by calculating the change in light intensity, reclassifying this light internsity difference raster, and then converting the raster to a stars object.\nNow, we want to convert the blackout stars to a vector format and fix any invalid geometries using st_make_valid(). Let’s visualize the blackout in vector format to check where we are at.\n\n# calculate the change in lights intensity from Feb 7 to Feb 16\ndiff &lt;- feb7 - feb16\n\n# reclassify difference raster so it is maleable\ndiff &lt;- rast(diff)\nrmask &lt;- diff\n\n# reclassify difference raster assuming a 200 nW shift was due to the blackout\nrmask[rmask &lt;= 200] = NA\n\n# convert back to a stars object \nblackout_stars &lt;- st_as_stars(rmask)\n\n# convert blackout to vector format and fix invalid geometries, set CRS\nblackout_sf_valid_trans &lt;- st_as_sf(blackout_stars, as_points = FALSE) %&gt;% \n  st_make_valid() %&gt;% \n  st_transform(crs = 3083)\n\nThe next piece of analysis involves defining the Houston metropolitan area and cropping the blackout mask to this area of interest. It is imperative that the coordinate reference systems align of these two spatial objects!\n\n# define Houston metropolitan coordinates\nhouston_coords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n\n# create Houston polygon outline\nhouston_poly &lt;- st_polygon(list(houston_coords))\n\n# convert to sf and initially assign CRS EPSG:4326\nhouston &lt;- st_sfc(houston_poly, crs = st_crs(\"EPSG:4326\"))\n\n# transform the Houston polygon to EPSG:3083 \nhouston &lt;- st_transform(houston, crs = \"EPSG:3083\")\n\n# crop the blackout to the Houston metropolitan region\ncropped_blackout_proj &lt;- st_intersection(blackout_sf_valid_trans, houston) \n \n# assign EPSG:3083 CRS to assure consistent CRS\ncropped_blackout_proj &lt;- st_transform(cropped_blackout_proj, crs = \"EPSG:3083\")\n\nplot(cropped_blackout_proj, \n     colorbar = FALSE,\n     main = \"Locations of Significant Light Intensity Change in Houston\")\n\n\n\n\nThis is a visual of the Houston metropolitan area light intensity from before and after the storm. The census tracts that lie in the places where the black dots show up are areas that have been impacted by the blackout.\nNext, we will refine the blackout analysis by excluding areas near highways. This helps in ensuring that the blackout mask we created earlier accurately reflects residential or non-highway areas that lost power, rather than areas simply experiencing less traffic during the storms.\n\n# define SQL query\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# load highway data\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query, quiet = TRUE)\n\n# reproject highway to EPSG:3083\nhighway_proj &lt;- st_transform(highways, crs = 3083)\n\n# identify areas within 200m of all highways, dissolve buffer, and convert to sf \nbuffer &lt;- highway_proj %&gt;% \n  st_buffer(200) %&gt;% \n  st_union() %&gt;% \n  st_as_sf()\n\n# exclude areas close to highways from blackout\nblackout &lt;- st_difference(cropped_blackout_proj, buffer)\n\n\nFinding Impacted Homes\nNext, we will look to find the number of homes that have been impacted by the blackouts. A buildings dataset will be loaded in and filtered for residential buildings. Then, we will filter for the homes that are in the blackout area and display this value.\n\n# define SQL query\nquery_buildings &lt;- \"\nSELECT * \nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\"\n\n# load buildings dataset using st_read and apply the query, reproject CRS\nbuildings_proj &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = query_buildings, quiet = TRUE) %&gt;% \n  st_transform(crs = 'EPSG:3083')\n\n# filter for homes within blackout areas by filtering\nimpacted_homes &lt;- buildings_proj[blackout, , op = st_intersects]\n\n# count the number of impacted homes\nnum_impacted_homes &lt;- nrow(impacted_homes)\n\n# show the number of impacted homes\nprint(paste(\"Number of impacted homes:\", num_impacted_homes))\n\n[1] \"Number of impacted homes: 157411\"\n\n\nAs the results show, a grand total of 157,411 residential buildings were impacted by the blackouts. This total quantifies the extreme impact of the storms and highlights how many individuals and families faced the consequences of prolonged blackouts.\n\n\nInvestigating Socioeconomic Factors\nUp next, we want to consider potential disproportionate socioeconomic outcomes, particularly the average income per household in each census tract in the Houston metropolitan area. We will read in a new dataset and pull the income layer from it, and filter for homes in the blackout region using a spatial join of the census tract data and buildings data with the st_join() function.\n\n# geodatabase file path\ngdp_path &lt;- 'blog/2023-12-2-geo-proj/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb'\n\n# load the geometries from the ACS data\nacs_geom_raw &lt;- st_read(here::here(gdp_path), layer = \"ACS_2019_5YR_TRACT_48_TEXAS\", quiet = TRUE)\n\n# transform CRS to EPSG:3083\nacs_geom &lt;- st_transform(acs_geom_raw, crs = 3083)\n\n# load the ACS income data\nacs_income &lt;- st_read(here::here(gdp_path), layer = \"X19_INCOME\", quiet = TRUE)\n\n# select the median income column\nacs_income_selected &lt;- acs_income[, c(\"GEOID\", \"B19013e1\")]\n\n# trim the GEOID in acs_income_selected to keep the last 11 characters\nacs_income_selected$GEOID &lt;- substr(acs_income_selected$GEOID, nchar(acs_income_selected$GEOID) - 10, nchar(acs_income_selected$GEOID))\n\n# ensure that GEOID columns are of the same data type\n# convert GEOID to character if they are not\nacs_income_selected$GEOID &lt;- as.character(acs_income_selected$GEOID)\nacs_geom$GEOID &lt;- as.character(acs_geom$GEOID)\n\n# join the datasets\nacs_data &lt;- merge(acs_geom, acs_income_selected, by = \"GEOID\")\n\n# merge census tract information to each impacted building\nimpacted_homes_with_tract &lt;- st_join(impacted_homes, acs_data, join = st_intersects)\n\n# aggregate to find unique census tracts that have had blackouts\ntracts_with_blackouts &lt;- unique(impacted_homes_with_tract$GEOID)\n\n# create a subset of acs_data with only tracts that experienced blackouts\nacs_data_blackouts &lt;- acs_data[acs_data$GEOID %in% tracts_with_blackouts, ]"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "href": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Results and Visualizations",
    "text": "Results and Visualizations\nAt this stage, it is time to get down to the main analysis and determine the potential difference in income between impacted versus unimpacted tracts. To conduct this analysis, we will create a a map of median income by census tract and a box-plot of the income in impacted versus unimpacted tracts.\n\n# add impact status column \nacs_data$impact_status &lt;- ifelse(acs_data$GEOID %in% tracts_with_blackouts, \"Impacted\", \"Unimpacted\")\n\n# re-establish bounding coordinates to plot the map in\nhouston_extent &lt;- st_bbox(houston)\n\n# plot the map of Houston \nmap &lt;- ggplot(data = acs_data) +\n  geom_sf(aes(fill = B19013e1, color = impact_status), show.legend = 'point') +\n  scale_color_manual(values = c(\"Impacted\" = \"red\", \"Unimpacted\" = \"green\")) + \n  labs(title = \"Median Income by Census Tract and Impact Status\",\n       fill = \"Median Income\",\n       color = \"Impact Status\") +\n  coord_sf(xlim = c(houston_extent$xmin, houston_extent$xmax), \n           ylim = c(houston_extent$ymin, houston_extent$ymax), \n           expand = FALSE) +\n  theme_minimal()\n\n# make sure median income is numeric\nacs_data$B19013e1 &lt;- as.numeric(acs_data$B19013e1)\n\n# make the box plot to compare distributions\nboxplot &lt;- ggplot(acs_data, aes(x = impact_status, y = B19013e1, fill = impact_status)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Impacted\" = \"red\", \"Unimpacted\" = \"blue\")) +\n  labs(title = \"Median Income of Impacted vs Unimpacted Tracts\",\n       x = \"Blackout Impact\", y = \"Median Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\nmap\n\n\n\n\n\nboxplot"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#conclusion",
    "href": "blog/2023-12-2-geo-proj/index.html#conclusion",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Conclusion",
    "text": "Conclusion\nThe results show that the median income of a census tract did not influence how likely they were to have experienced a blackout. This can be seen in the map, but the box plot reveals it in a clearer picture. While it may seem that poorer tracts would be less likely to be influenced by the blackout and that it would take longer for their power to be fully rejuvenated, it is important to consider that there are many factors to consider in this situation. For example, it is possible that the distribution of power infrastructure and its resilience to extreme weather events played a more significant role in the occurrence of blackouts than socioeconomic factors. Areas with older or less maintained power grids, regardless of the median income of the residents, might have been more susceptible to failures. Additionally, the physical geography of the region, such as elevation or proximity to water bodies, could have influenced the impact of the winter storms on power availability. It’s also worth considering the location of essential services and emergency response priorities, which might lead to quicker restoration in certain areas irrespective of their economic status. These complexities highlight the importance of a multifaceted approach in understanding and addressing the challenges posed by such natural disasters.\nHowever, when considering areas that extended outside of the Houston and looking at all areas of Texas impacted by the blackouts, a Rockefeller foundation study revealed that areas with a high share of minority population were more than four times as likely to suffer a blackout than predominantly white areas. (Hsu 2021) The findings of the Rockefeller Foundation’s study are a call for action. They underscore the urgency for more equitable infrastructure planning and disaster response strategies that recognize and these historical imbalances. As we move forward, it is imperative that resilience and emergency preparedness are seen through the lens of equity, ensuring that all communities, no matter their racial or ethnic makeup, are fortified with the means to withstand the challenges of such catastrophic events."
  },
  {
    "objectID": "cooking.html",
    "href": "cooking.html",
    "title": "Cooking",
    "section": "",
    "text": "Follow my culinary adventures on Instagram for more delicious content: Chef Xwell on Instagram"
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#about",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#about",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "About",
    "text": "About\n\nPurpose\nThis notebook is designed to perform a comprehensive environmental analysis using two distinct approaches: Air Quality Index (AQI) trend analysis for Santa Barbara County from 2017-2018 and remote sensing data visualization of the 2017 Thomas Fire in California. The primary objectives include demonstrating data manipulation and visualization techniques, time-series analysis for AQI, and the application of true and false color imagery in assessing wildfire impacts.\n\n\nHighlights of Analysis\n\nAQI Trend Analysis (2017-2018): Retrieval and preparation of AQI data from the EPA, focusing on data cleaning, concatenation, and column modification. Implementation of a 5-day rolling average for AQI to smooth daily variations and a detailed visualization of these trends over time.\nRemote Sensing Data Visualization: Utilization of Landsat 8 satellite imagery for creating true and false color images of Santa Barbara. Integration of California fire perimeter data to assess the spatial impact of the Thomas Fire, enhancing the understanding of wildfire effects through geospatial analysis.\nData Concatenation and Cleaning: Merging AQI datasets for two consecutive years, followed by data cleaning processes such as modifying column names for consistency and dropping unnecessary columns.\nVisualization Techniques: Development of plots and maps to compare daily AQI values with the 5-day average and to overlay wildfire perimeters on satellite imagery, providing a clear visual representation of both air quality trends and the extent of wildfire damage.\n\n\n\nDataset Description\nThe analysis leverages two primary datasets:\n\nAir Quality Index (AQI) Data: Daily AQI measurements by county for 2017 and 2018, sourced from the EPA, providing insights into air quality trends over the two-year period.\nLandsat 8 Satellite Imagery and Fire Perimeter Data: High-resolution imagery capturing various spectral bands, combined with shapefile data of the 2017 California fire perimeters, to visualize and analyze the impact of wildfires.\n\n\n\nReferences to Datasets\n\nEPA Air Data: Daily AQI by County for 2017 and 2018.\nNASA EarthData: Landsat 8 Imagery.\nCalifornia Fire Perimeters 2017: California Department of Forestry and Fire Protection."
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#air-quality-index-data",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#air-quality-index-data",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "Air Quality Index Data",
    "text": "Air Quality Index Data\n\nImporting Data\nThis section involves importing necessary libraries like pandas and matplotlib for data manipulation and visualization. The AQI data for 2017 and 2018 is fetched from online sources, ensuring access to the most relevant and up-to-date air quality information.\n\n# Import necessary libraries for data manipulation and visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in AQI data for the years 2017 and 2018 from online sources\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\n\n\nData Exploration\nHere, the code checks the data types and basic information of the datasets to understand their structure. This step is crucial for planning further data processing and analysis steps.\n\n# Check data types for each dataset\ndtypes_17 = aqi_17.dtypes\ndtypes_18 = aqi_18.dtypes\nprint(\"Data types for 2017 dataset:\\n\", dtypes_17)\n\n# Get basic information about the datasets\ninfo_17 = aqi_17.info()\ninfo_18 = aqi_18.info()\n\nData types for 2017 dataset:\n State Name                   object\ncounty Name                  object\nState Code                    int64\nCounty Code                   int64\nDate                         object\nAQI                           int64\nCategory                     object\nDefining Parameter           object\nDefining Site                object\nNumber of Sites Reporting     int64\ndtype: object\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 326801 entries, 0 to 326800\nData columns (total 10 columns):\n #   Column                     Non-Null Count   Dtype \n---  ------                     --------------   ----- \n 0   State Name                 326801 non-null  object\n 1   county Name                326801 non-null  object\n 2   State Code                 326801 non-null  int64 \n 3   County Code                326801 non-null  int64 \n 4   Date                       326801 non-null  object\n 5   AQI                        326801 non-null  int64 \n 6   Category                   326801 non-null  object\n 7   Defining Parameter         326801 non-null  object\n 8   Defining Site              326801 non-null  object\n 9   Number of Sites Reporting  326801 non-null  int64 \ndtypes: int64(4), object(6)\nmemory usage: 24.9+ MB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 327537 entries, 0 to 327536\nData columns (total 10 columns):\n #   Column                     Non-Null Count   Dtype \n---  ------                     --------------   ----- \n 0   State Name                 327537 non-null  object\n 1   county Name                327537 non-null  object\n 2   State Code                 327537 non-null  int64 \n 3   County Code                327537 non-null  int64 \n 4   Date                       327537 non-null  object\n 5   AQI                        327537 non-null  int64 \n 6   Category                   327537 non-null  object\n 7   Defining Parameter         327537 non-null  object\n 8   Defining Site              327537 non-null  object\n 9   Number of Sites Reporting  327537 non-null  int64 \ndtypes: int64(4), object(6)\nmemory usage: 25.0+ MB\n\n\n\n\nAnalysis\nData Cleaning and Preparation The datasets from different years are concatenated for a comprehensive analysis. Column names are cleaned for consistency, and irrelevant columns are removed, streamlining the data for effective analysis. The conversion of the ‘date’ column to a datetime object and setting it as an index is verified to ensure proper time-series analysis.\n\n# Concatenate the two data frames for combined analysis\naqi = pd.concat([aqi_17, aqi_18])\n\n# Cleaning column names for ease of use\naqi.columns = aqi.columns.str.lower().str.replace(' ', '_')\nprint(\"Cleaned column names:\", aqi.columns.tolist())\n\n# Filtering data for Santa Barbara County\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Removing unnecessary columns\nremove = ['state_name', 'county_name', 'state_code', 'county_code']\naqi_sb = aqi_sb.drop(columns=remove)\n\n# Convert 'date' column to datetime object and set as index\naqi_sb['date'] = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\nprint(\"Index data type:\", aqi_sb.index.dtype)\n\nCleaned column names: ['state_name', 'county_name', 'state_code', 'county_code', 'date', 'aqi', 'category', 'defining_parameter', 'defining_site', 'number_of_sites_reporting']\nIndex data type: datetime64[ns]\n\n\nTime-Series Analysis\nA 5-day rolling average for AQI is calculated and checked by displaying the first few entries. This step is critical for smoothing out daily fluctuations and observing longer-term trends in air quality.\n\n# Create a 5-day rolling average for AQI\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n# Verify the rolling average by checking the first few entries\nprint(\"Sample of 5-day rolling average:\", aqi_sb['five_day_average'].head())\n\nSample of 5-day rolling average: date\n2017-01-01    39.000000\n2017-01-02    37.500000\n2017-01-03    48.666667\n2017-01-04    45.000000\n2017-01-05    43.400000\nName: five_day_average, dtype: float64\n\n\n\n\nFinal Output\nThe output visualizes the Air Quality Index (AQI) in Santa Barbara County across two years: 2017 and 2018. The blue line represents the daily AQI values, showing considerable variability with several peaks indicating days of poor air quality. The orange line depicts the 5-day rolling average of AQI, which smooths out the daily fluctuations to reveal the underlying trends more clearly. Notably, there is a significant peak at the start of December 2017, marked by the dashed vertical line, which correlates to a big fire in the area. Overall, the visualization effectively communicates the temporal changes in air quality and the utility of using a rolling average to understand longer-term trends.\n\n# Plotting daily AQI and 5-day average AQI\naqi_sb['aqi'].plot(label='Daily AQI', color='blue')\naqi_sb['five_day_average'].plot(label='5-Day Average AQI', color='orange', linewidth=2)\nplt.axvline(pd.Timestamp('2017-12-01'), color='black', linestyle='--', label='Start of December 2017')\nplt.title('Daily AQI vs. 5-Day Average AQI')\nplt.xlabel('Date')\nplt.ylabel('AQI Value')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#false-color-image",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#false-color-image",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "False Color Image",
    "text": "False Color Image\n\nImporting Data\nIn this initial step, libraries essential for processing geospatial data, such as NumPy, Pandas, GeoPandas, and xarray, are imported. These tools enable the handling of complex raster and vector data formats necessary for environmental and geographical analyses.\n\n# Import necessary libraries for handling geospatial and raster data\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize\nfrom rasterio.crs import CRS\n\n# File path for raster data\ndata_path = os.path.join(os.getcwd(), \"data\", \"landsat8-2018-01-26-sb-simplified.nc\")\n\n# Open raster file\nlandsat = rioxr.open_rasterio(data_path)\n\n# read fire data\nfire = gpd.read_file(\"data/California_Fire_Perimeters_2017/California_Fire_Perimeters_2017.shp\")\n\n/Users/maxwellpatterson/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of \"\n\n\n\n\nGeographical Context\nThe geographical context is established by loading Landsat 8 satellite imagery for the Santa Barbara region and fire perimeter data from 2017. This step situates the analysis within the specific area affected by the Thomas Fire, setting the stage for a targeted examination of the landscape.\nData Preparation and Alignment\nThis section involves transforming the CRS of the fire perimeter data to match the Landsat data. The successful alignment of these datasets is confirmed, which is imperative for precise spatial overlay in the analysis.\n\n# Reduce dimensions\nlandsat_new = landsat.squeeze(['band'])\n\n# Display updated landsat\nlandsat_new.values\n\n&lt;bound method Mapping.values of &lt;xarray.Dataset&gt;\nDimensions:      (x: 870, y: 731)\nCoordinates:\n    band         int64 1\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0\nData variables:\n    red          (y, x) float64 ...\n    green        (y, x) float64 ...\n    blue         (y, x) float64 ...\n    nir08        (y, x) float64 ...\n    swir22       (y, x) float64 ...&gt;\n\n\n\n\nData Exploration\nHere, the code explores the Landsat data to ensure it aligns with the fire perimeter data’s CRS. This alignment is critical for accurate mapping and overlay of geospatial data, ensuring that subsequent analyses accurately reflect the true geographical relationships.\n\n# Plot fire\nfire.plot()\n\n# Look at the first 5 rows of fire data\nfire.head()\n\n# Look at landsat data\nlandsat_new\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (x: 870, y: 731)\nCoordinates:\n    band         int64 1\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0\nData variables:\n    red          (y, x) float64 ...\n    green        (y, x) float64 ...\n    blue         (y, x) float64 ...\n    nir08        (y, x) float64 ...\n    swir22       (y, x) float64 ...xarray.DatasetDimensions:x: 870y: 731Coordinates: (4)band()int641array(1)x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Indexes: (2)xPandasIndexPandasIndex(Index([121305.0, 121575.0, 121845.0, 122115.0, 122385.0, 122655.0, 122925.0,\n       123195.0, 123465.0, 123735.0,\n       ...\n       353505.0, 353775.0, 354045.0, 354315.0, 354585.0, 354855.0, 355125.0,\n       355395.0, 355665.0, 355935.0],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([3952395.0, 3952125.0, 3951855.0, 3951585.0, 3951315.0, 3951045.0,\n       3950775.0, 3950505.0, 3950235.0, 3949965.0,\n       ...\n       3757725.0, 3757455.0, 3757185.0, 3756915.0, 3756645.0, 3756375.0,\n       3756105.0, 3755835.0, 3755565.0, 3755295.0],\n      dtype='float64', name='y', length=731))Attributes: (0)\n\n\n\n\n\n\n\nAnalysis\nCreating True and False Color Images The creation of true and false color images utilizes specific bands from the Landsat data. These images are normalized to enhance visual contrast, aiding in the identification of different land features. Checks confirm the normalization process has occurred correctly.\nTrue Color Image\n\n# Select R, G, B bands\nred_band = landsat_new['red']\ngreen_band = landsat_new['green']\nblue_band = landsat_new['blue']\n\n# Stack the bands along the 'color' dimension\nrgb_image = xr.concat([red_band, green_band, blue_band], dim='color')\nrgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())\n\n# Plot the RGB image \nplt.imshow(rgb_image.transpose('y', 'x', 'color')) \n\n# Visualize map\nplt.show()\n\n\n\n\nFalse Color Image\n\n# Select spectral bands from the landsat data\nswir22 = landsat_new['swir22']\nnir    = landsat_new['nir08']\nred    = landsat_new['red']\n\n# Stack the bands along 'color' dimension\nfalse_color_image = xr.concat([swir22, nir, red], dim='color')\n\n# Normalize the false color image to increase contrast\nfalse_image = (false_color_image - false_color_image.min()) / (false_color_image.max() - false_color_image.min())\n\n# Plot the RGB image using imshow\nlandsat_new[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust= True)\n\n&lt;matplotlib.image.AxesImage at 0x7fc3b16f4850&gt;\n\n\n\n\n\n\n\nFinal Output\nThe final output is a composite image that illustrates the affected area during the Thomas Fire. This output serves as a potent visual tool for understanding the spatial extent of wildfires and highlights the value of remote sensing in environmental monitoring and disaster assessment.\n\n# Filter for Thomas fire\nthomas_fire = fire[fire['FIRE_NAME']==\"THOMAS\"]\n\n# Convert thomas_fire to GeoDataFrame to same crs as landsat\nthomas_fire = thomas_fire.to_crs(landsat_new.rio.crs)\n\n# Store false color map\nfalse_color = landsat_new[['swir22', 'nir08', 'red']].to_array()\n\n\n# Initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Plot outline of california and create key for legend\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# Plot lulc bounding box and create key for legend\nthomas_fire.plot(ax=ax, color='red', alpha=0.5, edgecolor='black', linewidth=0.5)\nthomas_patch = mpatches.Patch(color='red', label='Thomas Fire')\n\n# Create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')\n\nText(0.5, 1.0, 'False Color Image of Santa Barbara with Thomas Fire Boundary')"
  },
  {
    "objectID": "blog/2023-12-1-ethics-proj/index.html",
    "href": "blog/2023-12-1-ethics-proj/index.html",
    "title": "The AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact",
    "section": "",
    "text": "Introduction\nIn this epoch of technological evolution, artificial intelligence stands at the forefront, bringing forth a paradigm shift in all domains, including environmental research. This shift, however, is not without its dichotomies. AI, while being a potent instrument in environmental conservation efforts that can help address the biggest issues in the field, contributes to environmental issues in its own right, primarily through its significant carbon emissions. In addition to high carbon output, AI systems have a high human cost, as the technology is deeply rooted in the exploitation of human capital. Training AI models has been a revolution in the sense that it has essentially broken Moore’s Law, which states that computational capacity doubles roughly every 18 months. An analysis of the OpenAI research lab found that its AI training models had been doubling in capacity every 3.4 months from 2012 to 2018.(OpenAI 2023) This is roughly a 300,000x increase from Moore’s Law over this six year period, a shocking result that speaks to the speed and scale at which AI is being integrated, scaled and refined. However, this incredible increase in the capacity of AI comes with a price that is paid by the environment since the energy outputs from training these models are quite high and leave behind a significant carbon footprint. The aviation industry will be used as a parallel case study to demonstrate the potential transformation the field of AI could undergo. This discussion aims to delve deeper into this paradox of the benefits that AI brings to solving environmental problems while also realizing the negative environmental impact of these systems, exploring AI’s dual role in environmental research and its broader implications. Understanding and addressing AI’s environmental footprint is not just a technological imperative but a moral and ecological one that is only to get more nuanced as AI continues its sweep over modern civilization.\n\n\n\nThe centrality of technology\n\n\n(Communications 2020)\n\n\nParadox of AI in Environmental Research\nAI’s multifaceted nature in environmental research is a blend of promise and peril. It offers groundbreaking capabilities in analyzing and predicting environmental changes, such as climate variations and pollution trends. However, the carbon footprint associated with training sophisticated AI models is substantial, thereby contributing to the very environmental challenges it seeks to mitigate.\nFirst, consider the promise. There are many incredible AI systems being built that aid in the advancement of carbon neutrality. The International Methane Emissions Observatory leverages AI to revolutionize approaches of monitoring and mitigating methane emissions. The platform is an open source, public database that connects methane emissions data with action on science, transparency and policy to inform the best possible data driven decisions.(UNEP 2022) CO2 AI is another powerful industry player that helps corporations measure, track, simulate and optimize their emissions at scale. The company’s AI tools implement deep learning techniques and graph theory to increase the accuracy of emission measurements. (AI 2023) A third example of a company using AI to address the climate crisis is Earth Insights, a collaborative effort between Hewlett Packard and Conservation International, which uses AI to monitor biodiversity loss of tropical forests worldwide with the goal of protecting these ecosystems through science and policy initiatives. (Moussa 2023) These are only three of the many companies and initiatives that are utilizing AI to reduce the effects of the impending climate crisis through a wide array of strategies. Improvements in energy grid efficiency, vehicle carbon output, building and city emissions, industry-related efficiency improvements in design, sourcing, manufacturing, and distribution, and farming are some other examples of the benefits AI brings to the table.\nIn the swiftly evolving landscape of AI development, a proliferating concern has emerged regarding the environmental impact of these systems. The University of Massachusetts Amherst conducted a pivotal study, focusing on the energy consumption and consequent carbon emissions of training Natural Language Processing (NLP) models. Their findings are stark, revealing that the carbon footprint of training a single large language model is roughly an astonishing 300,000 kg of CO2 emissions. (Strubell 2019) To put this into a more tangible perspective, consider someone driving a car or flying in an airplane. The average car, for instance, emits about 4.6 metric tons of carbon dioxide each year. In this context, the emissions from training one of these large language models equates to the yearly emissions of nearly 65 cars. Similarly, if we consider air travel, a single flight from New York to San Francisco generates about 1 ton of CO2 per passenger. Thus, the emissions from training a large language model are roughly equivalent to 300 such flights. These comparisons shed light on the environmental footprint of AI development, underscoring the need for more sustainable practices in this rapidly advancing field. The energy required for these systems will increase as these AI systems get more powerful and robust, so this problem will only become more pressing over time unless there is a significant shift in where AI systems receive their energy from. (Visuals 2015)\n\n\n\nOne metric ton of C02 for scale\n\n\n(Visuals 2015)\n\n\nQuantifying the Carbon Footprint\n\nChallenges in Measurement\nThe endeavor to accurately quantify AI’s carbon footprint is riddled with complexities. The muddied nature of energy consumption in data centers, coupled with the diverse methodologies used in AI operations, makes it challenging to pinpoint the exact environmental impact. Crawford and Joler’s insightful analysis sheds light on these hidden costs, revealing the extensive energy consumption behind AI’s operations. (Crawford 2018) Their work, published by the SHARE Lab SHARE Foundation and the AI Now Institute NYU, offers a profound insight into the often-overlooked environmental consequences of AI development​​. This intricate web of energy use, stretching from the vast data centers to the minutiae of algorithmic calculations, uncovers a distinct reality. The environmental footprint of AI is not merely a byproduct of its computational processes but a deeply embedded aspect of its very existence. As Crawford and Joler illustrate, every facet of AI, from its design to deployment, is intertwined with significant energy demands. (Crawford 2018) This revelation calls for a recalibration in the approach to building AI tools, urging a shift towards more sustainable practices that consider the long-term ecological impacts. To make matters worse, there is a lack of incentives for companies to share data and publicly display their emissions output. The incredible pace at which the AI and computation industry has evolved and globalized has led to a few players holding the majority of the control of this infrastructure and policy adaptation.\n\n\nTools and Methods\nIn the face of these challenges, the field has witnessed the advent of innovative tools aimed at more accurately measuring the energy consumption and emissions of AI processes. A noteworthy contribution is the emissions calculator developed by Alexandre Lacoste and his team. This tool represents a significant step forward in our ability to pragmatically estimate the carbon footprint associated with AI operations. The underlying research in creating this calculator underscores that emissions are intricately linked to several factors: the geographical location of the training server, the characteristics of the energy grid powering it, the duration of the training process, and the specific hardware utilized for the training. (Lacoste, n.d.) This issue transcends mere technological hurdles, veering into the realms of political will and consumer awareness. \nThere is a pressing need for increased transparency in the AI sector. Contrary to a lack of knowledge, companies are quite cognizant of the extent of training conducted on their hardware. They possess detailed insights into the computational demands of various algorithms, akin to the aviation industry’s awareness of the energy efficiency of air travel. In aviation, there are established standards and detailed reports outlining the hardware used in planes, their flight duration, and distance travele. Similarly, in the AI industry, adopting such standardized reporting and transparency could lead to more informed choices and practices from people actually using the AI. It is vital to draw parallels from sectors like aviation to instill a culture of accountability and sustainability in AI development. Just as the aviation industry has evolved with a focus on energy efficiency and transparency, the AI sector too must embrace these values. This shift not only demands technological innovation but also a concerted effort from policymakers, industry leaders, and consumers to foster an environment where sustainable AI development is not just a choice but an expectation.\n\n\n\nImpact and Implications\nAI’s carbon footprint undeniably casts a long shadow over environmental ecosystems, influencing them at both granular and broader scales. This paradoxical situation, where AI’s immediate benefits in environmental research are contrasted against the more protracted environmental impacts of its carbon emissions, forms a complex ethical tableau. Data centers, pivotal to AI operations, are now outpacing the aviation industry in greenhouse gas emissions. (Cho 2023)\nVenturing into renewable energy solutions for AI systems uncovers additional ecological concerns. Consider lithium, a critical component in the creation of rechargeable batteries. The extraction of this element is a water-intensive process; every ton of lithium extraction requires about 500,000 gallons of water.(IER 2020) This level of water consumption has profound environmental repercussions. In Chile, the world’s second-largest lithium producer, the indigenous Copiapó communities find themselves in a struggle with mining companies over vital land and water rights.(Greenfield 2022) These mining activities in regions like Salar de Atacama, Chile are so water-intensive that, according to the Institute for Energy Research, they account for 65% of the area’s water usage.(IER 2020) The resultant water loss inflicts severe damage on the local ecosystems, leading to the depletion of wetlands and water sources. Such environmental degradation has far-reaching effects, endangering native flora and fauna and severely impacting the lives and livelihoods of local populations. This situation presents a nuanced challenge: while strides in AI technology are heralded for their potential to address environmental issues, their underlying infrastructure and energy sources inadvertently contribute to ecological degradation. The pursuit of technological advancement in AI, therefore, necessitates a careful consideration of its environmental trade-offs, urging a thoughtful and sustainable approach to innovation. The ethical implications of using high-carbon-footprint AI in environmental research revolve around a fundamental conflict. This conflict lies in balancing the immediate utility of AI in research endeavors against the long-term environmental costs, raising questions about the ethical responsibilities of researchers and developers.\n\n\nMitigation Practices and Future Directions\n\nSocietal Adaptation\nThe responsibility of steering AI towards greener practices lies significantly with how societies will adapt and unlock the powers of AI in the environmental space. Collective efforts from researchers, developers, and other civilians are essential in pioneering sustainable AI development. Furthermore, understanding the realities and consequences of climate change can allow communities to have better practices when it comes to climate awareness and outcomes by prioritizing less destructive AI systems. These are the most critical avenues in which society can adapt AI in an environmentally-conscious manner:\n\nRaising ecological awareness about AI’s benefits is crucial. Enhanced data collection, through citizen science initiatives, advanced sensors, and remote monitoring, enriches our understanding and application of AI in environmental contexts. This wealth of data aids in crafting more accurate and responsive solutions to ecological challenges. In high-risk areas, spreading knowledge on crisis management becomes imperative. Gathering data through surveys and community engagement can provide invaluable insights into local needs and vulnerabilities.\nDeveloping disaster maps and emergency plans, bolstered by AI analytics, empowers communities to better safeguard themselves during crises. Proactive measures, such as timely alerts via text and email, must seamlessly integrate into our daily routines, ensuring preparedness becomes a norm rather than an exception.\nBeyond immediate responses, AI’s role in enhancing societal systems is significant. Optimizing food distribution and growth, minimizing waste, and ensuring equitable access to resources are areas where AI can make a substantial difference. The public health sector also stands to gain, with AI-driven solutions potentially improving healthcare delivery and outcomes, particularly in environmentally vulnerable communities.\nCrucially, AI can play a transformative role in building resilient infrastructures that are attuned to the demands of a changing climate. Implementing eco-conscious solutions like wetlands, seawalls, and stormwater ponds can significantly bolster defenses in susceptible regions. This requires a communal shift in perspective, embracing and supporting such infrastructures for their long-term benefits.\nIn addition, AI’s capability in detecting and addressing issues in energy grids and water systems marks a leap forward in infrastructure management. Modern systems and buildings, equipped with AI technologies, are more adept at preemptively identifying and rectifying environmental and operational challenges. Therefore, the integration of AI into our societal fabric, from data gathering to infrastructure development, heralds a new era of eco-conscious and efficient environmental management.\n\n\n\n\nUnlocking Nature through AI\n\n\n(BCG 2023)\n\n\nPolicy and Regulation\nEffective policies, legal frameworks, and comprehensive governmental backing are fundamental in steering AI to be more sustainable. These elements can guide the tech industry in adopting environmentally responsible practices, thereby achieving a crucial balance between technological advancement and ecological conservation. To realize this, global cooperation and standardized AI policies, akin to international climate change agreements, are essential. Such policies can harmonize and mitigate the varying environmental impacts of AI technologies across diverse regions. Mandating transparency and reporting standards is another key step. By requiring companies to disclose their energy consumption and carbon emissions related to AI activities, we can draw on the aviation industry’s approach to transparency, underscoring the potential benefits of such practices in the realm of AI.\nTax incentives also play a vital role. Tax credits, grants, and subsidies can motivate companies, researchers, and institutions to invest in AI solutions that address environmental challenges. Moreover, a regulatory framework focused on the energy consumption of data centers and AI operations is necessary. Setting energy efficiency benchmarks and enforcing penalties for non-compliance could significantly expedite addressing AI’s environmental footprint. Public awareness is an equally important facet. Educating the public about AI’s environmental impact can shift consumer demand towards sustainable AI products and services, thus nudging the market towards greener practices. This comprehensive approach, encompassing policy, regulation, incentives, and awareness, is imperative to shape AI’s future in a way that is both technologically innovative and environmentally responsible.\n\n\n\nConclusion\nAs we stand at the crossroads of technological advancement and environmental preservation, the role of artificial intelligence in environmental research embodies a profound paradox. AI’s potential to address some of the most pressing environmental challenges is indisputable, yet its substantial carbon footprint and ecological implications present a legitimate counterbalance. This juxtaposition demands a concerted effort from all sectors of society – particularly policymakers, financial institutions, researchers, and the tech community – to forge a path that harmonizes technological innovation with ecological responsibility.\nFor policymakers, the imperative is clear: to enact comprehensive, forward-thinking legislation that not only promotes sustainable AI practices but also holds the industry accountable for its environmental impact. This involves creating regulatory frameworks that encourage green innovation, enforce transparency in energy consumption and emissions, and support the development of environmentally friendly AI applications. Financial institutions have a pivotal role to play. By directing investments towards sustainable AI ventures and research, they can accelerate the shift towards environmentally conscious technologies. This shift not only aligns with global environmental goals but also opens avenues for sustainable economic growth and long-term profitability in the green technology sector. Researchers and the tech community are tasked with the continuous innovation of AI technologies, ensuring they align with environmental objectives. This includes improving the energy efficiency of AI models, exploring alternative, less carbon-intensive computing methods, and advancing AI applications in environmental monitoring, conservation, and sustainable resource management.\nAbove all, the journey towards an eco-friendly AI future is a collective one. It requires a paradigm shift in how we perceive and utilize technology – not as an end in itself but as a means to a greater goal of ecological sustainability. By integrating ethical considerations into AI development and harnessing its power to serve environmental needs, we can ensure that the AI-driven era ahead is not only technologically advanced but also environmentally conscious and sustainable. Let this be a call to action for everyone involved: to balance the scales between the immense potential of AI and the urgent need to protect and preserve our environment.\n\n\n\nWho Shapes AI?\n\n\n(State 2023)\n\n\n\n\n\nReferences\n\nAI, CO2. 2023. “Overview.” https://co2ai.com/.\n\n\nBCG. 2023. “How AI Can Speed Climate Action.” https://www.bcg.com/publications/2023/how-ai-can-speedup-climate-action.\n\n\nCho, Renee. 2023. “AI’s Growing Carbon Footprint.” https://news.climate.columbia.edu/2023/06/09/ais-growing-carbon-footprint/.\n\n\nCommunications, Nature. 2020. “The Role of Artificial Intelligence in Achieving the Sustainable Development Goals.” https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-019-14108-y/MediaObjects/41467_2019_14108_Fig5_HTML.png.\n\n\nCrawford, Kate et al. 2018. “Anatomy of AI Systems.” https://anatomyof.ai/img/ai-anatomy-publication.pdf.\n\n\nGreenfield, Nicole. 2022. “Lithium Mining Is Leaving Chile’s Indigenous Communities High and Dry (Literally).” https://www.nrdc.org/stories/lithium-mining-leaving-chiles-indigenous-communities-high-and-dry-literally#:~:text=Chile%20is%20the%20second%2Dlargest,transition%20away%20from%20fossil%20fuels.\n\n\nIER. 2020. “The Environmental Impact of Lithium Batteries.” https://www.instituteforenergyresearch.org/renewable/the-environmental-impact-of-lithium-batteries/.\n\n\nLacoste, Alexandre et al. n.d. “Quantifying the Carbon Emissions of Machine Learning.” https://arxiv.org/abs/1910.09700.\n\n\nMoussa, Raed. 2023. “HP - Earth Insights.” https://raedmoussa.com/hp-earth-insights.\n\n\nOpenAI. 2023. https://openai.com/research/ai-and-compute.\n\n\nState, US Department of. 2023.\n\n\nStrubell, Emma. 2019. “Machine Learning Models for Efficient and Robust Natural Language Processing.” https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=2805&context=dissertations_2.\n\n\nUNEP. 2022. “How Artificial Intelligence Is Helping Tackle Environmental Challenges.” https://www.unep.org/news-and-stories/story/how-artificial-intelligence-helping-tackle-environmental-challenges.\n\n\nVisuals, Carbon. 2015. “One Metric Ton of Carbon.” https://www.carbonvisuals.com/projects/tag/carbon."
  },
  {
    "objectID": "blog/2023-12-7-neural-net-exp/neural-net-exp.html",
    "href": "blog/2023-12-7-neural-net-exp/neural-net-exp.html",
    "title": "Expressivity of Neural Networks",
    "section": "",
    "text": "The rise of neural networks as a tool has led to many technological advancements that have added significant value to society. Things such as voice and image recognition, medical diagnoses, and targeted marketing are a few examples of concepts that have seen significant improvements in performance and applica- tion from the application of neural networks. AI and deep learning unlock a whole new realm of what is possible within mathematics, as the computer is able to learn to differentiate among different representations in data that can reveal important trends and observations from some data set. In order to accomplish this, neural networks calculate, with the use of input and output data, some sort of pattern that can be applied to these situations such as voice recognition and medical diagnoses. In this project, we are tasked with exploring the na- ture in which neural networks can be applied to the approximation of functions. Overall goals of the project include developing an understanding of the training process dynamics, the ways in which the depth and width of the neural network influence the approximation, challenges and takeaways from this investigation."
  },
  {
    "objectID": "blog/2023-12-7-neural-net-exp/neural-net-exp.html#background",
    "href": "blog/2023-12-7-neural-net-exp/neural-net-exp.html#background",
    "title": "Expressivity of Neural Networks",
    "section": "Background",
    "text": "Background\n\nPerceptron\nWhile modern neural networks can have millions of layers, the first neural net- work had, naturally, only one layer. In July of 1958, Frank Rosenblatt revealed his prized perceptron, which set the stage for which modern day AI was built upon. Rosenblatt coined the perceptron as being the ”first machine which is capable of having an original idea” (Cornell). The perceptron is a type of single-layer neural network that is inspired from the collaborative nature of how neurons work in the brain. It operated by labeling inputs in two ways, such as left or right, or man or woman. In case of an incorrect prediction, the al- gorithm adjusts itself to improve the accuracy of future predictions. After the completion of thousands or millions of iterations, the neural networks gets more precise over time in order to obtain some valuable result. Weights and bias play a critical role in establishing the relationship between inputs and outputs in the context of perceptrons. A perceptron computes the weighted sum of the input features, \\[\nf(x) = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b = 0\n\\]\nx1, x2, …, xn represent the inputs, w1, w2, …, wn represent the weights associated with these inputs, and b is the bias term. The weights and bias are the keys that drive the neural network, as they are flexible controls that establish the model and allow for optimal relationships to be understood during the learning process. Bias allows for the offsetting of any constant in the data, which enables the perceptron to generate more accurate result.\nThe result that the perceptron generates, known as the decision boundary, is calculated using the input data, the associated weights in the network, and the bias term. This is done through the linear combination of these factors. The decision boundary can be defined as the set of points x such that \\[\nf(x) = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b = 0\n\\]\nIn this case, the sign of the function f(x) determines the significance of the output. For example, a positive value could be associated to the left direction and a negative value associated to the right direction. The flexibility of the per- ceptron is dictated by the weights and biases it possesses. The model is able to learn which input features are more important than others by assigning proper weight values to each input. Coupled with the bias term allow for certain con- stant to be offset in the data, the perceptron is able to pick up certain trends in the input data. Ultimately, the perceptron’s power lies in its ability to find an adequate decision boundary through the manipulation of weights and biases, highlighting how important these concepts are in the process of neural networks.\n\n\nActivation Function In Play\nActivation functions allow for neural networks to learn and understand compli- cated patterns between inputs and outputs by presenting the possibility of non- linearity into the network. Non-linear networks are much more powerful than linear networks as they are able to capture much deeper and more profound correlations in data. There are many usable activation functions today: the Sigmoid function, the tanh function, Reduced Linear Unit function, or ReLU, and LeakyReLU, and Exponential Linear Unit function, or ELU to name a few. The choice of activation function depends on the specific architecture of the network. It will be looked into later in the project as to how each of these activation functions is best suited for certain types of situations. Sigmoid : a smooth, S-shaped curve that maps input values to values in the range of 0 to 1. Mathematically, the function is defined as \\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nWhile the sigmoid function is sufficiently utilized in the outputs of binary sit- uations, it can struggle when dealing with more hidden layers due to vanishing gradient type of issues. In these erroneous scenarios, the gradient approaches zero or some large, positively or negatively, constant that hinders the learning ability of the network. tanh: smooth like the sigmoid function, but maps input values to the range of -1 to 1 instead of 0 to 1. Mathematically, the function can be defined as \\[\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nThe tanh function is effective in that its output is centered around zero, which can help deal with the vanishing gradient issue to a degree but still deals with the issue of blow-up values.\nReLU/LeakyReLU : a piecewise linear function that returns the input value if it is positive, and zero, or a small negative value associated the input, if the value is negative. Mathematically, the function is defined as \\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nReLU helps deal with the vanishing gradient issue since the gradient is constant for positive input values and cannot spiral in or out. The dying ReLU problem does exist however, in which the neurons deactivate for negative inputs which makes it difficult for the network to learn and adapt over time.\nELU: variation of the ReLU function that smooths the curve to the left of the x-axis. Mathematically, the function can be defined as \\[\n\\text{ELU}(x) =\n\\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\sigma \\times (e^x - 1) & \\text{if } x \\leq 0\n\\end{cases}\n\\]\nHere, σ is some positive constant. ELU helps to address the vanishing gradient issue and upholds a stronger curve. The differences in application and results obtained using these approximation functions will be discussed later.\n\n\nBackpropagation\nBackpropagation is an essential tool that is used in the training of neural net- works, especially ones that have a multitude of layers. Backpropagation calcu- lates the loss function’s gradient with respect to the weights and biases of the neural network, which then enables the network to update it’s weights and biases in a productive manner that increases its accuracy. The process of backprop- agation contains a forward and backward direction. In the forward direction, input data is fed into the neural network, which transmits the signal through the network for it to update itself. In the backwards direction, error is calcu- lated by analyzing the difference between predicted and actual outcomes. Then, the error value is used to update the network weights via gradient descent. To accomplish this, a loss-function will be defined that calculates the error value. Since gradient descent is used to minimize the error, its derivative to the weight matrix is obtained and can be multiplied with some positive value, σ, and sub- tracted value to complete one step. σ is also known as the learning rate. It is essential to set an adequate learning rate, as one that is too high will help the model learn faster, but opens the possibility of a failure to converge so that the network does not learn anything. If the learning rate is too low, the train- ing process may take too much time. Mathematically, using W as the weight matrices, this process looks like \\[\nW_{\\text{new}} = W_{\\text{old}} - \\sigma \\times \\delta E\n\\]"
  },
  {
    "objectID": "basketball.html",
    "href": "basketball.html",
    "title": "Basketball",
    "section": "",
    "text": "This is a test page with some analysis.\n\nlibrary(here)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\ndec2_data &lt;- read.csv(here('data/12-3-fantrax-stats.csv')) %&gt;% \n  clean_names()\n\nLet’s look at the average fantasy points per game of the top 20 scorers on each fantasy team\n\nfantasy_team_stats_top20 &lt;- dec2_data %&gt;%\n  group_by(status) %&gt;%\n  arrange(desc(fp_g)) %&gt;%\n  slice_max(order_by = fp_g, n = 20) %&gt;%\n  summarise(mean_fps_game = mean(fp_g),\n            age_top_20 = mean(age)) %&gt;% \n  arrange(desc(mean_fps_game))\n\nfantasy_team_stats_top20\n\n# A tibble: 15 × 3\n   status   mean_fps_game age_top_20\n   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n 1 BBB               33.5       30.7\n 2 Jmarr237          33.4       29.4\n 3 STARKS            31.5       28  \n 4 CCC               31.4       26.4\n 5 maxpat01          30.4       27.4\n 6 GBRAYERS          29.6       27.0\n 7 VSL               28.7       25.0\n 8 HHBC              28.5       26.0\n 9 Orcas             28.4       24  \n10 BIGFOOTS          25.4       24.2\n11 SERP              24.4       22.3\n12 SDP               23.2       24.2\n13 FA                15.7       28.2\n14 W (Mon)           13.2       20  \n15 W (Sun)           11.4       28.5\n\n\nWhich teams have drafted the best to win this season based on ADP?\n\ndec2_data$adp &lt;- as.numeric(dec2_data$adp)\n\nWarning: NAs introduced by coercion\n\ntop10_adp &lt;- dec2_data %&gt;% \n  group_by(status) %&gt;% \n  arrange(desc(fp_g)) %&gt;%\n  slice_max(order_by = fp_g, n = 6) %&gt;%\n  summarize(avg_adp = mean(adp),\n            st_adp = sd(adp),\n            avg_fp_g = mean(fp_g),\n            avg_age = mean(age))\n\ntop10_adp\n\n# A tibble: 15 × 5\n   status   avg_adp st_adp avg_fp_g avg_age\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 BBB         26.8   24.4     47.7    30  \n 2 BIGFOOTS   101.    61.6     35.9    23.7\n 3 CCC         47.6   42.2     42.6    28.7\n 4 FA         663.   520.      17.4    27.3\n 5 GBRAYERS    55.3   35.6     40.5    24.3\n 6 HHBC        50.3   32.2     42.2    26.2\n 7 Jmarr237    29.1   24.1     45.2    31.8\n 8 Orcas       90.2   80.0     42.3    23.7\n 9 SDP         52.5   29.9     36.5    22.3\n10 SERP        82.7   47.6     34.1    20.8\n11 STARKS      41.2   31.3     41.8    27.2\n12 VSL         57.8   56.1     40.4    25  \n13 W (Mon)    426.    NA       13.2    20  \n14 W (Sun)    810.   497.      11.4    28.5\n15 maxpat01    31.4   26.2     45.8    28.3\n\n\n\nteam_data &lt;- data.frame(\n  status = c(\"BBB\", \"BIGFOOTS\", \"CCC\", \"FA\", \"GBRAYERS\", \"HHBC\", \"Jmarr237\", \"Orcas\", \"SDP\", \"SERP\", \"STARKS\", \"VSL\", \"W (Mon)\", \"W (Sun)\", \"maxpat01\"),\n  avg_adp = c(36.444, 132.474, 73.996, 887.022, 74.268, 75.587, 41.462, 93.073, 113.332, 87.753, 61.734, 76.725, 425.520, 809.605, 88.989),\n  st_adp = c(23.61738, 79.90325, 51.43987, 484.52501, 38.97783, 43.63397, 28.84837, 66.76798, 102.91086, 39.61803, 44.00860, 58.68209, NA, 496.99000, 119.87338),\n  avg_fp_g = c(43.045, 31.145, 38.084, 16.635, 36.632, 37.360, 40.824, 36.683, 31.203, 30.979, 38.892, 36.071, 13.160, 11.360, 39.277),\n  avg_age = c(31.1, 24.4, 28.1, 27.4, 25.4, 27.2, 29.9, 23.9, 25.4, 22.6, 27.3, 24.8, 20.0, 28.5, 29.9)\n)\n\n\n# Normalize avg_adp, avg_fp_g, and avg_age\nteam_data &lt;- top10_adp %&gt;%\n  mutate(\n    norm_avg_adp = (avg_adp - min(avg_adp)) / (max(avg_adp) - min(avg_adp)),\n    norm_avg_fp_g = (avg_fp_g - min(avg_fp_g)) / (max(avg_fp_g) - min(avg_fp_g)),\n    norm_avg_age = (avg_age - min(avg_age)) / (max(avg_age) - min(avg_age))\n  )\n\n# Constants for scaling and weight\nepsilon = 0.01  # To avoid division by zero\nweight_adp = 0.2  # Weight for avg_adp\nweight_fp_g = 0.6  # Weight for avg_fp_g\nweight_age = 0.2  # Weight for avg_age\n\n# Calculate Win Now Score\nteam_data &lt;- team_data %&gt;%\n  mutate(\n    win_now_score = ((1 / (norm_avg_adp + epsilon)) * weight_adp) + \n                    (norm_avg_fp_g * weight_fp_g) +\n                    ((1 / (norm_avg_age + epsilon)) * weight_age)\n  )\n\n# View the dataframe with Win Now Scores\nprint(team_data)\n\n# A tibble: 15 × 9\n   status   avg_adp st_adp avg_fp_g avg_age norm_avg_adp norm_avg_fp_g\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 BBB         26.8   24.4     47.7    30        0              1     \n 2 BIGFOOTS   101.    61.6     35.9    23.7      0.0944         0.675 \n 3 CCC         47.6   42.2     42.6    28.7      0.0267         0.860 \n 4 FA         663.   520.      17.4    27.3      0.812          0.166 \n 5 GBRAYERS    55.3   35.6     40.5    24.3      0.0365         0.802 \n 6 HHBC        50.3   32.2     42.2    26.2      0.0300         0.849 \n 7 Jmarr237    29.1   24.1     45.2    31.8      0.00302        0.931 \n 8 Orcas       90.2   80.0     42.3    23.7      0.0810         0.851 \n 9 SDP         52.5   29.9     36.5    22.3      0.0328         0.691 \n10 SERP        82.7   47.6     34.1    20.8      0.0714         0.626 \n11 STARKS      41.2   31.3     41.8    27.2      0.0184         0.838 \n12 VSL         57.8   56.1     40.4    25        0.0396         0.799 \n13 W (Mon)    426.    NA       13.2    20        0.509          0.0495\n14 W (Sun)    810.   497.      11.4    28.5      1              0     \n15 maxpat01    31.4   26.2     45.8    28.3      0.00591        0.947 \n# ℹ 2 more variables: norm_avg_age &lt;dbl&gt;, win_now_score &lt;dbl&gt;\n\n\nCool! Now, let’s look at the most promising young players in this dynasty format.\n\nunder25 &lt;- dec2_data %&gt;% \n  filter(age &lt; 25, fp_g &gt; 20)\n\nIt would be neat to figure out the value of a future pick using trade information.\nLet’s make a trade function\n\n# read in data\ndec3_trades &lt;- read.csv(here('data/12-3-trades.csv'))\n\n\n# trade log\ntrades_df &lt;- data.frame(\n  From_Team = character(), \n  To_Team = character(), \n  Date = character(), \n  Period = integer(), \n  Item1 = character(), \n  Item2 = character(),\n  stringsAsFactors = FALSE\n)\n\n\n# # Assuming you've read your CSV file into a dataframe called 'csv_trades'\n# for (i in 1:nrow(dec3_trades)) {\n#   trades_df &lt;- add_trade(\n#     trades_df, \n#     dec3_trades$Player[i], \n#     dec3_trades$Team[i], \n#     dec3_trades$Position[i], \n#     dec3_trades$From[i], \n#     dec3_trades$To[i], \n#     dec3_trades$`Date (PST)`[i], \n#     dec3_trades$Period[i]\n#   )\n# }\n# \n# add_trade &lt;- function(trade_df, player, team, position, from_team, to_team, date, period) {\n#   new_trade &lt;- data.frame(\n#     Player = player,\n#     Team = team,\n#     Position = position,\n#     From = from_team,\n#     To = to_team,\n#     Date = date,\n#     Period = period,\n#     stringsAsFactors = FALSE\n#   )\n#   \n#   updated_trades_df &lt;- rbind(trade_df, new_trade)\n#   return(updated_trades_df)\n# }\n# \n#"
  }
]