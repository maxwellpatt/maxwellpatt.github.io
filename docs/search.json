[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "&lt;style&gt; .collage { display: flex; flex-wrap: wrap; justify-content: center; }\n.collage img { margin: 5px; border-radius: 10px; /* Optional: for rounded corners */ max-width: 200px; /* Adjust the size as needed */ height: auto; } &lt;/style&gt;"
  },
  {
    "objectID": "blog/2023-11-30-ethics-proj/index.html",
    "href": "blog/2023-11-30-ethics-proj/index.html",
    "title": "",
    "section": "",
    "text": "title: \"Ethics Project\"\n  description"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maxwell Patterson",
    "section": "",
    "text": "Education\n\n\nExperience"
  },
  {
    "objectID": "blog/2023-11-6-test-post/index.html",
    "href": "blog/2023-11-6-test-post/index.html",
    "title": "The AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact",
    "section": "",
    "text": "Introduction\nIn this epoch of technological evolution, artificial intelligence stands at the forefront, bringing forth a paradigm shift in various domains, including environmental research. This shift, however, is not without its dichotomies. AI, while being a potent instrument in environmental conservation efforts that can help address the biggest issues in the field, contributes to environmental issues as well, primarily through its significant carbon emissions. In addition to high carbon output, AI systems have a high human cost, as the technology is deeply rooted in the exploitation of human capital. Training AI models has been a revolution in the sense that it has almost broken Moore's Law, which states that computational capacity doubles roughly every 18 months. An analysis of the OpenAI research lab found that its AI training models had been doubling in capacity every 3.4 months from 2012 to 2018. This is roughly a 300,000x increase from Moore's Law over this six year period, a shocking result that speaks to the speed and scale at which AI is being integrated, scaled and refined. However, this incredible increase in the capacity of AI comes with a price that is paid by the environment since the energy outputs from training these models are high. The aviation industry will be used as a parallel case study to demonstrate the potential transformation the field of AI could undergo. This discussion aims to delve deeper into this paradox of the benefits that AI brings to solving environmental problems while also realizing the negative environmental impact of these systems, exploring AI’s dual role in environmental research and its broader implications. Understanding and addressing AI’s environmental footprint is not just a technological imperative but a moral and ecological one that is only to get more nuanced as AI continues its sweep over modern civilization.\n\n\nParadox of AI in Environmental Research\nAI’s multifaceted nature in environmental research is a blend of promise and peril. It offers groundbreaking capabilities in analyzing and predicting environmental changes, such as climate variations and pollution patterns. However, the carbon footprint associated with training sophisticated AI models is substantial, thereby contributing to the very environmental challenges it seeks to mitigate.\nFirst, consider the promise. There are many incredible AI systems being built that aid in the advancement of carbon neutrality. The International Methane Emissions Observatory leverages AI to revolutionize approaches of monitoring and mitigating methane emissions. The platform is an open source, public database that connects methane emissions data with action on science, transparency and policy to inform the best possible data driven decision. CO2 AI is another powerful industry player that helps corporations measure, track, simulate and optimize their emissions at scale. The company's AI tools implement deep learning techniques and graph theory to increase the accuracy of emission measurements. A third example of a company using AI to address the climate crisis is Earth Insights, a collaborative effort between Hewlett Packard and Conservation International, that uses AI to monitor biodiversity loss of tropical forests worldwide with the goal of protecting these ecosystems through science and policy initiatives. These are only three of the many companies and initiatives that are utilizing AI to reduce the effects of the impending climate crisis through a wide array of strategies. Improvements in energy grid efficiency, vehicle carbon output, building and city emissions, industry-related efficiency improvements in design, sourcing, manufacturing, and distribution, and farming are some other examples of the benefits AI brings to the table.\nIn the swiftly evolving landscape of AI development, a proliferating concern has emerged regarding the environmental impact of these sophisticated systems. The University of Massachusetts Amherst conducted a pivotal study, focusing on the energy consumption and consequent carbon emissions of training Natural Language Processing (NLP) models. Their findings are stark, revealing that the carbon footprint of training a single, large language model approximates an astonishing 300,000 kg of CO2 emissions. To put this into a more tangible perspective, consider someone driving a car or flying in an airplane. The average car, for instance, emits about 4.6 metric tons (or 4,600 kg) of carbon dioxide annually. In this context, the emissions from training one of these language models equate to the yearly emissions of nearly 65 cars. Similarly, if we consider air travel, a single flight from New York to San Francisco generates about 1 ton of CO2 per passenger. Thus, the emissions from training a large language model are roughly equivalent to 300 such flights. These comparisons shed light on the environmental footprint of AI development, underscoring the need for more sustainable practices in this rapidly advancing field. The energy required for these systems will increase as these AI systems get more powerful and robust, so this problem will only become more pressing over time unless there is a significant shift in where AI systems get their energy from.\n\n\nQuantifying the Carbon Footprint\n\nChallenges in Measurement\nThe endeavor to accurately quantify AI’s carbon footprint is riddled with complexities. The muddied nature of energy consumption in data centers, coupled with the diverse methodologies used in AI operations, makes it challenging to pinpoint the exact environmental impact. Crawford and Joler’s insightful analysis sheds light on these hidden costs, revealing the extensive energy consumption behind AI’s operations. Their work, published by the SHARE Lab SHARE Foundation and the AI Now Institute NYU, offers a profound insight into the often-overlooked environmental consequences of AI development​​. This intricate web of energy use, stretching from the vast data centers to the minutiae of algorithmic calculations, uncovers a distinct reality. The environmental footprint of AI is not merely a byproduct of its computational processes but a deeply embedded aspect of its very existence. As Crawford and Joler illustrate, every facet of AI, from its design to deployment, is intertwined with significant energy demands. This revelation calls for a recalibration in our approach to AI, urging a shift towards more sustainable practices that consider the long-term ecological impacts. To make matters worse, there is a lack of incentives for companies to share data and publicly display their emissions output. The incredible pace that AI and the overarching computation industry has evolved and globalized over time has led to a few players holding the majority of the control of this infrastructure and policy adaptation.\n\n\nTools and Methods\nIn the face of these challenges, the field has witnessed the advent of innovative tools aimed at more accurately measuring the energy consumption and emissions of AI processes. A noteworthy contribution is the emissions calculator developed by Alexandre Lacoste and his team. This tool represents a significant stride in our ability to pragmatically estimate the carbon footprint associated with AI operations. The underlying research in creating this calculator underscores that emissions are intricately linked to several factors: the geographical location of the training server, the characteristics of the energy grid powering it, the duration of the training process, and the specific hardware utilized for the training. This issue transcends mere technological hurdles, veering into the realms of political will and consumer awareness. \nThere is a pressing need for increased transparency in the AI sector. Contrary to a lack of knowledge, companies are quite cognizant of the extent of training conducted on their hardware. They possess detailed insights into the computational demands of various algorithms, akin to the aviation industry’s awareness of the energy efficiency of airplanes. In aviation, there are established standards and detailed reports outlining the hardware used in planes, their flight durations, and distances. Similarly, in the AI industry, adopting such standardized reporting and transparency could lead to more informed choices and practices. It is vital to draw parallels from sectors like aviation to instill a culture of accountability and sustainability in AI development. Just as the aviation industry has evolved with a focus on energy efficiency and transparency, the AI sector too must embrace these values. This shift not only demands technological innovation but also a concerted effort from policymakers, industry leaders, and consumers to foster an environment where sustainable AI development is not just a choice but an expectation.\n\n\n\nImpact and Implications\nAI’s carbon footprint undeniably casts a long shadow over environmental ecosystems, influencing them at both granular and broader scales. This paradoxical situation, where AI’s immediate benefits in environmental research are contrasted against the more protracted environmental impacts of its carbon emissions, forms a complex ethical tableau. An example of this dichotomy is that data centers, pivotal to AI operations, are now outpacing the aviation industry in greenhouse gas emissions. \nVenturing into renewable energy solutions for AI systems uncovers additional ecological concerns. Consider lithium, a critical component in the creation of rechargeable batteries. The extraction of this element is a water-intensive process; every ton of lithium extraction requires about 500,000 gallons of water. This immense water consumption has profound environmental repercussions. In Chile, the world’s second-largest lithium producer, the indigenous Copiapó communities find themselves in a contentious struggle with mining companies over vital land and water rights. These mining activities in regions like Salar de Atacama are so water-intensive that, according to the Institute for Energy Research, they account for 65% of the area’s water usage. The resultant water loss inflicts severe damage on the local ecosystems, leading to the depletion of wetlands and water sources. Such environmental degradation has far-reaching effects, endangering native flora and fauna and severely impacting the lives and livelihoods of local populations. This situation presents a nuanced challenge: while strides in AI technology are heralded for their potential to address environmental issues, their underlying infrastructure and energy sources inadvertently contribute to ecological degradation. The pursuit of technological advancement in AI, therefore, necessitates a careful consideration of its environmental trade-offs, urging a thoughtful and sustainable approach to innovation. The ethical implications of using high-carbon-footprint AI in environmental research revolve around a fundamental conflict. This conflict lies in balancing the immediate utility of AI in research endeavors against the long-term environmental costs, raising questions about the ethical responsibilities of researchers and developers.\n\n\nMitigation Practices and Future Directions\n\nSocietal Adaptation\nThe responsibility of steering AI towards greener practices lies significantly with how societies will adapt and unlock the powers of AI in the environmental space. Collective efforts from researchers, developers, and other civilians are essential in pioneering sustainable AI development. Furthermore, understanding the realities and consequences of climate change can allow communities to have better practices when it comes to climate awareness and outcomes by prioritizing less destructive AI sytems. These are the most critical avenues in which society can adapt AI in an environmentally-conscious manner:\n\nRaising ecological awareness about AI’s benefits is crucial. Enhanced data collection, through citizen science initiatives, advanced sensors, and remote monitoring, enriches our understanding and application of AI in environmental contexts. This wealth of data aids in crafting more accurate and responsive solutions to ecological challenges. In high-risk areas, spreading knowledge on crisis management becomes imperative. Gathering data through surveys and community engagement can provide invaluable insights into local needs and vulnerabilities.\nDeveloping disaster maps and emergency plans, bolstered by AI analytics, empowers communities to better safeguard themselves during crises. Proactive measures, such as timely alerts via text and email, must seamlessly integrate into our daily routines, ensuring preparedness becomes a norm rather than an exception.\nBeyond immediate responses, AI’s role in enhancing societal systems is significant. Optimizing food distribution and growth, minimizing waste, and ensuring equitable access to resources are areas where AI can make a substantial difference. The public health sector also stands to gain, with AI-driven solutions potentially improving healthcare delivery and outcomes, particularly in environmentally vulnerable communities.\nCrucially, AI can play a transformative role in building resilient infrastructures that are attuned to the demands of a changing climate. Implementing eco-conscious solutions like wetlands, seawalls, and stormwater ponds can significantly bolster defenses in susceptible regions. This requires a communal shift in perspective, embracing and supporting such infrastructures for their long-term benefits.\nIn addition, AI’s capability in detecting and addressing issues in energy grids and water systems marks a leap forward in infrastructure management. Modern systems and buildings, equipped with AI technologies, are more adept at preemptively identifying and rectifying environmental and operational challenges. Therefore, the integration of AI into our societal fabric, from data gathering to infrastructure development, heralds a new era of eco-conscious and efficient environmental management.\n\n\n\nPolicy and Regulation\nEffective policies, legal frameworks, and comprehensive governmental backing are fundamental in steering AI to be more sustainable. These elements can guide the tech industry in adopting environmentally responsible practices, thereby achieving a crucial balance between technological advancement and ecological conservation. To realize this, global cooperation and standardized AI policies, akin to international climate change agreements, are essential. Such policies can harmonize and mitigate the varying environmental impacts of AI technologies across diverse regions. Mandating transparency and reporting standards is another key step. By requiring companies to disclose their energy consumption and carbon emissions related to AI activities, we can draw on the aviation industry’s approach to transparency, underscoring the potential benefits of such practices in the realm of AI.\nTax incentives also play a vital role. Tax credits, grants, and subsidies can motivate companies, researchers, and institutions to invest in AI solutions that address environmental challenges. Moreover, a regulatory framework focused on the energy consumption of data centers and AI operations is necessary. Setting energy efficiency benchmarks and enforcing penalties for non-compliance could significantly expedite addressing AI's environmental footprint. Public awareness is an equally important facet. Educating the public about AI’s environmental impact can shift consumer demand towards sustainable AI products and services, thus nudging the market towards greener practices. This comprehensive approach, encompassing policy, regulation, incentives, and awareness, is imperative to shape AI’s future in a way that is both technologically innovative and environmentally responsible.\n\n\n\nConclusion\nIn conclusion, as we stand at the crossroads of technological advancement and environmental preservation, the role of artificial intelligence in environmental research embodies a profound paradox. AI’s potential to address some of the most pressing environmental challenges is indisputable, yet its substantial carbon footprint and ecological implications present a legitimate counterbalance. This juxtaposition demands a concerted effort from all sectors of society – particularly policymakers, financial institutions, researchers, and the tech community – to forge a path that harmonizes technological innovation with ecological responsibility.\nFor policymakers, the imperative is clear: to enact comprehensive, forward-thinking legislation that not only promotes sustainable AI practices but also holds the industry accountable for its environmental impact. This involves creating regulatory frameworks that incentivize green innovation, enforce transparency in energy consumption and emissions, and support the development of environmentally friendly AI applications. Financial institutions have a pivotal role to play. By directing investments towards sustainable AI ventures and research, they can accelerate the shift towards environmentally conscious technologies. This shift not only aligns with global environmental goals but also opens avenues for sustainable economic growth and long-term profitability in the green technology sector. Researchers and the tech community are tasked with the continuous innovation of AI technologies, ensuring they align with environmental objectives. This includes improving the energy efficiency of AI models, exploring alternative, less carbon-intensive computing methods, and advancing AI applications in environmental monitoring, conservation, and sustainable resource management.\nAbove all, the journey towards an eco-friendly AI future is a collective one. It requires a paradigm shift in how we perceive and utilize technology – not as an end in itself but as a means to a greater goal of ecological sustainability. By integrating ethical considerations into AI development and harnessing its power to serve environmental needs, we can ensure that the AI-driven era ahead is not only technologically advanced but also environmentally conscious and sustainable. Let this be a call to action for everyone involved: to balance the scales between the immense potential of AI and the urgent need to protect and preserve our environment."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Geospatial Project\n\n\n\n\n\nEDS223 Final Project\n\n\n\n\n\n\nDec 1, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nStats Project\n\n\n\n\n\nEDS222 Final Project\n\n\n\n\n\n\nDec 1, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\n  \n\n\n\n\nThe AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact\n\n\n\n\n\nEDS242 Final Project\n\n\n\n\n\n\nDec 1, 2023\n\n\nMaxwell Patterson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023-12-1-ethics-proj/index.html",
    "href": "blog/2023-12-1-ethics-proj/index.html",
    "title": "The AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact",
    "section": "",
    "text": "Introduction\nIn this epoch of technological evolution, artificial intelligence stands at the forefront, bringing forth a paradigm shift in various domains, including environmental research. This shift, however, is not without its dichotomies. AI, while being a potent instrument in environmental conservation efforts that can help address the biggest issues in the field, contributes to environmental issues as well, primarily through its significant carbon emissions. In addition to high carbon output, AI systems have a high human cost, as the technology is deeply rooted in the exploitation of human capital.(Dhar, n.d.) Training AI models has been a revolution in the sense that it has almost broken Moore’s Law, which states that computational capacity doubles roughly every 18 months. An analysis of the OpenAI research lab found that its AI training models had been doubling in capacity every 3.4 months from 2012 to 2018.(Dhar, n.d.) This is roughly a 300,000x increase from Moore’s Law over this six year period, a shocking result that speaks to the speed and scale at which AI is being integrated, scaled and refined. However, this incredible increase in the capacity of AI comes with a price that is paid by the environment since the energy outputs from training these models are high. The aviation industry will be used as a parallel case study to demonstrate the potential transformation the field of AI could undergo. This discussion aims to delve deeper into this paradox of the benefits that AI brings to solving environmental problems while also realizing the negative environmental impact of these systems, exploring AI’s dual role in environmental research and its broader implications. Understanding and addressing AI’s environmental footprint is not just a technological imperative but a moral and ecological one that is only to get more nuanced as AI continues its sweep over modern civilization.\n\n\n\nThe centrality of technology\n\n\n(Communications 2020)\n\n\nParadox of AI in Environmental Research\nAI’s multifaceted nature in environmental research is a blend of promise and peril. It offers groundbreaking capabilities in analyzing and predicting environmental changes, such as climate variations and pollution patterns. However, the carbon footprint associated with training sophisticated AI models is substantial, thereby contributing to the very environmental challenges it seeks to mitigate.\nFirst, consider the promise. There are many incredible AI systems being built that aid in the advancement of carbon neutrality. The International Methane Emissions Observatory leverages AI to revolutionize approaches of monitoring and mitigating methane emissions. The platform is an open source, public database that connects methane emissions data with action on science, transparency and policy to inform the best possible data driven decision.(UNEP 2022) CO2 AI is another powerful industry player that helps corporations measure, track, simulate and optimize their emissions at scale. The company’s AI tools implement deep learning techniques and graph theory to increase the accuracy of emission measurements. A third example of a company using AI to address the climate crisis is Earth Insights, a collaborative effort between Hewlett Packard and Conservation International, that uses AI to monitor biodiversity loss of tropical forests worldwide with the goal of protecting these ecosystems through science and policy initiatives. These are only three of the many companies and initiatives that are utilizing AI to reduce the effects of the impending climate crisis through a wide array of strategies. Improvements in energy grid efficiency, vehicle carbon output, building and city emissions, industry-related efficiency improvements in design, sourcing, manufacturing, and distribution, and farming are some other examples of the benefits AI brings to the table.\nIn the swiftly evolving landscape of AI development, a proliferating concern has emerged regarding the environmental impact of these sophisticated systems. The University of Massachusetts Amherst conducted a pivotal study, focusing on the energy consumption and consequent carbon emissions of training Natural Language Processing (NLP) models. Their findings are stark, revealing that the carbon footprint of training a single, large language model approximates an astonishing 300,000 kg of CO2 emissions. To put this into a more tangible perspective, consider someone driving a car or flying in an airplane. The average car, for instance, emits about 4.6 metric tons (or 4,600 kg) of carbon dioxide annually. In this context, the emissions from training one of these language models equate to the yearly emissions of nearly 65 cars. Similarly, if we consider air travel, a single flight from New York to San Francisco generates about 1 ton of CO2 per passenger. Thus, the emissions from training a large language model are roughly equivalent to 300 such flights. These comparisons shed light on the environmental footprint of AI development, underscoring the need for more sustainable practices in this rapidly advancing field. The energy required for these systems will increase as these AI systems get more powerful and robust, so this problem will only become more pressing over time unless there is a significant shift in where AI systems get their energy from.(Visuals 2015)\n\n\n\nOne metric ton of C02 for scale\n\n\n(Visuals 2015)\n\n\nQuantifying the Carbon Footprint\n\nChallenges in Measurement\nThe endeavor to accurately quantify AI’s carbon footprint is riddled with complexities. The muddied nature of energy consumption in data centers, coupled with the diverse methodologies used in AI operations, makes it challenging to pinpoint the exact environmental impact. Crawford and Joler’s insightful analysis sheds light on these hidden costs, revealing the extensive energy consumption behind AI’s operations. Their work, published by the SHARE Lab SHARE Foundation and the AI Now Institute NYU, offers a profound insight into the often-overlooked environmental consequences of AI development​​. This intricate web of energy use, stretching from the vast data centers to the minutiae of algorithmic calculations, uncovers a distinct reality. The environmental footprint of AI is not merely a byproduct of its computational processes but a deeply embedded aspect of its very existence. As Crawford and Joler illustrate, every facet of AI, from its design to deployment, is intertwined with significant energy demands. This revelation calls for a recalibration in our approach to AI, urging a shift towards more sustainable practices that consider the long-term ecological impacts. To make matters worse, there is a lack of incentives for companies to share data and publicly display their emissions output. The incredible pace that AI and the overarching computation industry has evolved and globalized over time has led to a few players holding the majority of the control of this infrastructure and policy adaptation.\n\n\nTools and Methods\nIn the face of these challenges, the field has witnessed the advent of innovative tools aimed at more accurately measuring the energy consumption and emissions of AI processes. A noteworthy contribution is the emissions calculator developed by Alexandre Lacoste and his team. This tool represents a significant stride in our ability to pragmatically estimate the carbon footprint associated with AI operations. The underlying research in creating this calculator underscores that emissions are intricately linked to several factors: the geographical location of the training server, the characteristics of the energy grid powering it, the duration of the training process, and the specific hardware utilized for the training. This issue transcends mere technological hurdles, veering into the realms of political will and consumer awareness. \n\n\n\nEco-Friendly AI\n\n\nThere is a pressing need for increased transparency in the AI sector. Contrary to a lack of knowledge, companies are quite cognizant of the extent of training conducted on their hardware. They possess detailed insights into the computational demands of various algorithms, akin to the aviation industry’s awareness of the energy efficiency of airplanes. In aviation, there are established standards and detailed reports outlining the hardware used in planes, their flight durations, and distances. Similarly, in the AI industry, adopting such standardized reporting and transparency could lead to more informed choices and practices. It is vital to draw parallels from sectors like aviation to instill a culture of accountability and sustainability in AI development. Just as the aviation industry has evolved with a focus on energy efficiency and transparency, the AI sector too must embrace these values. This shift not only demands technological innovation but also a concerted effort from policymakers, industry leaders, and consumers to foster an environment where sustainable AI development is not just a choice but an expectation.\n\n\n\nImpact and Implications\nAI’s carbon footprint undeniably casts a long shadow over environmental ecosystems, influencing them at both granular and broader scales. This paradoxical situation, where AI’s immediate benefits in environmental research are contrasted against the more protracted environmental impacts of its carbon emissions, forms a complex ethical tableau. An example of this dichotomy is that data centers, pivotal to AI operations, are now outpacing the aviation industry in greenhouse gas emissions. \nVenturing into renewable energy solutions for AI systems uncovers additional ecological concerns. Consider lithium, a critical component in the creation of rechargeable batteries. The extraction of this element is a water-intensive process; every ton of lithium extraction requires about 500,000 gallons of water. This immense water consumption has profound environmental repercussions. In Chile, the world’s second-largest lithium producer, the indigenous Copiapó communities find themselves in a contentious struggle with mining companies over vital land and water rights. These mining activities in regions like Salar de Atacama are so water-intensive that, according to the Institute for Energy Research, they account for 65% of the area’s water usage. The resultant water loss inflicts severe damage on the local ecosystems, leading to the depletion of wetlands and water sources. Such environmental degradation has far-reaching effects, endangering native flora and fauna and severely impacting the lives and livelihoods of local populations. This situation presents a nuanced challenge: while strides in AI technology are heralded for their potential to address environmental issues, their underlying infrastructure and energy sources inadvertently contribute to ecological degradation. The pursuit of technological advancement in AI, therefore, necessitates a careful consideration of its environmental trade-offs, urging a thoughtful and sustainable approach to innovation. The ethical implications of using high-carbon-footprint AI in environmental research revolve around a fundamental conflict. This conflict lies in balancing the immediate utility of AI in research endeavors against the long-term environmental costs, raising questions about the ethical responsibilities of researchers and developers.\n\n\nMitigation Practices and Future Directions\n\nSocietal Adaptation\nThe responsibility of steering AI towards greener practices lies significantly with how societies will adapt and unlock the powers of AI in the environmental space. Collective efforts from researchers, developers, and other civilians are essential in pioneering sustainable AI development. Furthermore, understanding the realities and consequences of climate change can allow communities to have better practices when it comes to climate awareness and outcomes by prioritizing less destructive AI sytems. These are the most critical avenues in which society can adapt AI in an environmentally-conscious manner:\n\nRaising ecological awareness about AI’s benefits is crucial. Enhanced data collection, through citizen science initiatives, advanced sensors, and remote monitoring, enriches our understanding and application of AI in environmental contexts. This wealth of data aids in crafting more accurate and responsive solutions to ecological challenges. In high-risk areas, spreading knowledge on crisis management becomes imperative. Gathering data through surveys and community engagement can provide invaluable insights into local needs and vulnerabilities.\nDeveloping disaster maps and emergency plans, bolstered by AI analytics, empowers communities to better safeguard themselves during crises. Proactive measures, such as timely alerts via text and email, must seamlessly integrate into our daily routines, ensuring preparedness becomes a norm rather than an exception.\nBeyond immediate responses, AI’s role in enhancing societal systems is significant. Optimizing food distribution and growth, minimizing waste, and ensuring equitable access to resources are areas where AI can make a substantial difference. The public health sector also stands to gain, with AI-driven solutions potentially improving healthcare delivery and outcomes, particularly in environmentally vulnerable communities.\nCrucially, AI can play a transformative role in building resilient infrastructures that are attuned to the demands of a changing climate. Implementing eco-conscious solutions like wetlands, seawalls, and stormwater ponds can significantly bolster defenses in susceptible regions. This requires a communal shift in perspective, embracing and supporting such infrastructures for their long-term benefits.\nIn addition, AI’s capability in detecting and addressing issues in energy grids and water systems marks a leap forward in infrastructure management. Modern systems and buildings, equipped with AI technologies, are more adept at preemptively identifying and rectifying environmental and operational challenges. Therefore, the integration of AI into our societal fabric, from data gathering to infrastructure development, heralds a new era of eco-conscious and efficient environmental management.\n\n\n\n\nUnlocking Nature through AI\n\n\n(BCG 2023)\n\n\nPolicy and Regulation\nEffective policies, legal frameworks, and comprehensive governmental backing are fundamental in steering AI to be more sustainable. These elements can guide the tech industry in adopting environmentally responsible practices, thereby achieving a crucial balance between technological advancement and ecological conservation. To realize this, global cooperation and standardized AI policies, akin to international climate change agreements, are essential. Such policies can harmonize and mitigate the varying environmental impacts of AI technologies across diverse regions. Mandating transparency and reporting standards is another key step. By requiring companies to disclose their energy consumption and carbon emissions related to AI activities, we can draw on the aviation industry’s approach to transparency, underscoring the potential benefits of such practices in the realm of AI.\nTax incentives also play a vital role. Tax credits, grants, and subsidies can motivate companies, researchers, and institutions to invest in AI solutions that address environmental challenges. Moreover, a regulatory framework focused on the energy consumption of data centers and AI operations is necessary. Setting energy efficiency benchmarks and enforcing penalties for non-compliance could significantly expedite addressing AI’s environmental footprint. Public awareness is an equally important facet. Educating the public about AI’s environmental impact can shift consumer demand towards sustainable AI products and services, thus nudging the market towards greener practices. This comprehensive approach, encompassing policy, regulation, incentives, and awareness, is imperative to shape AI’s future in a way that is both technologically innovative and environmentally responsible.\n\n\n\nConclusion\nIn conclusion, as we stand at the crossroads of technological advancement and environmental preservation, the role of artificial intelligence in environmental research embodies a profound paradox. AI’s potential to address some of the most pressing environmental challenges is indisputable, yet its substantial carbon footprint and ecological implications present a legitimate counterbalance. This juxtaposition demands a concerted effort from all sectors of society – particularly policymakers, financial institutions, researchers, and the tech community – to forge a path that harmonizes technological innovation with ecological responsibility.\nFor policymakers, the imperative is clear: to enact comprehensive, forward-thinking legislation that not only promotes sustainable AI practices but also holds the industry accountable for its environmental impact. This involves creating regulatory frameworks that incentivize green innovation, enforce transparency in energy consumption and emissions, and support the development of environmentally friendly AI applications. Financial institutions have a pivotal role to play. By directing investments towards sustainable AI ventures and research, they can accelerate the shift towards environmentally conscious technologies. This shift not only aligns with global environmental goals but also opens avenues for sustainable economic growth and long-term profitability in the green technology sector. Researchers and the tech community are tasked with the continuous innovation of AI technologies, ensuring they align with environmental objectives. This includes improving the energy efficiency of AI models, exploring alternative, less carbon-intensive computing methods, and advancing AI applications in environmental monitoring, conservation, and sustainable resource management.\nAbove all, the journey towards an eco-friendly AI future is a collective one. It requires a paradigm shift in how we perceive and utilize technology – not as an end in itself but as a means to a greater goal of ecological sustainability. By integrating ethical considerations into AI development and harnessing its power to serve environmental needs, we can ensure that the AI-driven era ahead is not only technologically advanced but also environmentally conscious and sustainable. Let this be a call to action for everyone involved: to balance the scales between the immense potential of AI and the urgent need to protect and preserve our environment.\n\n\n\nWho Shapes AI?\n\n\n(State 2023)\n\n\n\n\n\nReferences\n\nBCG. 2023. “How AI Can Speed Climate Action.” https://www.bcg.com/publications/2023/how-ai-can-speedup-climate-action.\n\n\nCommunications, Nature. 2020. “The Role of Artificial Intelligence in Achieving the Sustainable Development Goals.” https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-019-14108-y/MediaObjects/41467_2019_14108_Fig5_HTML.png.\n\n\nDhar. n.d.\n\n\nState, US Department of. 2023.\n\n\nUNEP. 2022. “How Artificial Intelligence Is Helping Tackle Environmental Challenges.” https://www.unep.org/news-and-stories/story/how-artificial-intelligence-helping-tackle-environmental-challenges.\n\n\nVisuals, Carbon. 2015. “One Metric Ton of Carbon.” https://www.carbonvisuals.com/projects/tag/carbon."
  },
  {
    "objectID": "blog/2023-12-1-ethics-proj/index.html#footnotes",
    "href": "blog/2023-12-1-ethics-proj/index.html#footnotes",
    "title": "The AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.carbonvisuals.com/projects/tag/carbon↩︎"
  },
  {
    "objectID": "about.html#my-photo-collage",
    "href": "about.html#my-photo-collage",
    "title": "About",
    "section": "My Photo Collage",
    "text": "My Photo Collage\n&lt;div class=“collage”&gt; &lt;img src=“” images/chef.png alt=“Description of image 1”&gt;  &lt;/div&gt;"
  },
  {
    "objectID": "cooking.html",
    "href": "cooking.html",
    "title": "Cooking",
    "section": "",
    "text": "Follow my culinary adventures on Instagram for more delicious content: Chef Xwell on Instagram"
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html",
    "href": "blog/2023-12-1-stats-proj/index.html",
    "title": "Stats Project",
    "section": "",
    "text": "I’ll be investigating economic freedom. The United States, China, Russia, and the EU will be the primary analysis of the project.\nFirst, let’s import the libraries we will need to conduct this analysis.\n\n# import libraries\nlibrary(here)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(tidyr)\nlibrary(MASS)\nlibrary(randomForest)\n\nNow, let’s import and clean the data containing the freedom index and other scores by country.\n\n# read in freedom data\nfreedom_raw &lt;- read.csv('data/efw_ratings.csv', header = FALSE)\n\nThis data needs a good bit of clean up.\nFiltering for a few countries I want to analyze will make analysis more convenient.\n\nfreedom_us_china_russia &lt;- freedom %&gt;% \n  filter(countries %in% c('United States', 'China', 'Russian Federation'))\n\nfreedom_capitalism &lt;- freedom %&gt;% \n  filter(countries %in% c('United States', 'Singapore', 'Hong Kong SAR, China', 'Switzerland', 'New Zealand', 'Australia', 'United Kingdom', 'Ireland', 'Netherlands'))\n\nfreedom_anticapitalism &lt;- freedom %&gt;% \n  filter(countries %in% c('Sweden', 'Norway', 'Finland', 'Denmark', 'France', 'Belgium', 'Austria', 'Germany', 'Italy'))\n\nNow, lets move on to entering the next dataset. This freedom data serves as the policy side of the data – now we want to append and compare environmental outcomes based on different political and economic factors.\n\n# read in esg data\nesg_wb &lt;- read.csv('data/esg_wb.csv') %&gt;% \n  clean_names()\n\nThis data also needs to be cleaned up a bit. Let’s get to work.\n\ncolumn_names &lt;- c(\"x1998_yr1998\", \"x1999_yr1999\", \"x2000_yr2000\", \n                  \"x2001_yr2001\", \"x2002_yr2002\", \"x2003_yr2003\", \n                  \"x2004_yr2004\", \"x2005_yr2005\", \"x2006_yr2006\", \n                  \"x2007_yr2007\", \"x2008_yr2008\", \"x2009_yr2009\", \n                  \"x2010_yr2010\", \"x2011_yr2011\", \"x2012_yr2012\", \n                  \"x2013_yr2013\", \"x2014_yr2014\", \"x2015_yr2015\", \n                  \"x2016_yr2016\", \"x2017_yr2017\", \"x2018_yr2018\", \n                  \"x2019_yr2019\", \"x2020_yr2020\", \"x2021_yr2021\", \n                  \"x2022_yr2022\")\n\n# Function to extract and convert the year part of a column name to numeric\nextract_year &lt;- function(column_name) {\n  year_str &lt;- substr(column_name, 2, 5)\n  as.numeric(year_str)\n}\n\nfirst &lt;- names(esg_wb)[1:4]\n\n# Apply the function to each column name\nnumeric_years &lt;- sapply(column_names, extract_year)\n\nnew_cols &lt;- c(first, numeric_years)\n\nnames(esg_wb) &lt;- new_cols\n\nesg_wb &lt;- esg_wb %&gt;% \n  mutate(across(5:ncol(.), ~ as.numeric(as.character(.))))\n\n\n# filtering for each series name in esg data\nagricultural_land_subset &lt;- filter(esg_wb, series_name == \"Agricultural land (% of land area)\")\n\nnet_forest_depletion_subset &lt;- filter(esg_wb, series_name == \"Adjusted savings: net forest depletion (% of GNI)\")\n\ncoastal_protection_subset &lt;- filter(esg_wb, series_name == \"Coastal protection\")\n\nwater_stress_level_subset &lt;- filter(esg_wb, series_name == \"Level of water stress: freshwater withdrawal as a proportion of available freshwater resources\")\n\nmethane_emissions_subset &lt;- filter(esg_wb, series_name == \"Methane emissions (metric tons of CO2 equivalent per capita)\")\n\nrenewable_energy_consumption_subset &lt;- filter(esg_wb, series_name == \"Renewable energy consumption (% of total final energy consumption)\")\n\nrenewable_electricity_output_subset &lt;- filter(esg_wb, series_name == \"Renewable electricity output (% of total electricity output)\")\n\nunemployment_total_subset &lt;- filter(esg_wb, series_name == \"Unemployment, total (% of total labor force) (modeled ILO estimate)\")"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html",
    "href": "blog/2023-12-2-geo-proj/index.html",
    "title": "Geospatial Project",
    "section": "",
    "text": "In the heart of winter, the city of Houston found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis reeling, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines.\n\n\n\nUrban blackouts are more than mere inconveniences; they’re a stark reminder of our dependence on stable power sources. In Houston, a city with a complex infrastructure and a propensity for extreme weather events, the susceptibility to blackouts raises critical concerns about preparedness and resilience. A study by the U.S. Energy Information Administration highlights Houston’s energy landscape, noting its significant role in national energy supply but also its vulnerability to disruptions. This backdrop sets the stage for our analysis of the 2021 blackout, a pivotal moment that not only challenged the city’s infrastructure but also tested the resilience of its communities.\n\n\n\nThe analysis hinges on three key data sources: VIIRS night light data, OpenStreetMap data detailing roads and buildings, and American Community Survey (ACS) socio-economic data. We employed R, utilizing libraries such as dplyr, sf, and terra, to manipulate and analyze these datasets. Our methodology involved a step-by-step geospatial approach, beginning with the aggregation of night light intensity data to assess blackout areas. We then integrated this with road and building data to estimate the number of homes affected and concluded with an examination of socio-economic factors, offering insights into the recovery dynamics of different communities.\nLet’s get to coding.\nStarting off by loading in the necessary libraries.\n\n# import libraries\nlibrary(dplyr)\nlibrary(abind)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stars)\nlibrary(here)\n\nNext, importing data. The first set of data is the night light data, tiles or images that capture the light intensity emanating from the surface of the planet during nighttime. The data is stored as rasters, and the stars package is used for raster handling.\n\n\n\n\nNight Lights Intensity Maps:\n\nPresent the visual comparison of night lights before and after the blackout\n\nBlackout Impact Analysis:\n\nShow maps identifying areas affected by the blackout\nDiscuss any interesting patterns or findings\n\nSocio-Economic Analysis:\n\nDisplay the box plot of median incomes and discuss socio-economic disparities\n\n\n\n\n\n\nSummarize the key findings of your analysis\nDiscuss the implications of these findings on urban planning and emergency management\nAcknowledge any limitations of your study\nSuggest potential areas for future research\n\n\n\n\n\nList all references used in your analysis, formatted appropriately.\n\n\n\n\n\nInclude any additional information, code snippets, or extended data visualizations."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#introduction",
    "href": "blog/2023-12-2-geo-proj/index.html#introduction",
    "title": "Geospatial Project",
    "section": "",
    "text": "In the heart of winter, the city of Houston found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis reeling, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#background",
    "href": "blog/2023-12-2-geo-proj/index.html#background",
    "title": "Geospatial Project",
    "section": "",
    "text": "Urban blackouts are more than mere inconveniences; they’re a stark reminder of our dependence on stable power sources. In Houston, a city with a complex infrastructure and a propensity for extreme weather events, the susceptibility to blackouts raises critical concerns about preparedness and resilience. A study by the U.S. Energy Information Administration highlights Houston’s energy landscape, noting its significant role in national energy supply but also its vulnerability to disruptions. This backdrop sets the stage for our analysis of the 2021 blackout, a pivotal moment that not only challenged the city’s infrastructure but also tested the resilience of its communities."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "href": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "title": "Geospatial Project",
    "section": "",
    "text": "The analysis hinges on three key data sources: VIIRS night light data, OpenStreetMap data detailing roads and buildings, and American Community Survey (ACS) socio-economic data. We employed R, utilizing libraries such as dplyr, sf, and terra, to manipulate and analyze these datasets. Our methodology involved a step-by-step geospatial approach, beginning with the aggregation of night light intensity data to assess blackout areas. We then integrated this with road and building data to estimate the number of homes affected and concluded with an examination of socio-economic factors, offering insights into the recovery dynamics of different communities.\nLet’s get to coding.\nStarting off by loading in the necessary libraries.\n\n# import libraries\nlibrary(dplyr)\nlibrary(abind)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stars)\nlibrary(here)\n\nNext, importing data. The first set of data is the night light data, tiles or images that capture the light intensity emanating from the surface of the planet during nighttime. The data is stored as rasters, and the stars package is used for raster handling."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "href": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "title": "Geospatial Project",
    "section": "",
    "text": "Night Lights Intensity Maps:\n\nPresent the visual comparison of night lights before and after the blackout\n\nBlackout Impact Analysis:\n\nShow maps identifying areas affected by the blackout\nDiscuss any interesting patterns or findings\n\nSocio-Economic Analysis:\n\nDisplay the box plot of median incomes and discuss socio-economic disparities"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#conclusions-and-discussion",
    "href": "blog/2023-12-2-geo-proj/index.html#conclusions-and-discussion",
    "title": "Geospatial Project",
    "section": "",
    "text": "Summarize the key findings of your analysis\nDiscuss the implications of these findings on urban planning and emergency management\nAcknowledge any limitations of your study\nSuggest potential areas for future research"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#references",
    "href": "blog/2023-12-2-geo-proj/index.html#references",
    "title": "Geospatial Project",
    "section": "",
    "text": "List all references used in your analysis, formatted appropriately."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#appendix-optional",
    "href": "blog/2023-12-2-geo-proj/index.html#appendix-optional",
    "title": "Geospatial Project",
    "section": "",
    "text": "Include any additional information, code snippets, or extended data visualizations."
  },
  {
    "objectID": "basketball.html",
    "href": "basketball.html",
    "title": "Basketball",
    "section": "",
    "text": "This is a test page with some analysis.\n\nlibrary(here)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\ndec2_data &lt;- read.csv(here('data/12-3-fantrax-stats.csv')) %&gt;% \n  clean_names()\n\nLet’s look at the average fantasy points per game of the top 20 scorers on each fantasy team\n\nfantasy_team_stats_top20 &lt;- dec2_data %&gt;%\n  group_by(status) %&gt;%\n  arrange(desc(fp_g)) %&gt;%\n  slice_max(order_by = fp_g, n = 20) %&gt;%\n  summarise(mean_fps_game = mean(fp_g),\n            age_top_20 = mean(age)) %&gt;% \n  arrange(desc(mean_fps_game))\n\nfantasy_team_stats_top20\n\n# A tibble: 15 × 3\n   status   mean_fps_game age_top_20\n   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n 1 BBB               33.5       30.7\n 2 Jmarr237          33.4       29.4\n 3 STARKS            31.5       28  \n 4 CCC               31.4       26.4\n 5 maxpat01          30.4       27.4\n 6 GBRAYERS          29.6       27.0\n 7 VSL               28.7       25.0\n 8 HHBC              28.5       26.0\n 9 Orcas             28.4       24  \n10 BIGFOOTS          25.4       24.2\n11 SERP              24.4       22.3\n12 SDP               23.2       24.2\n13 FA                15.7       28.2\n14 W (Mon)           13.2       20  \n15 W (Sun)           11.4       28.5\n\n\nWhich teams have drafted the best to win this season based on ADP?\n\ndec2_data$adp &lt;- as.numeric(dec2_data$adp)\n\nWarning: NAs introduced by coercion\n\ntop10_adp &lt;- dec2_data %&gt;% \n  group_by(status) %&gt;% \n  arrange(desc(fp_g)) %&gt;%\n  slice_max(order_by = fp_g, n = 6) %&gt;%\n  summarize(avg_adp = mean(adp),\n            st_adp = sd(adp),\n            avg_fp_g = mean(fp_g),\n            avg_age = mean(age))\n\ntop10_adp\n\n# A tibble: 15 × 5\n   status   avg_adp st_adp avg_fp_g avg_age\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 BBB         26.8   24.4     47.7    30  \n 2 BIGFOOTS   101.    61.6     35.9    23.7\n 3 CCC         47.6   42.2     42.6    28.7\n 4 FA         663.   520.      17.4    27.3\n 5 GBRAYERS    55.3   35.6     40.5    24.3\n 6 HHBC        50.3   32.2     42.2    26.2\n 7 Jmarr237    29.1   24.1     45.2    31.8\n 8 Orcas       90.2   80.0     42.3    23.7\n 9 SDP         52.5   29.9     36.5    22.3\n10 SERP        82.7   47.6     34.1    20.8\n11 STARKS      41.2   31.3     41.8    27.2\n12 VSL         57.8   56.1     40.4    25  \n13 W (Mon)    426.    NA       13.2    20  \n14 W (Sun)    810.   497.      11.4    28.5\n15 maxpat01    31.4   26.2     45.8    28.3\n\n\n\nteam_data &lt;- data.frame(\n  status = c(\"BBB\", \"BIGFOOTS\", \"CCC\", \"FA\", \"GBRAYERS\", \"HHBC\", \"Jmarr237\", \"Orcas\", \"SDP\", \"SERP\", \"STARKS\", \"VSL\", \"W (Mon)\", \"W (Sun)\", \"maxpat01\"),\n  avg_adp = c(36.444, 132.474, 73.996, 887.022, 74.268, 75.587, 41.462, 93.073, 113.332, 87.753, 61.734, 76.725, 425.520, 809.605, 88.989),\n  st_adp = c(23.61738, 79.90325, 51.43987, 484.52501, 38.97783, 43.63397, 28.84837, 66.76798, 102.91086, 39.61803, 44.00860, 58.68209, NA, 496.99000, 119.87338),\n  avg_fp_g = c(43.045, 31.145, 38.084, 16.635, 36.632, 37.360, 40.824, 36.683, 31.203, 30.979, 38.892, 36.071, 13.160, 11.360, 39.277),\n  avg_age = c(31.1, 24.4, 28.1, 27.4, 25.4, 27.2, 29.9, 23.9, 25.4, 22.6, 27.3, 24.8, 20.0, 28.5, 29.9)\n)\n\n\n# Normalize avg_adp, avg_fp_g, and avg_age\nteam_data &lt;- top10_adp %&gt;%\n  mutate(\n    norm_avg_adp = (avg_adp - min(avg_adp)) / (max(avg_adp) - min(avg_adp)),\n    norm_avg_fp_g = (avg_fp_g - min(avg_fp_g)) / (max(avg_fp_g) - min(avg_fp_g)),\n    norm_avg_age = (avg_age - min(avg_age)) / (max(avg_age) - min(avg_age))\n  )\n\n# Constants for scaling and weight\nepsilon = 0.01  # To avoid division by zero\nweight_adp = 0.2  # Weight for avg_adp\nweight_fp_g = 0.6  # Weight for avg_fp_g\nweight_age = 0.2  # Weight for avg_age\n\n# Calculate Win Now Score\nteam_data &lt;- team_data %&gt;%\n  mutate(\n    win_now_score = ((1 / (norm_avg_adp + epsilon)) * weight_adp) + \n                    (norm_avg_fp_g * weight_fp_g) +\n                    ((1 / (norm_avg_age + epsilon)) * weight_age)\n  )\n\n# View the dataframe with Win Now Scores\nprint(team_data)\n\n# A tibble: 15 × 9\n   status   avg_adp st_adp avg_fp_g avg_age norm_avg_adp norm_avg_fp_g\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 BBB         26.8   24.4     47.7    30        0              1     \n 2 BIGFOOTS   101.    61.6     35.9    23.7      0.0944         0.675 \n 3 CCC         47.6   42.2     42.6    28.7      0.0267         0.860 \n 4 FA         663.   520.      17.4    27.3      0.812          0.166 \n 5 GBRAYERS    55.3   35.6     40.5    24.3      0.0365         0.802 \n 6 HHBC        50.3   32.2     42.2    26.2      0.0300         0.849 \n 7 Jmarr237    29.1   24.1     45.2    31.8      0.00302        0.931 \n 8 Orcas       90.2   80.0     42.3    23.7      0.0810         0.851 \n 9 SDP         52.5   29.9     36.5    22.3      0.0328         0.691 \n10 SERP        82.7   47.6     34.1    20.8      0.0714         0.626 \n11 STARKS      41.2   31.3     41.8    27.2      0.0184         0.838 \n12 VSL         57.8   56.1     40.4    25        0.0396         0.799 \n13 W (Mon)    426.    NA       13.2    20        0.509          0.0495\n14 W (Sun)    810.   497.      11.4    28.5      1              0     \n15 maxpat01    31.4   26.2     45.8    28.3      0.00591        0.947 \n# ℹ 2 more variables: norm_avg_age &lt;dbl&gt;, win_now_score &lt;dbl&gt;\n\n\nCool! Now, let’s look at the most promising young players in this dynasty format.\n\nunder25 &lt;- dec2_data %&gt;% \n  filter(age &lt; 25, fp_g &gt; 20)\n\nIt would be neat to figure out the value of a future pick using trade information.\nLet’s make a trade function\n\n# read in data\ndec3_trades &lt;- read.csv(here('data/12-3-trades.csv'))\n\n\n# trade log\ntrades_df &lt;- data.frame(\n  From_Team = character(), \n  To_Team = character(), \n  Date = character(), \n  Period = integer(), \n  Item1 = character(), \n  Item2 = character(),\n  stringsAsFactors = FALSE\n)\n\n\n# # Assuming you've read your CSV file into a dataframe called 'csv_trades'\n# for (i in 1:nrow(dec3_trades)) {\n#   trades_df &lt;- add_trade(\n#     trades_df, \n#     dec3_trades$Player[i], \n#     dec3_trades$Team[i], \n#     dec3_trades$Position[i], \n#     dec3_trades$From[i], \n#     dec3_trades$To[i], \n#     dec3_trades$`Date (PST)`[i], \n#     dec3_trades$Period[i]\n#   )\n# }\n# \n# add_trade &lt;- function(trade_df, player, team, position, from_team, to_team, date, period) {\n#   new_trade &lt;- data.frame(\n#     Player = player,\n#     Team = team,\n#     Position = position,\n#     From = from_team,\n#     To = to_team,\n#     Date = date,\n#     Period = period,\n#     stringsAsFactors = FALSE\n#   )\n#   \n#   updated_trades_df &lt;- rbind(trade_df, new_trade)\n#   return(updated_trades_df)\n# }\n# \n#"
  }
]