[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Fantasy Basketball 9 Category Simulations\n\n\nAn NBA API Investigation\n\n\n\nMaxwell Patterson\n\n\nMay 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Word Embeddings with Kern County Oil News Articles\n\n\n\n\n\n\nMaxwell Patterson\n\n\nMay 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey Through Math\n\n\n\n\n\n\nMaxwell Patterson\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Failure of Agricultural Subsidies: How Government Policies are Harming Our Health and the Environment\n\n\n\n\n\n\nMaxwell Patterson\n\n\nApr 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Analysis: Biodiversity Loss\n\n\n\n\n\n\nMaxwell Patterson\n\n\nApr 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Dissolved Inorganic Carbon with Machine Learning\n\n\n\n\n\n\nMaxwell Patterson\n\n\nApr 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Dynamics of Dynasty: A Fantasy Basketball Visual Exploration\n\n\nVisualizing the dynamics between fantasy scoring and age in the A.S.S. League\n\n\n\nMaxwell Patterson\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Geospatial Analysis of Houston’s Blackout Sensitivity\n\n\n\n\n\n\nMaxwell Patterson\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Dynamic Between Economic Freedom and Environmental Outcomes\n\n\nA Statistical Climate Investigation\n\n\n\nMaxwell Patterson\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire and Air Quality Inspection\n\n\nA geospatial analysis of Santa Barbara wildfire\n\n\n\nMaxwell Patterson\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact\n\n\n\n\n\n\nMaxwell Patterson\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Expressivity of Neural Networks\n\n\n\n\n\n\nMaxwell Patterson\n\n\nMay 11, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023-12-7-neural-net-exp/index.html",
    "href": "blog/2023-12-7-neural-net-exp/index.html",
    "title": "The Expressivity of Neural Networks",
    "section": "",
    "text": "Networks are everywhere\n\n\nHere’s a project I did in my Pattern Theory course in the spring semester of 2023. Note that there are a few typos in the project.\nSee below for the project PDF.\n\nThis browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"exp_of_nn.pdf\"&gt;Download PDF&lt;/a&gt;"
  },
  {
    "objectID": "blog/2024-2-28-data-viz-proj/index.html",
    "href": "blog/2024-2-28-data-viz-proj/index.html",
    "title": "The Dynamics of Dynasty: A Fantasy Basketball Visual Exploration",
    "section": "",
    "text": "In this blog post, I will be diving into some fantasy basketball data visualizations! One of our classes this quarter, Data Visualization and Communication (EDS240), has opened my eyes to how critical data communication is through visual means, how the small changes that an observer might not bat a second glance at can change the entire gist of a visual, and how narrative is imperative in creating a visual for a specific (or general) audience. Building beautiful, crisp visualizations takes time and attention to details, and the class has been a fun deep dive on the different layers that go into building these kind of visuals.\nA.L. Linall Jr frames the power of visualization well: “Visualization and belief in a pattern of reality activates the creative power of realization.” Interpreting raw data is not fun for anyone, but staring at a beautiful scatter plot, stacked area plot, ridge line plot, or heat map can make the process of digesting data more enjoyable. Further, a detailed and aesthetically pleasing visual can guide the observer towards a key takeaway that the creator intended. It is almost impossible to remove all biases and keep all of the important information from a dataset when creating a visual. Therefore, it is important to keep in mind what story the visualization is feeding you into as a consumer.\nFor this final assignment, I will be constructing three different visuals that investigate the dynamic of a dynasty format fantasy basketball league that I am in. Each visual will be tailored for a certain audience: one for a general audience without domain expertise, one that would go well in a presentation, and one for domain experts, or people who know a lot about how dynasty leagues work.\n\n\nI have been in this fantasy basketball league for a few years, but we just switched over to a dynasty format this offseason. Dynasty league formats exist for all fantasy sports, and they are leagues where managers keep their entire roster each season. Most fantasy leagues are redraft format, where there is a full draft each year where teams draft entirely new rosters. In the dynasty format, there is simply a rookie draft where managers only select from the rookie player pool to keep building their rosters. In this sense, a dynasty league is more like being a GM of an NBA team, where players are kept year over year and new players are acquired through trades or the rookie draft. Personally, I have loved this format as it completely changes the dynamic of assessing a player’s value. More on this later.\nStarting a dynasty league can be tricky. The most important distinction between redraft versus dynasty leagues is that all league members must be in it for the long haul - having a team bail after a year or two, for whatever reason, can cause issues if the league is not able to find a replacement. Also, some people might not be as excited about the dynasty format because they may not ever have the opportunity to draft certain players, especially top-end young players, because their value is higher in dynasty formats than redraft leagues. While the dynasty format is not for everyone, I am lucky to be in a league where everyone is engaged, active, and excited about the format. As rookie league member Danny put it, “What other league gets reprimanded for talking too much ball at work?”.\nLeading up to the draft, I think there was a balance of teams that wanted to draft young players and compete for a championship in future years and teams that wanted to get players that could help them be competitive this season. This played out accordingly on draft night, with only two or three teams going young player heavy while the rest of the league went with a more diverse selection of young and older players. However, as the season went on, a few teams really pivoted their dynasty strategy (especially my team) to get younger due to how strong the top of the league is this season. The Bikini Bottom Ballers solidified themselves as one of the greatest dynasty rosters ever assembled, ending the season with a 17-1 record and having over 4000 more fantasy points than the second closest team. During the season, the Ballers acquired even more win-now talent to add on to the players and now have a roster full of win-now players: Kevin Durant, Anthony Davis, Giannis Antetokounmpo, Kawhi Leonard, and Kyrie Irving to name a few. The Ballers are the clear favorites to win the championship this season, but it will be interesting to see how long their reign over the league lasts with many of their players being older and beginning to decline. Anyways, I’m getting ahead of myself as these will be investigated in the following visuals.\nFor more context, my roster transitioned from win-now to building for the future over the course of the season. I had some talented older players on my roster in the beginning of the season, like Kevin Durant and Paul George. Once I realized that the Ballers had such a strong roster that would be hard to beat come playoff time, I made a bunch of trades to build around my teams cornerstone, and arguably the most valuable dynasty asset of all time: Victor Wembanyama. While he is just a rookie, Wemby has put up some insane numbers this season and is looking like he will be a top player as early as next season. Wemby is only 20 years old and is already averaging over 50 fantasy points per game, vaulting himself into unprecedented terrain when it comes to rookie fantasy performance in the modern era. In a recent league survey, the Santa Barbara Swell (my team, by the way the team names and logos in the league are pretty epic) was voted as the team that others would most want to swap rosters with. What an honor!"
  },
  {
    "objectID": "blog/2024-2-28-data-viz-proj/index.html#introduction",
    "href": "blog/2024-2-28-data-viz-proj/index.html#introduction",
    "title": "The Dynamics of Dynasty: A Fantasy Basketball Visual Exploration",
    "section": "",
    "text": "In this blog post, I will be diving into some fantasy basketball data visualizations! One of our classes this quarter, Data Visualization and Communication (EDS240), has opened my eyes to how critical data communication is through visual means, how the small changes that an observer might not bat a second glance at can change the entire gist of a visual, and how narrative is imperative in creating a visual for a specific (or general) audience. Building beautiful, crisp visualizations takes time and attention to details, and the class has been a fun deep dive on the different layers that go into building these kind of visuals.\nA.L. Linall Jr frames the power of visualization well: “Visualization and belief in a pattern of reality activates the creative power of realization.” Interpreting raw data is not fun for anyone, but staring at a beautiful scatter plot, stacked area plot, ridge line plot, or heat map can make the process of digesting data more enjoyable. Further, a detailed and aesthetically pleasing visual can guide the observer towards a key takeaway that the creator intended. It is almost impossible to remove all biases and keep all of the important information from a dataset when creating a visual. Therefore, it is important to keep in mind what story the visualization is feeding you into as a consumer.\nFor this final assignment, I will be constructing three different visuals that investigate the dynamic of a dynasty format fantasy basketball league that I am in. Each visual will be tailored for a certain audience: one for a general audience without domain expertise, one that would go well in a presentation, and one for domain experts, or people who know a lot about how dynasty leagues work.\n\n\nI have been in this fantasy basketball league for a few years, but we just switched over to a dynasty format this offseason. Dynasty league formats exist for all fantasy sports, and they are leagues where managers keep their entire roster each season. Most fantasy leagues are redraft format, where there is a full draft each year where teams draft entirely new rosters. In the dynasty format, there is simply a rookie draft where managers only select from the rookie player pool to keep building their rosters. In this sense, a dynasty league is more like being a GM of an NBA team, where players are kept year over year and new players are acquired through trades or the rookie draft. Personally, I have loved this format as it completely changes the dynamic of assessing a player’s value. More on this later.\nStarting a dynasty league can be tricky. The most important distinction between redraft versus dynasty leagues is that all league members must be in it for the long haul - having a team bail after a year or two, for whatever reason, can cause issues if the league is not able to find a replacement. Also, some people might not be as excited about the dynasty format because they may not ever have the opportunity to draft certain players, especially top-end young players, because their value is higher in dynasty formats than redraft leagues. While the dynasty format is not for everyone, I am lucky to be in a league where everyone is engaged, active, and excited about the format. As rookie league member Danny put it, “What other league gets reprimanded for talking too much ball at work?”.\nLeading up to the draft, I think there was a balance of teams that wanted to draft young players and compete for a championship in future years and teams that wanted to get players that could help them be competitive this season. This played out accordingly on draft night, with only two or three teams going young player heavy while the rest of the league went with a more diverse selection of young and older players. However, as the season went on, a few teams really pivoted their dynasty strategy (especially my team) to get younger due to how strong the top of the league is this season. The Bikini Bottom Ballers solidified themselves as one of the greatest dynasty rosters ever assembled, ending the season with a 17-1 record and having over 4000 more fantasy points than the second closest team. During the season, the Ballers acquired even more win-now talent to add on to the players and now have a roster full of win-now players: Kevin Durant, Anthony Davis, Giannis Antetokounmpo, Kawhi Leonard, and Kyrie Irving to name a few. The Ballers are the clear favorites to win the championship this season, but it will be interesting to see how long their reign over the league lasts with many of their players being older and beginning to decline. Anyways, I’m getting ahead of myself as these will be investigated in the following visuals.\nFor more context, my roster transitioned from win-now to building for the future over the course of the season. I had some talented older players on my roster in the beginning of the season, like Kevin Durant and Paul George. Once I realized that the Ballers had such a strong roster that would be hard to beat come playoff time, I made a bunch of trades to build around my teams cornerstone, and arguably the most valuable dynasty asset of all time: Victor Wembanyama. While he is just a rookie, Wemby has put up some insane numbers this season and is looking like he will be a top player as early as next season. Wemby is only 20 years old and is already averaging over 50 fantasy points per game, vaulting himself into unprecedented terrain when it comes to rookie fantasy performance in the modern era. In a recent league survey, the Santa Barbara Swell (my team, by the way the team names and logos in the league are pretty epic) was voted as the team that others would most want to swap rosters with. What an honor!"
  },
  {
    "objectID": "blog/2024-2-28-data-viz-proj/index.html#setting-the-scene",
    "href": "blog/2024-2-28-data-viz-proj/index.html#setting-the-scene",
    "title": "The Dynamics of Dynasty: A Fantasy Basketball Visual Exploration",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nQuestion: What are the dynamics between age and fantasy scoring across teams in the fantasy league? Which teams are going all in now, and which are looking to build rosters with talented younger players?\n\nData\nNow it’s time to do a little exploration to understand the data that is being worked with. The platform we use for the league, Fantrax, allows for easy downloading of data in CSV formats. I’ll start by importing the data, doing some simple cleaning so it will be ready for further wrangling and analysis. This code is hidden for the reader’s convenience so we can get right to the data exploration.\n\nColumns of Interest\nBelow is a list of the column names in the cleaned data set (rostered):\n\n\nCode\ncolnames(rostered)\n\n\n [1] \"player\"       \"team\"         \"position\"     \"rk_ov\"        \"fantasy_team\"\n [6] \"age\"          \"f_pts\"        \"fp_g\"         \"adp\"          \"fgm\"         \n[11] \"fga\"          \"x3ptm\"        \"ftm\"          \"fta\"          \"pts\"         \n[16] \"reb\"          \"ast\"          \"st\"           \"blk\"          \"to\"          \n[21] \"x3d\"          \"x2d\"         \n\n\nMost of the columns in the data are statistical categories, like points, rebounds, assists, turnovers, etc. The columns that are most important for this analysis are the age (age), fantasy scoring (f_pts and fp_g), and fantasy team (fantasy_team) columns. Throughout the analysis, summary tables utilizing information from these columns will be generated as needed.\n\n\nLimitations\nI was hoping that Fantrax would save the fantasy team that each player was on at a certain point in the season, but unfortunately this is not the case. For example, I traded for Jalen Duren right before the trade deadline on February 9th, but when downloading data from the first week of the season, it says that Duren was on my roster at this time. I was curious to see if I could track how fantasy rosters changed over the course of the season, but this reality makes that nearly impossible to do as I would have to enter data manually. Instead, I will focus on data from February 26th, which is just 2 weeks out of the fantasy playoffs and gives a good sense of where teams are at towards the end of the season in terms of roster construction.\n\n\n\nApproach\nFor the three graphs, I will be creating a scatter plot, a ridge line plot, and a sunburst plot for the domain expert, general audience, and presentation groups respectively. Each of the plots takes a different look at the dynamic between age and fantasy scoring across each of the fantasy teams. The motivation for choosing each specific graphic form will be discussed after each graph is shown in the next section.\nWhile I played around with different basketball related themes, I will be going with a simple background that is a similar color to NBA courts. Two graphs have all of the team logos (scatter and ridge line), and the sunburst plot is extremely colorful with lots going on, so I opted for keeping the background simple and minimizing distractions from the visualizations. I defined custom colors based on teams logos that are used in the sunburst plot to add to the aesthetics of the visual as there are lots of colors going on. Additionally, I chose the Courier font as the go-to for all three visuals, as it is a crisp and easy-going font that has just the right amount of character.\nWhile DEI has been highlighted in our lecture materials and some other assignments in EDS 240, this analysis does not fit into the category of DEI application."
  },
  {
    "objectID": "blog/2024-2-28-data-viz-proj/index.html#analysis",
    "href": "blog/2024-2-28-data-viz-proj/index.html#analysis",
    "title": "The Dynamics of Dynasty: A Fantasy Basketball Visual Exploration",
    "section": "Analysis",
    "text": "Analysis\n\nPlot 1: Scatter plot\n\n\nCode\n# Importing libraries\nlibrary(ggimage)\nlibrary(ggtext) # Set font for annotations\n\n# Rank players within each team by their fp_g and filter for the top 10 assets\ntop_players &lt;- rostered %&gt;%\n  group_by(fantasy_team) %&gt;%\n  filter(player %in% top_assets)\n\n\n# Calculate average age and fantasy points per game by fantasy team (top 10 assets)\nteam_averages &lt;- top_players %&gt;%\n  group_by(fantasy_team) %&gt;%\n  summarise(\n    AverageAge = mean(age, na.rm = TRUE),\n    AverageFpG = mean(fp_g, na.rm = TRUE)\n  ) \n\n# Add logo and logo path for each team\nteam_averages$logo_path &lt;- c(\"images/orcas_logo.png\", \"images/bigfoots_logo.png\",\n                             \"images/ballers_logo.png\", \"images/crusaders_logo.png\",\n                             \"images/hounds_logo.png\", \"images/scorpions_logo.png\",\n                             \"images/milkers_logo.png\", \"images/pilots_logo.png\",\n                             \"images/swell_logo.png\", \"images/stotches_logo.png\",\n                             \"images/serpents_logo.png\", \"images/starks_logo.png\")\n\n# Create scatter plot base\nscatter &lt;- ggplot(team_averages, aes(x = AverageAge, y = AverageFpG)) +\n  geom_image(aes(image = logo_path), size = 0.09, alpha = 0.7) +\n  labs(\n    title = \"Fantasy Scoring versus Age of Top 10 Assets per Team\",\n    x = \"Average Age\",\n    y = \"Average Fantasy Points Per Game\"\n  ) +\n  theme_bw() +\n  theme(\n    text = element_text(family = \"Courier\"),\n    plot.background = element_rect(fill = \"antiquewhite\"), # Set background color\n    panel.background = element_rect(fill = \"antiquewhite\"), # Match panel background \n    panel.grid.major = element_line(color = \"#f7f7f7\"), # Grid lines fainter color\n    panel.grid.minor = element_line(colour = \"#f7f7f7\"), # Grid lines fainter color\n    axis.title.x = element_text(margin = margin(20, 0, 0, 0, \"pt\")), # Adds space\n    axis.title.y = element_text(margin = margin(0, 20, 0, 0, \"pt\")) # Adds space\n  ) + \n  expand_limits(x = c(min(team_averages$AverageAge) - 1, max(team_averages$AverageAge) + 1),\n                y = c(min(team_averages$AverageFpG) - 1, max(team_averages$AverageFpG) + 1)) # Stretch axes\n\n# Add annotations to scatter plot\nscatter +\n  geom_text( # BB text\n    x = 29.5,\n    y = 47.5,\n    label = \"One first round pick\",\n    size = 2.5,\n    color = \"black\",\n    hjust = \"inward\",\n    family = \"Courier\"\n  ) +\n  annotate( # BB arrow\n    geom = \"curve\",\n    x = 29.7, xend = 30.6,\n    y = 47.5, yend = 47.5,\n    curvature = -.02,\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_text( # MM text\n    x = 32,\n    y = 36.5,\n    label = \"Two first round picks\",\n    size = 2.5,\n    color = \"black\",\n    hjust = \"inward\",\n    family = \"Courier\"\n  ) +\n  annotate( # MM arrow\n    geom = \"curve\",\n    x = 31, xend = 30.65,\n    y = 36.8, yend = 41,\n    curvature = .6,\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_text( # SB text\n    x = 20.5,\n    y = 36.65,\n    label = \"Seven first round picks\",\n    size = 2.5,\n    color = \"black\",\n    hjust = \"inward\",\n    family = \"Courier\"\n  ) +\n  annotate( # SB arrow\n    geom = \"curve\",\n    x = 22.6, xend = 23.2,\n    y = 35.6, yend = 33,\n    curvature = -0.55,\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_text( # CC text\n    x = 29.2,\n    y = 35.4,\n    label = \"One first round pick\",\n    size = 2.5,\n    color = \"black\",\n    hjust = \"inward\",\n    family = \"Courier\"\n  ) +\n  annotate( # CC arrow\n    geom = \"curve\",\n    x = 28.2, xend = 27.35,\n    y = 35.9, yend = 39.1,\n    curvature = 0.5,\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_text( # SD text\n    x = 23.2,\n    y = 42,\n    label = \"Two first round picks\",\n    size = 2.5,\n    color = \"black\",\n    hjust = \"inward\",\n    family = \"Courier\"\n  ) +\n  annotate( # SD arrow\n    geom = \"curve\",\n    x = 24.7, xend = 25.22,\n    y = 41.1, yend = 38,\n    curvature = -0.3,\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    family = \"Courier\"\n  )  \n\n\n\n\n\nThis plot investigates the average age and fantasy points per game of the top 10 assets on each fantasy team. While there is no perfect number of assets to choose (5, 10, 15), there are 10 starting spots for each night of fantasy play, so it seemed like an appropriate choice. The plot reveals that teams whose top assets are older tend to have higher average fantasy points scoring. A scatter plot is a great plot to visualize this relationship as it clearly displays each team’s top assets as individual points, allowing for the direct observation of the relationship between average age and fantasy points per game. By representing each team’s top assets as individual data points, the scatter plot allows for a direct examination of how age and fantasy points per game are related across the league. The x-axis represents the average age of a team’s top 10 assets, providing a clear measure of the overall experience and veteran presence on each roster. The y-axis displays the average fantasy points per game for those top assets, quantifying the offensive output and statistical production of each team’s core players.\nOne of the key strengths of using a scatter plot in this context is its ability to reveal patterns, trends, and potential outliers in the data. The plot clearly shows a clustering of teams in the upper-right quadrant, indicating that teams with older top assets tend to have higher fantasy points per game averages. This visual representation makes it easy to identify the teams that have prioritized veteran talent and are currently enjoying the benefits of experienced, productive players Furthermore, the scatter plot allows for a quick assessment of the spread and distribution of teams along both dimensions. The plot shows a considerable range in both average age and fantasy points per game, highlighting the diversity of roster construction strategies employed by different teams. Some teams have opted for a mix of younger and older players, while others have focused primarily on either end of the age spectrum.\nAnnotations are added to provide more context on the draft capital of specific teams that have an interesting number of first round picks. In the beginning of the season, each team had 3 first round picks: 2025, 2026, and 2027 (or the next 3 years for the case of future seasons). Teams can trade these picks around, which is a big part of what makes fantasy leagues exciting and unique from redraft leagues. The best teams in the league tend to have less draft capital as they traded it away to acquire win-now players. Also of note, the Santa Barbara Swell owning 7 first round picks is likely to set the franchise up well in the long-term. Teams not labeled either have 3 or 4 first round picks over the next 3 years.\n\n\nPlot 2: Ridge line plot\n\n\nCode\n# Installing new packages\nlibrary(ggridges)\nlibrary(viridis)\n\n# Calculate the average age for each fantasy team\nteam_avg_age &lt;- rostered %&gt;%\n  group_by(fantasy_team) %&gt;%\n  summarise(AverageAge = mean(age, na.rm = TRUE))\n\n# Merge the average age data with full data\nrostered &lt;- rostered %&gt;%\n  left_join(team_avg_age, by = \"fantasy_team\")\n\n# Order the teams by average fp_g\nteam_order &lt;- rostered %&gt;%\n  group_by(fantasy_team) %&gt;%\n  summarise(AverageFpG = mean(fp_g, na.rm = TRUE)) %&gt;%\n  arrange(desc(AverageFpG)) %&gt;%\n  pull(fantasy_team)\n\n# Make sure data is ordered correctly for plotting\nrostered$fantasy_team &lt;- factor(rostered$fantasy_team, levels = team_order)\n\n# Define team logos for ridge plot \nlogos &lt;- data.frame(team = c(\"Anacortes\", \"Beaverton\", \"Bikini Bottom\",\n                             \"Cream City\", \"Helsinki\", \"Las Vegas\", \"Malibu\",\n                             \"San Diego\", \"Santa Barbara\", \"Scottsdale\", \n                             \"Slytherin\", \"Winterfell\"),\n                    logo = c(\"images/orcas_logo.png\", \"images/bigfoots_logo.png\", \n                             \"images/ballers_logo.png\", \"images/crusaders_logo.png\",\n                             \"images/hounds_logo.png\", \"images/scorpions_logo.png\",\n                             \"images/milkers_logo.png\", \"images/pilots_logo.png\", \n                             \"images/swell_logo.png\", \"images/stotches_logo.png\",\n                             \"images/serpents_logo.png\", \"images/starks_logo.png\"))\n\n# Adding new column for player images \nlogos$player &lt;- NA\nlogos$player_xcoord &lt;- NA \n\n# Adding player image and x-loc paths\nlogos[logos$team == \"Anacortes\", \"player\"] &lt;- \"images/embiid.png\"\nlogos[logos$team == \"Anacortes\", \"player_xcoord\"] &lt;- 71\nlogos[logos$team == \"Winterfell\", \"player\"] &lt;- \"images/luka.png\"\nlogos[logos$team == \"Winterfell\", \"player_xcoord\"] &lt;- 71\nlogos[logos$team == \"Cream City\", \"player\"] &lt;- \"images/jokic.png\"\nlogos[logos$team == \"Cream City\", \"player_xcoord\"] &lt;- 71\nlogos[logos$team == \"Santa Barbara\", \"player\"] &lt;- \"images/wemby.png\"\nlogos[logos$team == \"Santa Barbara\", \"player_xcoord\"] &lt;- 57\nlogos[logos$team == \"Scottsdale\", \"player\"] &lt;- \"images/hali.png\"\nlogos[logos$team == \"Scottsdale\", \"player_xcoord\"] &lt;- 61\nlogos[logos$team == \"Helsinki\", \"player\"] &lt;- \"images/sga.png\"\nlogos[logos$team == \"Helsinki\", \"player_xcoord\"] &lt;- 68\nlogos[logos$team == \"Las Vegas\", \"player\"] &lt;- \"images/tatum.png\"\nlogos[logos$team == \"Las Vegas\", \"player_xcoord\"] &lt;- 55\nlogos[logos$team == \"Slytherin\", \"player\"] &lt;- \"images/chet.png\"\nlogos[logos$team == \"Slytherin\", \"player_xcoord\"] &lt;- 52\nlogos[logos$team == \"Beaverton\", \"player\"] &lt;- \"images/fox.png\"\nlogos[logos$team == \"Beaverton\", \"player_xcoord\"] &lt;- 55\nlogos[logos$team == \"San Diego\", \"player\"] &lt;- \"images/book.png\"\nlogos[logos$team == \"San Diego\", \"player_xcoord\"] &lt;- 56\nlogos[logos$team == \"Bikini Bottom\", \"player\"] &lt;- \"images/giannis.png\"\nlogos[logos$team == \"Bikini Bottom\", \"player_xcoord\"] &lt;- 68\nlogos[logos$team == \"Malibu\", \"player\"] &lt;- \"images/lebron.png\"\nlogos[logos$team == \"Malibu\", \"player_xcoord\"] &lt;- 58\n\n# Create ridge plot\nridge_plot &lt;- ggplot(rostered, aes(x = fp_g, y = fantasy_team, fill = AverageAge)) + geom_density_ridges(\n    aes(height = ..density..), \n    alpha = 0.5, \n    scale = 2, \n    rel_min_height = 0.05,\n    trim = TRUE,\n    adjust = 0.2\n  ) +\n  scale_fill_viridis_c(name = \"Average Age\", option = \"A\") +\n  labs(title = \"Fantasy Scoring Distribution by Team\",\n       subtitle = \"Includes a heatshot of each team's best player\",\n       x = \"Fantasy Points per Game\",\n       y = \"\") +\n  theme_ridges(grid = FALSE) +\n  theme(\n    text = element_text(family = \"Courier\"),\n    plot.background = element_rect(fill = \"antiquewhite\"), \n    panel.background = element_blank(), \n    axis.title = element_text(size = 11), \n    axis.text.y = element_text(size = 10), \n    plot.subtitle = element_text(size = 9),\n    axis.title.x = element_text(margin = margin(20, 0, 0, 0, \"pt\")), # Adds space\n    axis.title.y = element_text(margin = margin(0, 20, 0, 0, \"pt\")) # Adds space\n  ) \n\n# Add team logos to ridge plot\nridge_plot + \n  geom_image(data = logos, aes(x = -12, y = team, image = logo), size = 0.07, inherit.aes = FALSE) +\n  geom_image(data = logos, aes(x = player_xcoord, y = team, image = player), size = 0.1, inherit.aes = FALSE) \n\n\n\n\n\nThis plot shows the fantasy points per game for players on each fantasy team. The visualization captures the distribution and density of fantasy points, highlighting the variance in player performance across different teams. It lets viewers easily compare the scoring potential and depth of talent on each fantasy team, allowing the ridge line plot to shine. A head shot of the best player on each fantasy team is added for additional context about each team. The ridges are ordered from lowest fantasy team on the top and highest fantasy scoring team on the bottom of the plot. A colorblind-friendly fill is used for average team age as well using the viridis package.\nSome key takeaways from this ridge line plot are:\n\nThe Bikini Bottom team has the highest concentration of top-end talent, with a wide and tall ridge indicating a roster filled with high-scoring players.\nThe Malibu team also has a strong presence of very good players, as evidenced by their ridge’s width and height.\nMost players in the league fall within the 20 to 30 fantasy points per game range, as seen by the majority of the density being concentrated in this area across all teams.\nTeams with more high-scoring players tend to have older rosters, as indicated by the darker ridge fills for teams like Bikini Bottom and Malibu.\n\nBy presenting the data in this way, the ridge line plot allows for a quick and easy comparison of the fantasy scoring distribution and roster composition across all teams in the league. It provides valuable insights into the strengths and weaknesses of each team, as well as the overall trends in player performance and age distribution within the league.\n\n\nPlot 3: Sunburst plot\n\n\nCode\n# Importing new libraries\nlibrary(data.table)\nlibrary(plotly)\nlibrary(tidyr)\nlibrary(scales)\n\n# Defining sunburst function\nas.sunburstDF &lt;- function(DF, value_column = NULL, add_root = FALSE){\n  require(data.table)\n  \n  if(is.data.table(DF)){\n    DT &lt;- copy(DF)\n  } else {\n    DT &lt;- data.table(DF, stringsAsFactors = FALSE)\n  }\n  \n  if(add_root){\n    DT[, root := \"Total\"]\n  }\n  \n  colNamesDT &lt;- names(DT)\n  hierarchy_columns &lt;- setdiff(colNamesDT, value_column)\n  DT[, (hierarchy_columns) := lapply(.SD, as.factor), .SDcols = hierarchy_columns]\n  \n  if(is.null(value_column)){\n    if(add_root){\n      setcolorder(DT, c(\"root\", names(DF)))\n    }\n  } else {\n    setnames(DT, value_column, \"values\", skip_absent=TRUE)\n    if(add_root){\n      setcolorder(DT, c(\"root\", setdiff(names(DF), value_column), \"values\"))\n    } else {\n      setcolorder(DT, c(setdiff(names(DF), value_column), \"values\"))\n    }\n  }\n  \n  hierarchyList &lt;- list()\n  for(i in seq_along(hierarchy_columns)){\n    current_columns &lt;- colNamesDT[1:i]\n    if(is.null(value_column)){\n      currentDT &lt;- unique(DT[, ..current_columns][, values := .N, by = current_columns], by = current_columns)\n    } else {\n      currentDT &lt;- DT[, lapply(.SD, sum, na.rm = TRUE), by=current_columns, .SDcols = \"values\"]\n    }\n    setnames(currentDT, length(current_columns), \"labels\")\n    hierarchyList[[i]] &lt;- currentDT\n  }\n  \n  hierarchyDT &lt;- rbindlist(hierarchyList, use.names = TRUE, fill = TRUE)\n  \n  parent_columns &lt;- setdiff(names(hierarchyDT), c(\"labels\", \"values\"))\n  hierarchyDT[, parents := apply(.SD, 1, function(x){ifelse(all(is.na(x)), NA_character_, paste(x[!is.na(x)], collapse = \" - \"))}), .SDcols = parent_columns]\n  hierarchyDT[, ids := apply(.SD, 1, function(x){paste(x[!is.na(x)], collapse = \" - \")}), .SDcols = c(\"parents\", \"labels\")]\n  hierarchyDT[, c(parent_columns) := NULL]\n  \n  return(hierarchyDT)\n}\n\n\nNote: the code for creating the sunburst function was adopted from https://rpubs.com/DragonflyStats/Sunburst-Plots-With-Plotly\n\n\nCode\n# Summarize total fantasy points by fantasy team\nteam_summary &lt;- rostered %&gt;%\n  group_by(fantasy_team) %&gt;%\n  summarize(fantasy_points = sum(f_pts, na.rm = TRUE), .groups = 'drop')\n\n# Convert the summary to the format required for a sunburst plot\nsunburst_data_teams_only &lt;- as.sunburstDF(team_summary, value_column = \"fantasy_points\", add_root = TRUE)\n\n# Create the sunburst plot for fantasy teams\nplot_ly(data = sunburst_data_teams_only,\n        ids = ~ids, \n        labels = ~labels, \n        parents = ~parents,\n        values = ~values,\n        type = 'sunburst',\n        branchvalues = 'total',\n        textinfo = 'label+values',\n        hoverinfo = 'label+percent parent+percent root',\n        insidetextorientation = 'radial'\n) %&gt;%\n  layout(title = \"Fantasy Scoring Distribution by Team\",\n         margin = list(t = 40, b = 10),\n         sunburstcolorway = c(\"#89AEAA\", \"#020818\", \"#F8DDA2\", \"#9F9F9F\", \"#102450\",\n                              \"#99C783\", \"#D93633\", \"#33ABC5\", \"#A1C7E3\", \"#008A79\",\n                              \"#F9CE84\", \"#8F0B19\"))\n\n\n\n\n\n\nThis plot breaks down the total fantasy scoring across the entire season by team. Let’s go one step further and break each team down by age group: under 25, 25-29, and over 30. This will provide more context on the construction of each fantasy team\n\n\nCode\n# Categorize each player into age group bin\nrostered &lt;- rostered %&gt;%\n  mutate(age_group = case_when(\n    age &lt; 25 ~ \"Under 25\",\n    age &gt;= 25 & age &lt;= 29 ~ \"25-29\",\n    age &gt; 29 ~ \"Over 30\"\n  ))\n\n# Group by fantasy team and age group, then summarize total fantasy points\nage_group_summary &lt;- rostered %&gt;%\n  group_by(fantasy_team, age_group) %&gt;%\n  summarize(fantasy_points = sum(f_pts, na.rm = TRUE), .groups = 'drop')\n\nsunburst_data &lt;- as.sunburstDF(age_group_summary, value_column = \"fantasy_points\", add_root = TRUE)\n\n# Creating color pallette \ncbbPalette &lt;- viridis(3, option = \"D\")\nn_colors &lt;- nrow(sunburst_data)  # Get the number of rows in sunburst_data\ncbbPalette &lt;- hue_pal()(n_colors)\n\nplot_ly(data = sunburst_data,\n             ids = ~ids, \n             labels = ~labels, \n             parents = ~parents,\n             values = ~values,\n             type = 'sunburst',\n             branchvalues = 'total',\n             textinfo = 'label',  # Include both label and values\n             hoverinfo = 'label+value+percent parent+percent root',  # Show label, value, and percentages on hover\n             insidetextorientation = 'radial'  # Set text orientation\n             ) %&gt;%\n  layout(title = \"Fantasy Scoring Distribution by Team and Age Group\",\n         margin = list(t = 40, b = 10),  # Adjust the top and bottom margins\n         sunburstcolorway = c(\"#89AEAA\", \"#020818\", \"#F8DDA2\", \"#9F9F9F\", \"#102450\",\n                              \"#99C783\", \"#D93633\", \"#33ABC5\", \"#A1C7E3\", \"#008A79\",\n                              \"#F9CE84\", \"#8F0B19\"))  # Starts with BB c-clockwise\n\n\n\n\n\n\nThis sunburst plot breaks down the total fantasy scoring in the league, by both team and then age groups within each team. This is the perfect opportunity to utilize a sunburst plot, since it effectively illustrates hierarchical relationships and proportions in a visually compact and intuitive manner. The sunburst plot’s outer ring shows age group contributions to total fantasy points for each team, highlighting the impact of age diversity on each fantasy roster. It distinguishes between teams relying on youth versus experience, efficiently mapping the league’s age and scoring dynamics in a single, clear visual. Text is easily integrated using plotly to reveal the percentage of total fantasy points in each section of the plot. The visualization also uses a custom color scheme to distinguish between teams and age groups.\nKey insights from the sunburst plot include:\n\nThe Bikini Bottom Ballers have the largest overall contribution to the league’s total fantasy points, with a significant portion coming from the “Over 30” age group.\nTeams like Slytherin and Las Vegas have a larger proportion of their fantasy points contributed by the “Under 25” age group, indicating a focus on younger talent.\nThe Cream City Crusaders have a significant portion of their fantasy points coming from the “25-29” age group, suggesting a strong core of players in their prime playing years.\n\nThe sunburst plot offers a comprehensive overview of the league’s fantasy scoring landscape, broken down by team and age group, allowing for quick identification of each team’s age composition."
  },
  {
    "objectID": "blog/2024-2-28-data-viz-proj/index.html#takeaways",
    "href": "blog/2024-2-28-data-viz-proj/index.html#takeaways",
    "title": "The Dynamics of Dynasty: A Fantasy Basketball Visual Exploration",
    "section": "Takeaways",
    "text": "Takeaways\nThe exploration of the A.S.S. League’s dynamics through these visualizations reveals some interesting insights into the dynamic between player age and fantasy performance, shedding light on strategic roster constructions across the league. Teams that have older players tend to be the higher fantasy scoring teams, while lower scoring teams are filled with young players who fantasy managers hope will continue to develop and increase their fantasy output. Notably, the Bikini Bottom Ballers’ dominance this season, powered by some elite assets, underscores the potential short-term merits (winning a championship) of investing in win-now talent. However, the Santa Barbara Swell’s forward-looking approach, amassing an impressive stack of first-round picks and building around a once-in-a-lifetime prospect in Victor Wembanyama, signals a promising focus towards long-term dominance. This strategy contrast underlines the league’s evolving nature, where teams aim for championships in distinct time frames. As we head into the fantasy playoffs, the unfolding narratives in the league will certainly add another thrilling chapter to the A.S.S. League’s storied history, with every team vying not just for this season’s glory but for a lasting legacy."
  },
  {
    "objectID": "blog/2024-4-28-food-subs/index.html",
    "href": "blog/2024-4-28-food-subs/index.html",
    "title": "The Failure of Agricultural Subsidies: How Government Policies are Harming Our Health and the Environment",
    "section": "",
    "text": "Introduction\nHave you ever stopped to think about how the food on your plate got there? Sure, you probably bought it at the grocery store or ordered it at a restaurant, but what influenced the availability and price of that food? The answer may surprise you: government agricultural subsidies play a significant role in shaping our food system. In this blog post, we’ll take an in-depth look at how these subsidies are allocated in the United States and compare them to policies in other countries. We’ll also explore the consequences of these subsidies on our health and the environment.\n\n\nThe Problem with Rent-Seeking and Collective Action\nAt the heart of the issue lies the profit-driven nature of agricultural production in the United States. Large corporations often capture a disproportionate share of government subsidies, engaging in a practice known as “rent-seeking.” This means that instead of focusing on producing nutritious, high-quality food, these companies are more interested in competing for subsidies. The result? A food system that prioritizes quantity over quality, with serious implications for our health. But why don’t consumers just demand better options? The answer lies in the collective action dilemma and cheaper costs of food that is less nutrient dense. With the sheer volume of food consumed in the US each year, one person’s dietary choices have little impact on overall demand. This can make efforts to consume thoughtfully grown, nutritious products feel pointless. Furthermore, limited access to information about proper nutrition and the influence of lobbying efforts by large agricultural corporations make it even harder for individuals to make informed decisions. Furthermore, fast food companies and a lack of quality produce available incentivize an inefficient diet.\n\n\nThe Corn and Cattle Conundrum\nNow, let’s dive deeper into one of the most striking examples of problematic agricultural subsidies: corn. Corn subsidies have been a mainstay of US agricultural policy for decades, accounting for 27% of all agricultural subsidies since 1995 (Hughes 2021). These subsidies have led to a massive overproduction of corn, with far-reaching consequences on our health and the environment. One of the most significant impacts of corn subsidies has been the proliferation of high-fructose corn syrup (HFCS). This cheap, highly-processed sweetener has become ubiquitous in the American food supply, thanks in large part to the artificially low price of corn. However, numerous studies have linked the consumption of HFCS to a host of chronic health problems, including obesity, diabetes, and heart disease (L. et al. 2015). By subsidizing the production of corn, the government is essentially subsidizing the production of an ingredient that is making us sick.\nBut the negative impacts of corn subsidies don’t stop there. Roughly one-third of all subsidized corn is used for livestock feed, particularly in the beef industry (S. et al. 2016). This has led to the rise of massive, industrialized cattle feedlots, where animals are crammed together and fattened up on a diet of cheap, subsidized corn. These feedlots are environmental disasters, producing staggering amounts of greenhouse gases, water pollution, and land degradation. The combination of corn subsidies and industrialized cattle production creates a vicious cycle of environmental destruction and health consequences. The cheap, abundant corn produced due to subsidies enables the growth of environmentally harmful feedlots, while the concentration of animals in these feedlots leads to increased pollution and disease risk (K. et al., n.d.). Meanwhile, the overconsumption of corn-based products like HFCS contributes to a range of chronic health problems that burden our healthcare system and diminish quality of life.\nDespite the clear and well-documented harm caused by corn subsidies and industrialized cattle production, these practices continue to receive significant government support. This is a clear failure of agricultural policy, one that prioritizes the interests of large agribusinesses over the health of our citizens and the environment. It’s time for a fundamental shift in how we approach agricultural subsidies, one that recognizes the true costs of our current system and seeks to promote more sustainable, health-promoting practices.\n\n\nLooking Overseas for Solutions\nWhile the situation in the United States may seem bleak, other countries offer valuable examples of alternative approaches to agricultural subsidies.\nLet’s start with Australia, where pasture cropping practices are gaining traction (Whitelaw 2021). This method involves growing an annual crop alongside perennial pasture grasses, allowing for the production of both food and livestock while maintaining soil health and preventing erosion. By promoting sustainable practices, the Australian government is actively working to mitigate the environmental damage caused by traditional livestock farming.\nNew Zealand takes things a step further by completely eliminating agricultural subsidies (Clair 2020). This bold move has forced farmers to adopt more efficient practices and has led to improved environmental outcomes, such as better water quality and reduced overgrazing. New Zealand’s success story proves that it is possible to have a thriving agricultural sector without relying on government handouts.\nFinally, let’s look at Finland, where the government has taken a proactive approach to improve the health of its citizens through targeted subsidies. By subsidizing specialty crops and reducing subsidies for commodity crops, Finland has seen a decrease in cardiovascular mortality rates and improvements in average blood pressure and cholesterol levels (P. et al. 2001). This case study demonstrates the potential for government policies to directly influence public health outcomes.\n\n\nThe Path Forward\nSo, what can we learn from these international examples, and how can we apply them to the United States?\nFirst and foremost, we need to recognize that the current system of agricultural subsidies is broken. It prioritizes the interests of large corporations over the health of our citizens and the environment. By redirecting subsidies away from commodity crops and towards specialty crops and regenerative farming practices, we could make nutritious foods more affordable and accessible while also promoting environmental sustainability. However, this shift will require more than just policy changes. We need a concerted effort to educate the public about the importance of proper nutrition and the impact of our food choices on the environment. This could involve expanding nutrition education programs in schools, launching public awareness campaigns, and increasing transparency about the environmental and health costs of our current food system. Additionally, we must hold our elected officials accountable for the policies they support. This means demanding an end to subsidies that prioritize corporate interests over public health and environmental sustainability. It also means supporting candidates who are committed to reforming our agricultural system and backing regenerative practices.\n\n\nConclusion\nThe failure of agricultural subsidies in the United States is a complex and multifaceted issue, but it is not an impossible situation to solve. By learning from the successes and failures of other countries, and by committing to prioritizing health and sustainability over corporate profits, we can create a food system that works for everyone. But this change won’t happen overnight, and it won’t happen without the active participation of informed and engaged citizens. So, the next time you sit down to a meal, take a moment to think about the journey that food took to reach your plate. And then ask yourself: what can I do to ensure that journey becomes a healthier, more sustainable one? Together, we have the power to transform our food system and create a better future for ourselves and the planet.\n\n\n\n\n\nReferences\n\nal., Kolok et. n.d. “The Environmental Impact of Growth.” https://link.springer.com/chapter/10.1007/978-0-387-77030-7_1.\n\n\nal., Lowette et. 2015. “Effects of High-Fructose Diets on Central Appetite Signaling and Cognitive Function.” https://pubmed.ncbi.nlm.nih.gov/25988134/.\n\n\nal., Pietinen et. 2001. “Nutrition and Cardiovascular Disease in Finland Since the Early 1970s: A Success Story.” https://pubmed.ncbi.nlm.nih.gov/11458284/.\n\n\nal., Siegel et. 2016. “Association of Higher Consumption of Foods Derived from Subsidized Commodities with Adverse Cardiometabolic Risk Among US Adults.” https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2530901.\n\n\nClair, Tony St. 2020. “Farming Without Subsidies - a Better Way.” https://www.politico.eu/article/viewpoint-farming-without-subsidies-a-better-way-why-new-zealand-agriculture-is-a-world-leader/.\n\n\nHughes, Parker. 2021. “The Farm Subsidy Paradox.” https://weekly.regeneration.works/p/-the-farm-subsidy-paradox.\n\n\nWhitelaw, Andrew. 2021. “Unever Playing Field for Aussie Farmers.” https://www.farmweekly.com.au/story/7104254/uneven-playing-field-for-aussie-farmers/."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html",
    "href": "blog/2023-12-2-geo-proj/index.html",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "",
    "text": "In the heart of winter, the greater Houston region found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis in a precarious state, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#introduction",
    "href": "blog/2023-12-2-geo-proj/index.html#introduction",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "",
    "text": "In the heart of winter, the greater Houston region found itself plunged into darkness. The February 2021 blackout, a consequence of severe winter storms, left a metropolis in a precarious state, exposing the vulnerabilities in urban power grids. This blog post aims to dissect the Houston blackout, employing geospatial analysis to estimate the number of homes affected and explore the socio-economic factors influencing community resilience during power outages. This intricate analysis is not just a tale of a city in darkness but a lens into the fragility of our urban lifelines."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#background",
    "href": "blog/2023-12-2-geo-proj/index.html#background",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Background",
    "text": "Background\nThe February 2021 blackout in Houston transcends being a singular, isolated power outage, instead unfolding as a critical incident that checked the robustness of urban infrastructures against extreme weather conditions. This event provides a unique opportunity to analyze the resilience of a major metropolitan area’s energy grid and its capacity to withstand unforeseen natural disasters. This particular incident in Houston, a city known for its dynamic growth and complex urban layout, becomes a focal point for examining urban resilience in the face of environmental challenges. The blackout serves as a view into understanding the repercussions of such events on a city’s operational continuity and the well-being of its residents. Houston is a good example for examining the intersection of urban infrastructure with environmental circumstances, offering insights into the areas where improvements are needed to enhance city-wide resilience. The significance of this analysis lies in its potential to influence future urban planning and emergency response strategies. By investigating the impacts of the Houston blackout, valuable lessons can be learned about the importance of designing cities that are not only efficient under normal conditions but also resilient and adaptable in the face of climate challenges. This blackout demonstrates the necessity of anticipating and preparing for extreme scenarios, ensuring that cities are not just places of progress and development, but also places of safety and stability for all civilians."
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "href": "blog/2023-12-2-geo-proj/index.html#data-and-methodology",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Data and Methodology",
    "text": "Data and Methodology\nThe analysis hinges on three key data sources: night light data (NASA 2023), OpenStreetMap data detailing roads and buildings (OpenStreetMaps 2023), and American Community Survey (ACS) socio-economic data. OpenStreetMap and the Landsat data from NASA are regularly-updated databases that are available to the public. I employed R, utilizing libraries such as stars, sf, and terra, to manipulate and analyze these datasets. The methodology involved a step-by-step approach, beginning with the aggregation of night light intensity data to assess blackout areas. I then integrated this with road and building data to estimate the number of homes affected and concluded with an examination of socio-economic factors, offering insights into the recovery dynamics of different communities.\nResearch Question: How did economic status influence the susceptibility and resilience of communities in Houston in the 2021 winter blackouts?\nLet’s get to coding.\nStarting off by loading in the necessary libraries.\n\n# import libraries\nlibrary(dplyr)\nlibrary(abind)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stars)\nlibrary(here)\n\n# clear environment for sanity\nrm(list = ls())\n\nNext, importing data. The first set of data is the night light data, made up of tiles, or images, that capture the light intensity emanating from the surface of the planet during nighttime. The data is stored as rasters, and the stars package is used for raster handling.\n\n# read in night lights tiles\nfeb7_h08v05 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nfeb7_h0806 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\nfeb16_h0805 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nfeb16_h0806 &lt;- read_stars(here::here(\"blog\", \"2023-12-2-geo-proj\", \"data\", \"VNP46A1\", \"VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\nNext, we want to combine the tiles that have the same date and create two objects. We can utilize the st_mosaic() function to combine the tiles.\n\n# combine tiles for each date to create 2 objects\nfeb7 &lt;- st_mosaic(feb7_h08v05, feb7_h0806)\nfeb16 &lt;- st_mosaic(feb16_h0805, feb16_h0806)\n\nGreat. The next step involves creating a blackout mask to identify areas in Houston that lost power during the winter storms. This is done by calculating the change in light intensity, reclassifying this light internsity difference raster, and then converting the raster to a stars object.\nNow, we want to convert the blackout stars to a vector format and fix any invalid geometries using st_make_valid(). Let’s visualize the blackout in vector format to check where we are at.\n\n# calculate the change in lights intensity from Feb 7 to Feb 16\ndiff &lt;- feb7 - feb16\n\n# reclassify difference raster so it is maleable\ndiff &lt;- rast(diff)\nrmask &lt;- diff\n\n# reclassify difference raster assuming a 200 nW shift was due to the blackout\nrmask[rmask &lt;= 200] = NA\n\n# convert back to a stars object \nblackout_stars &lt;- st_as_stars(rmask)\n\n# convert blackout to vector format and fix invalid geometries, set CRS\nblackout_sf_valid_trans &lt;- st_as_sf(blackout_stars, as_points = FALSE) %&gt;% \n  st_make_valid() %&gt;% \n  st_transform(crs = 3083)\n\nThe next piece of analysis involves defining the Houston metropolitan area and cropping the blackout mask to this area of interest. It is imperative that the coordinate reference systems align of these two spatial objects!\n\n# define Houston metropolitan coordinates\nhouston_coords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n\n# create Houston polygon outline\nhouston_poly &lt;- st_polygon(list(houston_coords))\n\n# convert to sf and initially assign CRS EPSG:4326\nhouston &lt;- st_sfc(houston_poly, crs = st_crs(\"EPSG:4326\"))\n\n# transform the Houston polygon to EPSG:3083 \nhouston &lt;- st_transform(houston, crs = \"EPSG:3083\")\n\n# crop the blackout to the Houston metropolitan region\ncropped_blackout_proj &lt;- st_intersection(blackout_sf_valid_trans, houston) \n \n# assign EPSG:3083 CRS to assure consistent CRS\ncropped_blackout_proj &lt;- st_transform(cropped_blackout_proj, crs = \"EPSG:3083\")\n\nplot(cropped_blackout_proj, \n     colorbar = FALSE,\n     main = \"Locations of Significant Light Intensity Change in Houston\")\n\n\n\n\nThis is a visual of the Houston metropolitan area light intensity from before and after the storm. The census tracts that lie in the places where the black dots show up are areas that have been impacted by the blackout.\nNext, we will refine the blackout analysis by excluding areas near highways. This helps in ensuring that the blackout mask we created earlier accurately reflects residential or non-highway areas that lost power, rather than areas simply experiencing less traffic during the storms.\n\n# define SQL query\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# load highway data\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query, quiet = TRUE)\n\n# reproject highway to EPSG:3083\nhighway_proj &lt;- st_transform(highways, crs = 3083)\n\n# identify areas within 200m of all highways, dissolve buffer, and convert to sf \nbuffer &lt;- highway_proj %&gt;% \n  st_buffer(200) %&gt;% \n  st_union() %&gt;% \n  st_as_sf()\n\n# exclude areas close to highways from blackout\nblackout &lt;- st_difference(cropped_blackout_proj, buffer)\n\n\nFinding Impacted Homes\nNext, we will look to find the number of homes that have been impacted by the blackouts. A buildings dataset will be loaded in and filtered for residential buildings. Then, we will filter for the homes that are in the blackout area and display this value.\n\n# define SQL query\nquery_buildings &lt;- \"\nSELECT * \nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\"\n\n# load buildings dataset using st_read and apply the query, reproject CRS\nbuildings_proj &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = query_buildings, quiet = TRUE) %&gt;% \n  st_transform(crs = 'EPSG:3083')\n\n# filter for homes within blackout areas by filtering\nimpacted_homes &lt;- buildings_proj[blackout, , op = st_intersects]\n\n# count the number of impacted homes\nnum_impacted_homes &lt;- nrow(impacted_homes)\n\n# show the number of impacted homes\nprint(paste(\"Number of impacted homes:\", num_impacted_homes))\n\n[1] \"Number of impacted homes: 157411\"\n\n\nAs the results show, a grand total of 157,411 residential buildings were impacted by the blackouts. This total quantifies the extreme impact of the storms and highlights how many individuals and families faced the consequences of prolonged blackouts.\n\n\nInvestigating Socioeconomic Factors\nUp next, we want to consider potential disproportionate socioeconomic outcomes, particularly the average income per household in each census tract in the Houston metropolitan area. We will read in a new dataset and pull the income layer from it, and filter for homes in the blackout region using a spatial join of the census tract data and buildings data with the st_join() function.\n\n# geodatabase file path\ngdp_path &lt;- 'blog/2023-12-2-geo-proj/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb'\n\n# load the geometries from the ACS data\nacs_geom_raw &lt;- st_read(here::here(gdp_path), layer = \"ACS_2019_5YR_TRACT_48_TEXAS\", quiet = TRUE)\n\n# transform CRS to EPSG:3083\nacs_geom &lt;- st_transform(acs_geom_raw, crs = 3083)\n\n# load the ACS income data\nacs_income &lt;- st_read(here::here(gdp_path), layer = \"X19_INCOME\", quiet = TRUE)\n\n# select the median income column\nacs_income_selected &lt;- acs_income[, c(\"GEOID\", \"B19013e1\")]\n\n# trim the GEOID in acs_income_selected to keep the last 11 characters\nacs_income_selected$GEOID &lt;- substr(acs_income_selected$GEOID, nchar(acs_income_selected$GEOID) - 10, nchar(acs_income_selected$GEOID))\n\n# ensure that GEOID columns are of the same data type\n# convert GEOID to character if they are not\nacs_income_selected$GEOID &lt;- as.character(acs_income_selected$GEOID)\nacs_geom$GEOID &lt;- as.character(acs_geom$GEOID)\n\n# join the datasets\nacs_data &lt;- merge(acs_geom, acs_income_selected, by = \"GEOID\")\n\n# merge census tract information to each impacted building\nimpacted_homes_with_tract &lt;- st_join(impacted_homes, acs_data, join = st_intersects)\n\n# aggregate to find unique census tracts that have had blackouts\ntracts_with_blackouts &lt;- unique(impacted_homes_with_tract$GEOID)\n\n# create a subset of acs_data with only tracts that experienced blackouts\nacs_data_blackouts &lt;- acs_data[acs_data$GEOID %in% tracts_with_blackouts, ]"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "href": "blog/2023-12-2-geo-proj/index.html#results-and-visualizations",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Results and Visualizations",
    "text": "Results and Visualizations\nAt this stage, it is time to get down to the main analysis and determine the potential difference in income between impacted versus unimpacted tracts. To conduct this analysis, we will create a a map of median income by census tract and a box-plot of the income in impacted versus unimpacted tracts.\n\n# add impact status column \nacs_data$impact_status &lt;- ifelse(acs_data$GEOID %in% tracts_with_blackouts, \"Impacted\", \"Unimpacted\")\n\n# re-establish bounding coordinates to plot the map in\nhouston_extent &lt;- st_bbox(houston)\n\n# plot the map of Houston \nmap &lt;- ggplot(data = acs_data) +\n  geom_sf(aes(fill = B19013e1, color = impact_status), show.legend = 'point') +\n  scale_color_manual(values = c(\"Impacted\" = \"red\", \"Unimpacted\" = \"green\")) + \n  labs(title = \"Median Income by Census Tract and Impact Status\",\n       fill = \"Median Income\",\n       color = \"Impact Status\") +\n  coord_sf(xlim = c(houston_extent$xmin, houston_extent$xmax), \n           ylim = c(houston_extent$ymin, houston_extent$ymax), \n           expand = FALSE) +\n  theme_minimal()\n\n# make sure median income is numeric\nacs_data$B19013e1 &lt;- as.numeric(acs_data$B19013e1)\n\n# make the box plot to compare distributions\nboxplot &lt;- ggplot(acs_data, aes(x = impact_status, y = B19013e1, fill = impact_status)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Impacted\" = \"red\", \"Unimpacted\" = \"blue\")) +\n  labs(title = \"Median Income of Impacted vs Unimpacted Tracts\",\n       x = \"Blackout Impact\", y = \"Median Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\nmap\n\n\n\n\n\nboxplot"
  },
  {
    "objectID": "blog/2023-12-2-geo-proj/index.html#conclusion",
    "href": "blog/2023-12-2-geo-proj/index.html#conclusion",
    "title": "A Geospatial Analysis of Houston’s Blackout Sensitivity",
    "section": "Conclusion",
    "text": "Conclusion\nThe results show that the median income of a census tract did not influence how likely they were to have experienced a blackout. This can be seen in the map, but the box plot reveals it in a clearer picture. While it may seem that poorer tracts would be less likely to be influenced by the blackout and that it would take longer for their power to be fully rejuvenated, it is important to consider that there are many factors to consider in this situation. For example, it is possible that the distribution of power infrastructure and its resilience to extreme weather events played a more significant role in the occurrence of blackouts than socioeconomic factors. Areas with older or less maintained power grids, regardless of the median income of the residents, might have been more susceptible to failures. Additionally, the physical geography of the region, such as elevation or proximity to water bodies, could have influenced the impact of the winter storms on power availability. It’s also worth considering the location of essential services and emergency response priorities, which might lead to quicker restoration in certain areas irrespective of their economic status. These complexities highlight the importance of a multifaceted approach in understanding and addressing the challenges posed by such natural disasters.\nHowever, when considering areas that extended outside of the Houston and looking at all areas of Texas impacted by the blackouts, a Rockefeller foundation study revealed that areas with a high share of minority population were more than four times as likely to suffer a blackout than predominantly white areas. (Hsu 2021) The findings of the Rockefeller Foundation’s study are a call for action. They underscore the urgency for more equitable infrastructure planning and disaster response strategies that recognize and these historical imbalances. As we move forward, it is imperative that resilience and emergency preparedness are seen through the lens of equity, ensuring that all communities, no matter their racial or ethnic makeup, are fortified with the means to withstand the challenges of such catastrophic events."
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html",
    "href": "blog/2024-3-15-cal-cofi/analysis.html",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "",
    "text": "This blog post is reflecting on the last assignment done in EDS232 of the Bren MEDS program. We were given data from CalCOFI, an orgainzation that has existed over 70 years and conducts important oceanographic research. In this blog post, I will flesh out some of the work done in the assignment and add context around the importance of dissolved inorganic compounds in climate conservation efforts."
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#intro",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#intro",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "",
    "text": "This blog post is reflecting on the last assignment done in EDS232 of the Bren MEDS program. We were given data from CalCOFI, an orgainzation that has existed over 70 years and conducts important oceanographic research. In this blog post, I will flesh out some of the work done in the assignment and add context around the importance of dissolved inorganic compounds in climate conservation efforts."
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#whats-dic",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#whats-dic",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "What’s DIC?",
    "text": "What’s DIC?\nDissolved inorganic carbon (DIC) plays a critical role in the ocean’s carbon cycle and has significant implications on the health of marine life, in addition to more general climate change impacts. Image this: instead of being fluid, the ocean is a massive sponge that soaks up carbon dioxide from the atmosphere. Over time, the carbon dioxide dissolves into the water and transforms into different kinds of dissolved inorganic carbon, like carbonic acid and bicarbonate (I won’t cover the chemistry of this transition, but you can learn more about that at [@ScienceDirect]). These forms of DIC are kept stable by the pH levels of the ocean (roughly 8.1). Now imagine the sponge again, already full, trying to squeeze in more carbon dioxide. This is where things are at now: more carbon dioxide gets stuffed into the atmosphere by burning fossil fuels and cutting down forests, which is causing the oceans to absord more carbon dioxide than they are ready to handle. Despite this, oceans will continue to soak in carbon dioxide. As a result, the amount of carbon dioxide absorbed by the ocean leads to a process called ocean acidification. The greater carbon dioxide absorption causes a shift in the delicate balance of DIC in the ocean, leading to a decrease in the ocean’s pH level. Marine life, and everything that relies on it, are and will be the first to pay the price of this devastating effect, and ripples of the acidification process will be felt in every corner of the planet.\nOcean acidification can have serious consequences on marine organisms that rely on calcification to build their skeletons and shells, such as coral, plankton, crabs, lobsters, and more. Coral reefs are home to an abundance of life. Often referred to as the rainforests of the sea, coral reefs cover less than one percent of the ocean floor but support about 25 percent of all known marine species. As the ocean gets more acidic (when the pH lowers), there is a lower supply of carbonate ions which in turns makes it challening for these organisms to build their essential body structures. This reminds me of a situation where the half of the wood on Earth gets infested with something and is unusable, causing the construction industry to go down, then all goods-producing industries, then subsequently every other industry in the global economy to just crash and burn. Coral reefs have it even worse than the shellfish since they face the impact of reduced calcification rates and higher rates of dissolution of their calcium carbonate structures (or backbone). To make maters worse, rising ocean temperatures and deviations in typical oceanographic patterns can impact the spread and cycle of DIC across the ocean. Warmer waters aren’t able to absorb as much carbon dioxide, so as the ocean rises it is likely going to be the case that less carbon is actually drawn from the atmosphere and absorbed by the ocean. Double whammie! Ocean acidification has extreme consequences, and it is our obligation to limit it as best we can.\nHere is what can be done: reduce our carbon emissions and restore coastal ecosystems, and we could be in luck!"
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#importance-of-predicting-dic",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#importance-of-predicting-dic",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Importance of Predicting DIC",
    "text": "Importance of Predicting DIC\n\nUsing ML\nIf we want to tackle the challenges posed by ocean acidification and climate change, we need to be able to predict DIC levels accurately. This is where machine learning comes in to play.\nBy harnessing the power of data and algorithms, we can develop models that help us assess the extent of ocean acidification across different locations and time scales. This is critical for understanding the potential impacts on marine ecosystems and identifying the areas that are most vulnerable. Predicting DIC levels also enables us to establish specific, tailored, and powerful mitigation strategies. By anticipating future changes in DIC and their consequences, we can make informed, (hopefully) accurate decisions about how to allocate resources, prioritize conservation efforts, and adapt management approaches. Sounds wonderful, and even feasible?\nIn this analysis, my goal is to build a predictive model for DIC using machine learning approaches. I found that XGBoost, a powerful algorithm known for its ability to handle complex relationships and deliver accurate predictions, was the best performer on the data set we were given. I tried a few Random Forest methods, but did not get quite the same results as from XGBoost. By training the model on a dataset of various oceanographic parameters, such as temperature, salinity, and nutrient concentrations, the goal was to capture and understand the underlying patterns and relationships that influence DIC levels. I also dove deep into the data, exploring the distributions, correlations, and potential multicollinearity among the variables. This let me gain (and now you, the reader) a more holistic understanding of the factors at play.\n\n\nEquity Consequences\nThe impacts of elevated DIC levels on marine ecosystems extend far beyond the realm of environmental conservation. They have profound socio-economic implications that ring through coastal communities and beyond. Many of these communities rely heavily on the health and productivity of marine environments for their livelihoods, and the degradation of these ecosystems can have severe consequences.\nConsider the coral reefs again, which are particularly vulnerable to ocean acidification. These underwater sanctuaries are not only beautiful but also serve as the foundation for thriving ecosystems that support an incredible diversity of life. Coral reefs attract tourists from around the world, generating significant revenue for local economies through activities such as snorkeling, diving, and beach tourism. However, as DIC levels rise and the ocean becomes more acidic, the delicate balance that sustains these reefs is disrupted. The deterioration of coral reefs can lead to a landslide of ecological effects, culminating in the loss of biodiversity and the decline of the tourism industry that relies on their allure.\nMoreover, the socio-economic consequences of elevated DIC levels extend to the realm of fisheries. Lots of coastal communities rely on fishing as a primary source of both sustenance and income. As ocean acidification alters the chemistry of precious waters, it can have detrimental effects on the growth, survival, and reproduction of many marine species. The destruction of fish populations and altered species distributions can have severe implications for the livelihoods of fishermen and the communities they support. The loss of income and food security can ripple through these societies, exacerbating poverty and social instability. For many coastal communities, the ocean is not merely a resource to be exploited but an integral part of their cultural heritage and identity. The degradation of marine environments can erode the very fabric of these societies, undermining traditional practices, knowledge systems, and social structures that have evolved in harmony with the sea. The loss of cultural heritage and the severing of deep-rooted connections to the ocean can have profound psychological and emotional consequences for these communities."
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#tools-and-techniques",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#tools-and-techniques",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Tools and Techniques",
    "text": "Tools and Techniques\nI worked in Python and used its ecosystems of data science libraries, such as pandas, matplotlib, and scikit-learn. Utilizing techniques like cross-validation and hyperparameter tuning, I strove to build a sturdy and generalizable pipeline that serves as an example of how data science can help curb the effects of the climate crisis.\n\n# Import libraries\nimport numpy as np \nimport pandas as pd\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()"
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#data-exploration-and-preprocessing",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#data-exploration-and-preprocessing",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Data Exploration and Preprocessing",
    "text": "Data Exploration and Preprocessing\nFirst, the data is read in and quickly cleaned up.\n\n# Read in data\ndata = pd.read_csv(\"data/training.csv\")\ntest_data = pd.read_csv('data/test.csv')\n\n# Remove the NA values\ndata = data.drop('Unnamed: 12', axis=1)\n\n# Rename columns to match test data\ndata = data.rename(columns={'TA1.x': 'TA1'})\n\n\ndata.columns\n\nIndex(['id', 'Lat_Dec', 'Lon_Dec', 'NO2uM', 'NO3uM', 'NH3uM', 'R_TEMP',\n       'R_Depth', 'R_Sal', 'R_DYNHT', 'R_Nuts', 'R_Oxy_micromol.Kg', 'PO4uM',\n       'SiO3uM', 'TA1', 'Salinity1', 'Temperature_degC', 'DIC'],\n      dtype='object')\n\n\nLooking at the columns, this dataset encompasses a wide array of oceanographic variables that can potentially influence dissolved inorganic carbon (DIC) levels in the ocean. Key features include latitude and longitude (Lat_Dec, Lon_Dec), which provide spatial context and can capture regional variations in DIC. Nutrient concentrations such as nitrite (NO2uM), nitrate (NO3uM), ammonia (NH3uM), phosphate (PO4uM), and silicate (SiO3uM) are essential for marine primary production and can affect the biological pump that influences DIC levels. Physical parameters like temperature (R_TEMP, Temperature_degC), depth (R_Depth), salinity (R_Sal, Salinity1), and dynamic height (R_DYNHT) play crucial roles in determining the solubility and distribution of DIC in the water column. Dissolved oxygen (R_Oxy_micromol.Kg) and total alkalinity (TA1) are closely linked to the carbonate system and can provide insights into the carbonate chemistry that regulates DIC concentrations.\n\n# Distribution of DIC in data\nplt.figure(figsize=(8, 6))\nplt.hist(data['DIC'], bins=20)\nplt.xlabel('DIC')\nplt.ylabel('Frequency')\nplt.title('Distribution of DIC')\nplt.show()\n\n\n\n\nThe distribution of DIC is somewhat bimodal, with beaks around 2000 and 2250.\nNext, let’s look at a correlation heat map to better understand the features in the data and how they relate to one another.\n\n# Correlation heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm', square=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\nA quick glance at the heat map reveals that there are some strong relationships working in the data across the different features. While some are obviosuly 1 (like temperature in farenheit and celcius), closely examining the heatmap gives insights into how the features are related to each other, and points to the idea that it would be benefitial to look into the issue over multicollinearity this data. Multicollinearity arises when two or more features are highly correlated with each other, and it can lead to unreliable coefficient esimates and make the impact of individual features difficult. In this case, temperature in degrees farenheit and celcius are going to be identical and having one or the other will not change the predictions under random forest operators that we will be implementing. The issue of multicollinearity in this context can be skipped over since we are not as much concerened about interpreting each individual variable’s coefficients, but rather the predictive power of a random forest model (or some other similar kind of ML algorithm).\n\nplt.figure(figsize=(12, 10))\nsns.pairplot(data, diag_kind='kde')\nplt.title('Pairwise Scatter Plots')\nplt.show()\n\n&lt;Figure size 864x720 with 0 Axes&gt;\n\n\n\n\n\nA gem of a chart. This shows the distribution of each variable against each other one, and then the distribution of each variable along the axis. This chart is very helpful for identifying redundant features and understanding the distribution of each column to check if normalizing the data would be a productive strategy.\nNow, I’ll go ahead and split the data into training and testing sets, choosing an 80/20 split.\n\n# Remove highly correlated features from the training data\ncolumns_to_remove = ['Temperature_degC', 'Salinity1', 'R_Nuts', 'R_Sal', 'R_DYNHT']\n\nX = data.drop(['DIC'] + columns_to_remove, axis=1)\ny = data['DIC']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nprint(f'X_train : {X_train.shape}')\nprint(f'y_train : {y_train.shape}')\nprint(f'X_test : {X_test.shape}')\nprint(f'y_test : {y_test.shape}')\n\nX_train : (1163, 12)\ny_train : (1163,)\nX_test : (291, 12)\ny_test : (291,)"
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#model-selection-and-training",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#model-selection-and-training",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Model Selection and Training",
    "text": "Model Selection and Training\nI started by choosing Random Forest as my model of choice to predict DIC levels, but I found that XGBoost performs stronger on the training and testing data so I opted for that one instead.\nI tried using both grid search and randomized search, and found that they perform similarly in terms of RMSE. However, grid search takes a lot longer to train and so I opted for randomized search in my final approach.\n\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Define the parameter grid for random forest\nrf_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7, 9],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'max_features': ['sqrt', 'log2']\n}\n\n# Define the parameter grid for AdaBoost\nada_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.03, 0.05, 0.1]\n}\n\n# Create the base random forest model\nrf_model = RandomForestRegressor(random_state=42)\n\n# Perform grid search for random forest\nrf_grid_search = RandomizedSearchCV(estimator=rf_model, param_distributions=rf_param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\nrf_grid_search.fit(X_train, y_train)\n\n# Get the best random forest model\nbest_rf_model = rf_grid_search.best_estimator_\n\n# Create the AdaBoost regressor with the best random forest as the base estimator\nada_model = AdaBoostRegressor(estimator=best_rf_model, random_state=42)\n\n# Perform grid search for AdaBoost\nada_grid_search = RandomizedSearchCV(estimator=ada_model, param_distributions=ada_param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\nada_grid_search.fit(X_train, y_train)\n\n# Get the best boosted random forest model\nbest_boosted_rf_model = ada_grid_search.best_estimator_\n\n# Predict on the test set using the best boosted random forest model\npredictions = best_boosted_rf_model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, predictions)\nprint(\"Root Mean Squared Error:\", np.sqrt(mse))\n\n# Prepare the actual test data features\ntest_data_features = test_data[X.columns]\n\n# Make predictions on the actual test data using the best boosted random forest model\npredictions_actual_test_data = best_boosted_rf_model.predict(test_data_features)\n\nsubmission_df = pd.DataFrame({\n    'id': test_data['id'],\n    'DIC': predictions_actual_test_data\n})\n\nsubmission_csv_path = 'data/final_submission_boosted_rf_tuned_regularized.csv'\nsubmission_df.to_csv(submission_csv_path, index=False)\n\n/Users/maxwellpatterson/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nRoot Mean Squared Error: 5.960879188324126\n\n\nIn this code chunk below, I leverage the XGBoost algorithm to build an ensemble of regression models for predicting dissolved inorganic carbon (DIC) levels. By employing random search and cross-validation, I fine-tune each XGBoost model with optimal hyperparameters. The resulting ensemble combines the strengths of multiple models, enabling accurate predictions that can aid in understanding ocean acidification\n\n'''\n# XGBoost\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import VotingRegressor\n\nparam_distributions = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n    'n_estimators': [50, 100, 200, 300, 400, 500],\n    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'alpha': [0, 0.1, 0.5, 1, 2],  # L1 regularization\n    'lambda': [0, 0.1, 0.5, 1, 2],  # L2 regularization\n    'objective': ['reg:squarederror'],\n    'eval_metric': ['rmse']\n}\n\n# Create a list to store the individual XGBoost models\nmodels = []\n\n# Number of models in the ensemble\nn_models = 5\n\n# Perform random search with cross-validation for each model\nfor i in range(n_models):\n    # Create the XGBoost model\n    xgb_model = xgb.XGBRegressor(random_state=42+i, objective='reg:squarederror', eval_metric='rmse')\n\n    # Perform random search\n    random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions,\n                                       n_iter=50, cv=5, scoring='neg_mean_squared_error', verbose=1, random_state=42+i)\n    random_search.fit(X_train, y_train)\n\n    # Get the best model and add it to the list of models\n    best_model = random_search.best_estimator_\n    models.append(best_model)\n\n    print(f\"Best Parameters for Model {i+1}:\", random_search.best_params_)\n\n# Create the voting regressor\nensemble = VotingRegressor(estimators=[('model'+str(i), model) for i, model in enumerate(models)])\n\n# Fit the ensemble on the training data\nensemble.fit(X_train, y_train)\n\n# Get the best model and its parameters\nbest_model = random_search.best_estimator_\nbest_params = random_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Make predictions using the best model on the training set\ny_train_pred = best_model.predict(X_train)\n\n# Calculate the training RMSE\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\nprint(\"RMSE on the training set:\", train_rmse)\n\n# Make predictions using the best model on the test set\ny_test_pred = best_model.predict(X_test)\n\n# Calculate the test RMSE\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\nprint(\"RMSE on the test set:\", test_rmse)\n'''\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters for Model 1: {'subsample': 0.8, 'objective': 'reg:squarederror', 'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.05, 'lambda': 0.5, 'eval_metric': 'rmse', 'colsample_bytree': 0.8, 'alpha': 0.5}\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters for Model 2: {'subsample': 0.6, 'objective': 'reg:squarederror', 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.05, 'lambda': 1, 'eval_metric': 'rmse', 'colsample_bytree': 0.8, 'alpha': 1}\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters for Model 3: {'subsample': 0.9, 'objective': 'reg:squarederror', 'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.05, 'lambda': 0.5, 'eval_metric': 'rmse', 'colsample_bytree': 1.0, 'alpha': 2}\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters for Model 4: {'subsample': 1.0, 'objective': 'reg:squarederror', 'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.05, 'lambda': 2, 'eval_metric': 'rmse', 'colsample_bytree': 0.6, 'alpha': 0.1}\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters for Model 5: {'subsample': 0.6, 'objective': 'reg:squarederror', 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05, 'lambda': 2, 'eval_metric': 'rmse', 'colsample_bytree': 0.7, 'alpha': 2}\nBest Parameters: {'subsample': 0.6, 'objective': 'reg:squarederror', 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05, 'lambda': 2, 'eval_metric': 'rmse', 'colsample_bytree': 0.7, 'alpha': 2}\nRMSE on the training set: 2.1782897894791775\nRMSE on the test set: 5.692170324604025\n\n\nThe XGBoost model shows a small improvement in RMSE from 5.937 to 5.692. I believe that implementing an ensemble method of averaging across different methods like XGBoost and Bagging would lead to the best submission for the Kaggle competition that this assignment was for!"
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#model-evaluation-and-interpretation",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#model-evaluation-and-interpretation",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Model Evaluation and Interpretation",
    "text": "Model Evaluation and Interpretation\nThe obtained root mean squared error of 5.69 on the test set indicates that, on average, the predictions made by the best XGBoost model deviate from the actual values by approximately 5.69 units. This indicates solid predictive power of our model.\nThe XGBoost model with random search and cross-validation provides a solid foundation for predicting the target variable, but it is important to acknowledge the limitations and potential areas for improvement. One key aspect is the limited hyperparameter search space. While the random search explores a predefined set of hyperparameter values, it may not exhaustively cover all possible combinations that could lead to better performance. Additionally, the stochastic nature of random search means that it randomly samples hyperparameter configurations, which may not guarantee finding the optimal set of parameters. Repeating the search with different random states or increasing the number of iterations could help mitigate this limitation. Another concern is the risk of overfitting, especially given the lower training RMSE. This suggests that the model may be too complex and is fitting the noise in the training data, potentially leading to poor generalization on unseen data. To address this, techniques like regularization or early stopping could be employed. Furthermore, the model’s performance could be improved by obtaining more diverse samples from different areas, as this would provide a more representative dataset for training and evaluation. By expanding the data collection efforts and incorporating samples from various regions, the model’s ability to capture the underlying patterns and generalize well to new data could be enhanced.\n\n# Feature Importance\nxgb_model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\nxgb_model.fit(X_train, y_train)\n\nfeature_importances = xgb_model.feature_importances_\nindices = np.argsort(feature_importances)[::-1]\n\nplt.figure()\nplt.title(\"Feature Importances\")\nplt.bar(range(X_train.shape[1]), feature_importances[indices],\n        color=\"r\", align=\"center\")\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()\n\n\n\n\nThis plot shows that trioxidosilicate (SiO3) and phosphate (PO4) are far and away the most important features in determining DIC values."
  },
  {
    "objectID": "blog/2024-3-15-cal-cofi/analysis.html#implications-and-future-work",
    "href": "blog/2024-3-15-cal-cofi/analysis.html#implications-and-future-work",
    "title": "Predicting Dissolved Inorganic Carbon with Machine Learning",
    "section": "Implications and Future Work",
    "text": "Implications and Future Work\nThis example of machine learning model could be effective when resources are limited to get samples of water and extract DIC content. Dissolved inorganic carbon amounts could be predicted by getting samples from a few locations and then using spatial interpolation to estimate the distribution across the entire region. This could save valuable time and resources and make results easier to obtain, which could be used in the fight to protect marine life and ocean ecosystems."
  },
  {
    "objectID": "blog/2024-5-17-embeddings/index.html",
    "href": "blog/2024-5-17-embeddings/index.html",
    "title": "Exploring Word Embeddings with Kern County Oil News Articles",
    "section": "",
    "text": "Introduction\nWord embeddings are a powerful technique in natural language processing (NLP) that represent words as numerical vectors in a high-dimensional space. These vector representations capture semantic and syntactic relationships between words, allowing machines to understand and process human language more effectively. The main idea behind word embeddings is that words with similar meanings or contexts tend to have similar vector representations, enabling computers to perform tasks such as sentiment analysis, text classification, and language generation with greater precision and nuance.\nWord embeddings have played a crucial role in the development of large language models (LLMs) like GPT-4, Claude, and many others. These models rely on the ability to understand and represent the relationships between words, phrases, and sentences. Through utilizing pre-trained word embeddings or learning their own embeddings during training, LLMs can capture the rich semantic and contextual information present in text data.\nThe importance of word embeddings in LLMs is just massive. They provide a strong foundation for these models to understand and generate human-like language, enabling them to perform a wide range of tasks, from answering all sorts of questions and summarizing text to poetry and writing code. As LLMs continue to advance and find applications in various domains, the role of word embeddings in capturing linguistic nuances and enabling effective language understanding and generation will become even more important. Maybe word embeddings can even help us determine when LLMs become agents, when AGI truly exists, or maybe even prove each of these impossible.\n\n\nTraining Custom Word Embeddings\nIn this first section, we explore the process of training custom word embeddings using a corpus of news articles related to the oil industry in Kern County, California. I am interested in this topic as it directly applied to the capstone project I’m working on. Kern County produces over 70% of the oil in California, a staggering amount that has lead to severe health consequences for people living in the county. Using the tidytext, quanteda, and a few other R packages, we preprocess the text data, create n-gram representations, and compute co-occurrence statistics to build a co-occurrence matrix. This matrix captures the relationships between words based on their contexts within the corpus.\n\n\nCode\nsetwd(\"/Users/maxwellpatterson/Desktop/personal/maxwellpatt.github.io/big\")\n\n# Reading in docx files\npost_files &lt;- list.files(pattern = \".docx\",\n                         path = getwd(),\n                         full.names = TRUE,\n                         recursive = TRUE,\n                         ignore.case = TRUE)\n\n# Use LNT to handle docs\ndat &lt;- lnt_read(post_files)\n\n\nWarning in lnt_asDate(date.v, ...): More than one language was detected. The\nmost likely one was chosen (English 99.28%)\n\n\nCode\narticles &lt;- dat@articles$Article\n\n# Create df with articles\narticles_df &lt;- data.frame(ID = seq_along(articles), Text = articles, stringsAsFactors = FALSE)\n\n# Create unigram probs\nunigram_probs &lt;- articles_df %&gt;%\n  unnest_tokens(word, Text) %&gt;%\n  anti_join(stop_words, by = 'word') %&gt;%\n  count(word, sort = T) %&gt;%\n  mutate(p = n / sum(n))\n\n# Build consituent info abuot each 5-gram \nskipgrams &lt;- articles_df %&gt;% \n  unnest_tokens(ngram, Text, token = \"ngrams\", n=5) %&gt;% \n  mutate(ngramID = row_number()) %&gt;% \n  tidyr::unite(skipgramID, ID, ngramID) %&gt;% \n  unnest_tokens(word, ngram) %&gt;% \n  anti_join(stop_words, by = 'word')\n\n# Sum the total number of occurrences of each pair of words\nskipgram_probs &lt;- skipgrams %&gt;%  \n  pairwise_count(item = word, feature = skipgramID, upper = F) %&gt;% \n  mutate(p = n / sum (n))\n\n# Normalize probabilities\nnormalized_probs &lt;- skipgram_probs %&gt;% \n  rename(word1 = item1, word2 = item2) %&gt;% \n  left_join(unigram_probs %&gt;% \n              select(word1 = word, p1 = p), by = 'word1') %&gt;% \n  left_join(unigram_probs %&gt;% \n              select(word2 = word, p2 = p), by = 'word2') %&gt;% \n  mutate(p_together = p/p1/p2)\n\n\nNext let’s perform dimensionality reduction using Singular Value Decomposition (SVD) to get vector representations for each word in the corpus. These word vectors encode semantic and syntactic information, which lets us explore semantically similar words and perform arithmetic operations on the vectors to uncover interesting relationships.\n\n\nCode\n# Reduce dimensionality\npmi_matrix &lt;- normalized_probs %&gt;% \n  mutate(pmi = log10(p_together)) %&gt;% \n  cast_sparse(word1, word2, pmi)\n\n\n\n\nExploring Semantically Similar Words\nOne of the powerful applications of word embeddings is the ability to identify semantically similar words based on their vector representations. By computing the cosine similarity between word vectors, we can find words that are closely related in meaning or context.\n\n\nCode\n# Replace NAs with 0\npmi_matrix@x[is.na(pmi_matrix@x)] &lt;- 0\n\n# Perform SVD on pmi_matrix\npmi_svd &lt;- irlba(pmi_matrix, 100, verbose = F)\n\nword_vectors &lt;- pmi_svd$u\n\nrownames(word_vectors) &lt;- rownames(pmi_matrix)\n\n# Build function to pull synonyms\nsearch_synonyms &lt;- function(word_vectors, selected_vector, original_word) {\n  dat = word_vectors %*% selected_vector\n  similarities &lt;- as.data.frame(dat) %&gt;%\n    tibble(token = rownames(dat), similarity = dat[,1]) %&gt;%\n    filter(token != original_word) %&gt;%\n    arrange(desc(similarity)) %&gt;%\n    select(token, similarity)\n  \n  return(similarities)\n}\n\n\nIn this analysis, I’ve chosen to explore the top 10 most similar words for the terms “energy,” “disadvantaged,” and “law” using the custom word embeddings trained on the Kern County oil news corpus and the pre-trained GloVe embeddings (more on that later). The results reveal interesting insights into the semantic relationships captured by the embeddings.\n\n\nCode\n# Defining words to explore\nenergy_synonyms &lt;- search_synonyms(word_vectors, word_vectors[\"energy\",], \"energy\") %&gt;% head(10)\ndisad_synonyms &lt;- search_synonyms(word_vectors, word_vectors[\"disadvantaged\",], \"disadvantaged\") %&gt;% head(10)\nlaw_synonyms &lt;- search_synonyms(word_vectors, word_vectors[\"law\",], \"law\") %&gt;% head(10)\n\n# Plotting results\nbind_rows(\n  energy_synonyms %&gt;% mutate(target_word = \"energy\"),\n  disad_synonyms %&gt;% mutate(target_word = \"disadvantaged\"),\n  law_synonyms %&gt;% mutate(target_word = \"law\")\n) %&gt;%\n  group_by(target_word) %&gt;%\n  top_n(10, similarity) %&gt;%\n  ungroup() %&gt;%\n  mutate(token = reorder_within(token, similarity, target_word)) %&gt;%\n  ggplot(aes(token, similarity, fill = target_word)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ target_word, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL, y = \"Similarity\")\n\n\n\n\n\nFor example, the custom embeddings associate “energy” with terms like “renewable,” “clean,” and “affordable,” reflecting the context of the energy industry in Kern county. But the bigger takeaway here is what is Jennifer doing?\n\n\nWord Math\nWord embeddings also enable us to perform arithmetic operations on word vectors, revealing interesting relationships and analogies. By adding or subtracting vectors, we can explore the semantic spaces and uncover meaningful combinations or contrasts.\n\n\nCode\n# Assemble word math equations\nenergy_disad &lt;- word_vectors[\"water\",] + word_vectors[\"resources\",]\nsearch_synonyms(word_vectors, energy_disad, \"\") %&gt;% head(25)\n\n\n# A tibble: 25 × 2\n   token      similarity\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 water          0.766 \n 2 resources      0.616 \n 3 natural        0.281 \n 4 produced       0.206 \n 5 corp           0.129 \n 6 drinking       0.124 \n 7 defense        0.101 \n 8 supply         0.0994\n 9 department     0.0961\n10 supplies       0.0952\n# ℹ 15 more rows\n\n\nCode\nenergy_law &lt;- word_vectors[\"court\",] + word_vectors[\"permitting\",]\nsearch_synonyms(word_vectors, energy_law, \"\") %&gt;% head(25)\n\n\n# A tibble: 25 × 2\n   token      similarity\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 court          0.620 \n 2 permitting     0.539 \n 3 process        0.165 \n 4 kern's         0.155 \n 5 superior       0.147 \n 6 appeals        0.126 \n 7 judge          0.106 \n 8 activity       0.104 \n 9 ruling         0.103 \n10 appeal         0.0934\n# ℹ 15 more rows\n\n\nCode\ndisad_law &lt;- word_vectors[\"disadvantaged\",] - word_vectors[\"health\",]\nsearch_synonyms(word_vectors, disad_law, \"\") %&gt;% head(25)\n\n\n# A tibble: 25 × 2\n   token      similarity\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 review         0.0762\n 2 hearing        0.0575\n 3 set            0.0552\n 4 mitigation     0.0518\n 5 law            0.0507\n 6 comment        0.0420\n 7 potential      0.0408\n 8 attorney       0.0393\n 9 county         0.0387\n10 2022           0.0372\n# ℹ 15 more rows\n\n\nLet’s consider water + resources here. It makes sense that irrigation, groundwater, and drainage are the most similar to the sum of these two word vectors.\n\n\nPretrained GloVe Embeddings\nWhile training custom word embeddings can be valuable for domain-specific tasks, pre-trained embeddings like GloVe (Global Vectors for Word Representation) offer a powerful alternative. These embeddings are trained on massive amounts of text data, capturing a broad range of semantic and syntactic relationships across various domains.\n\n\nCode\nsetwd(\"/Users/maxwellpatterson/Desktop/personal/maxwellpatt.github.io/big\")\nglove6b &lt;- read_csv(\"glove6b.csv\")\n\n\n\n\nCode\n# Transform embeddings to tidy \ntidy_glove &lt;- glove6b %&gt;%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %&gt;%\n  rename(item1 = token)\n\n# Build nn func to get similar words\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\ntidy_glove %&gt;% nearest_neighbors(\"energy\")\n\n\n# A tibble: 400,000 × 2\n   item1       value\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 energy      1    \n 2 resources   0.729\n 3 gas         0.724\n 4 renewable   0.710\n 5 natural     0.703\n 6 petroleum   0.699\n 7 electricity 0.693\n 8 oil         0.686\n 9 power       0.664\n10 development 0.664\n# ℹ 399,990 more rows\n\n\nCode\ntidy_glove %&gt;% nearest_neighbors(\"disadvantaged\")\n\n\n# A tibble: 400,000 × 2\n   item1           value\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 disadvantaged   1    \n 2 underprivileged 0.879\n 3 needy           0.728\n 4 low-income      0.704\n 5 at-risk         0.696\n 6 marginalized    0.687\n 7 underserved     0.684\n 8 poorer          0.660\n 9 educationally   0.657\n10 handicapped     0.641\n# ℹ 399,990 more rows\n\n\nCode\ntidy_glove %&gt;% nearest_neighbors(\"law\")\n\n\n# A tibble: 400,000 × 2\n   item1          value\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 law            1    \n 2 laws           0.877\n 3 legal          0.768\n 4 rules          0.749\n 5 constitutional 0.722\n 6 act            0.721\n 7 federal        0.719\n 8 legislation    0.708\n 9 statute        0.707\n10 court          0.705\n# ℹ 399,990 more rows\n\n\n\n\nCode\n# make into a matrix\ntidy_matrix &lt;- tidy_glove %&gt;% \n  cast_sparse(item1, dimension, value)\n\n# replace any na values with 0\ntidy_matrix@x[is.na(tidy_matrix@x)] &lt;- 0\n\n# perform decomposition\ntidy_svd &lt;-svd(tidy_matrix)\n\n# set the row names\nword_vectors &lt;- tidy_svd$u\nrownames(word_vectors) &lt;- rownames(tidy_matrix)\n\n# write out equation\nequation &lt;- word_vectors[\"berlin\",] - word_vectors[\"germany\",] + word_vectors[\"france\",]\n\n# find the most related words\nsearch_synonyms(word_vectors = word_vectors,\n                       selected_vector = equation, # select specific word vector\n                       original_word = \"\")\n\n\n# A tibble: 400,000 × 2\n   token         similarity\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 paris           0.000381\n 2 france          0.000335\n 3 le              0.000301\n 4 french          0.000290\n 5 brussels        0.000275\n 6 de              0.000275\n 7 université      0.000271\n 8 conservatoire   0.000270\n 9 lille           0.000269\n10 marseille       0.000268\n# ℹ 399,990 more rows\n\n\n\n\nCode\n# make into a matrix\ntidy_matrix &lt;- tidy_glove %&gt;% \n  cast_sparse(item1, dimension, value)\n\n# replace any na values with 0\ntidy_matrix@x[is.na(tidy_matrix@x)] &lt;- 0\n\n# perform decomposition\ntidy_svd &lt;-svd(tidy_matrix)\n\n# set the row names\nword_vectors &lt;- tidy_svd$u\nrownames(word_vectors) &lt;- rownames(tidy_matrix)\n\n# write out equation\nequation &lt;- word_vectors[\"berlin\",] - word_vectors[\"germany\",] + word_vectors[\"france\",]\n\n# find the most related words\nsearch_synonyms(word_vectors = word_vectors,\n                       selected_vector = equation, # select specific word vector\n                       original_word = \"\")\n\n\n# A tibble: 400,000 × 2\n   token         similarity\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 paris           0.000381\n 2 france          0.000335\n 3 le              0.000301\n 4 french          0.000290\n 5 brussels        0.000275\n 6 de              0.000275\n 7 université      0.000271\n 8 conservatoire   0.000270\n 9 lille           0.000269\n10 marseille       0.000268\n# ℹ 399,990 more rows\n\n\nA fun little example is given here with Berlin - Germany + France. Are you surprised by the results of this word math?\nIn the analysis here, we explore the GloVe embeddings and compare them to our custom embeddings. We perform similar analyses, such as finding semantically similar words and conducting word math operations, using the GloVe embeddings. The results highlight similarities and differences between the custom and pre-trained embeddings. While the GloVe embeddings capture more general relationships, they may lack some of the nuances and domain-specific associations present in the custom embeddings trained on the Kern County oil news corpus.\n\n\nComparing Custom and Pretrained Embeddings\nBy comparing the results obtained from our custom word embeddings and the pre-trained GloVe embeddings, we can gain insights into the strengths and limitations of each approach. Custom embeddings, trained on domain-specific data, tend to capture more nuanced and contextual relationships relevant to the specific domain. In our case, the custom embeddings trained on the Kern County oil news corpus reflect the language and concepts related to the oil industry, environmental concerns, and local issues.\nThe choice between custom and pre-trained embeddings ultimately depends on the specific task and requirements. For domain-specific applications where capturing nuanced relationships is crucial, custom embeddings may be more appropriate. However, for more general NLP tasks or when computational resources are limited, pre-trained embeddings can offer a powerful and efficient solution.\n\n\nCode\n# Find synonyms using GloVe embeddings\nenergy_synonyms_glove &lt;- search_synonyms(word_vectors, word_vectors[\"energy\",], \"energy\") %&gt;% head(10)\ndisad_synonyms_glove &lt;- search_synonyms(word_vectors, word_vectors[\"disadvantaged\",], \"disadvantaged\") %&gt;% head(10)\nlaw_synonyms_glove &lt;- search_synonyms(word_vectors, word_vectors[\"law\",], \"law\") %&gt;% head(10)\n\n# Plot the results\nbind_rows(\n  energy_synonyms_glove %&gt;% mutate(target_word = \"energy\"),\n  disad_synonyms_glove %&gt;% mutate(target_word = \"disadvantaged\"),\n  law_synonyms_glove %&gt;% mutate(target_word = \"law\")\n) %&gt;%\n  group_by(target_word) %&gt;%\n  top_n(10, similarity) %&gt;%\n  ungroup() %&gt;%\n  mutate(token = reorder_within(token, similarity, target_word)) %&gt;%\n  ggplot(aes(token, similarity, fill = target_word)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ target_word, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL, y = \"Similarity\")\n\n\n\n\n\n\n\nCode\n# Word math equations with GloVe embeddings\nenergy_disad_glove &lt;- word_vectors[\"water\",] + word_vectors[\"resources\",]\nsearch_synonyms(word_vectors, energy_disad_glove, \"\") %&gt;% head(10)\n\n\n# A tibble: 10 × 2\n   token       similarity\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 water         0.000711\n 2 resources     0.000629\n 3 irrigation    0.000590\n 4 groundwater   0.000546\n 5 drainage      0.000528\n 6 potable       0.000520\n 7 sanitation    0.000513\n 8 nutrients     0.000493\n 9 sewage        0.000491\n10 geothermal    0.000483\n\n\nCode\nenergy_law_glove &lt;- word_vectors[\"court\",] + word_vectors[\"permitting\",]\nsearch_synonyms(word_vectors, energy_law_glove, \"\") %&gt;% head(10)\n\n\n# A tibble: 10 × 2\n   token       similarity\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 court         0.000600\n 2 injunction    0.000594\n 3 courts        0.000530\n 4 judge         0.000498\n 5 extradition   0.000493\n 6 supreme       0.000487\n 7 appeals       0.000480\n 8 appellate     0.000479\n 9 rulings       0.000471\n10 upheld        0.000465\n\n\nCode\ndisad_law_glove &lt;- word_vectors[\"disadvantaged\",] - word_vectors[\"health\",]\nsearch_synonyms(word_vectors, disad_law_glove, \"\") %&gt;% head(10)\n\n\n# A tibble: 10 × 2\n   token           similarity\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 disadvantaged     0.000438\n 2 oppressed         0.000355\n 3 educationally     0.000352\n 4 underprivileged   0.000342\n 5 economically      0.000304\n 6 disenfranchised   0.000296\n 7 at-risk           0.000288\n 8 impressionable    0.000285\n 9 low-income        0.000282\n10 gf                0.000282\n\n\nSee how these results are similar to what we calculated previously, but the GloVe results are more general and might not be as useful when looking at Kern county in particular.\n\n\nConclusion\nWord embeddings have revolutionized the field of natural language processing, enabling machines to understand and process human language with unprecedented accuracy and nuance. By representing words as numerical vectors, word embeddings capture the relationships and contexts in language, creating the lane for advanced language models and a wide range of NLP applications.\nAs this analysis has shown, both custom and pre-trained word embeddings can offer valuable insights and capabilities. Custom embeddings, trained on domain-specific data, are able to capture nuanced and contextual relationships relevant to a particular domain, while pre-trained embeddings like GloVe provide a more broad and general representation of language, serving as a powerful foundation for various NLP tasks.\nLooking forward, the future of word embeddings related work is closely tied to the continued advancement of large language models and their applications. As these models become more sophisticated and capable of handling increasingly complex language tasks, the role of word embeddings in capturing linguistic nuances and enabling effective language understanding and generation will become even more important. There are also challenges and constraints to consider. Training high-quality word embeddings requires tons of text data and computational resources, which can be a limiting factor for some applications or domains with limited data availability. Furthermore, word embeddings can struggle to capture certain linguistic phenomena, such as polysemy (words with multiple meanings) and context-dependent word senses.\nTo address these challenges, researchers and practitioners are exploring new techniques and architectures for word representations, such as contextualized word embeddings (e.g. BERT) and transformer-based language models (e.g., GPT-4, DALL-E). These approaches look to capture more nuanced and context-dependent representations of words, further enhancing the ability of machines to understand and generate human-like language.\nAs the field of natural language processing continues to grow and evolve, word embeddings will remain a fundamental building block by enabling machines to “understand” and communicate with humans in increasingly nuanced ways. The future of word embeddings really lies in their continued refinement, integration with advanced language models, and adaptation to new domains and applications, which can lead the charge for more capable language technologies."
  },
  {
    "objectID": "cooking.html",
    "href": "cooking.html",
    "title": "Cooking",
    "section": "",
    "text": "Follow my culinary adventures on Instagram for more delicious content: Chef Xwell on Instagram\nClick on the first photo for a gallery view!"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "This page just has some of the books I have read recently. I decided to put this page up after conquering the Throne of Glass series by Sarah J. Maas, which is as captivating as it is long and I recommend to everyone who’s looking to get into fiction!\n\n\n\n\n\n\n\n\n\n\n\n\nBook\nAuthor\nRating\nThoughts\n\n\n\n\nThere Are Places in the World Where Rules Are Less Important Than Kindness\nCarlo Rovelli\n4/5\nA fun writing style, where Rovelli seems to appreciate the fly-trap nature of his observations.\n\n\nRemarkably Bright Creatures\nShelby Van Pelt\n5/5\nMy definition of a comfort read. A gripping story that makes you want to hug your loved ones and appreciate the important things in life.\n\n\nKingdom of Ash\nSarah J Maas\n5/5\nA perfect ending to an epic series.\n\n\nThe Call of the Wild\nJack London\n4/5\nA perfect story for a cold winter day. A good reminder that there is some untamed nature in all of us.\n\n\nThe Love Hypothesis\nAli Hazelwood\n4/5\nA fun love story that makes you remember that love can happen anywhere, any time.\n\n\nThe Mountain in the Sea\nRay Nayler\n5/5\nA masterpiece. Nayler makes the story feel totally real despite its ridiculousness. A perfect intertwining of AI, philosophy, and consciousness.\n\n\nWhy Fish Don’t Exist\nLulu Miller\n4.5/5\nThis book may or may not change your life (in a good way).\n\n\nGoing Infinite: The Rise and Fall of a New Tycoon\nMichael Lewis\n2/5\nWasn’t impressed. Lewis seems to have some sort of agenda while writing this book, and I got rubbed the wrong way.\n\n\nThe Age of AI and OUr Human Future\nHenry Kissinger\n3.5/5\nUseful and informative, but not groundbreaking.\n\n\nThe Joy of X\nSteven Strogatz\n3/5\nThis book is great if you want to read about math without doing any.\n\n\nQueen of Shadows\nSarah J Maas\n5/5\nMaybe the best book in the series.\n\n\nThe Assassin’s Blade\nSarah J Maas\n4.5/5\nThe short story format keeps you on the edge of your seat. Sam.\n\n\nDaisy Jones & The Six\nTaylor Jenkins Reid\n4.5/5\nThis reads like the characters are right there with you. A fun and flashy story that you will want to finish in one sitting\n\n\nHeir of Fire\nSarah J Maas\n5/5\nWhat a journey this one is. Contains some of the most memorable moments in the whole series.\n\n\nWhat I Talk About When I Talk About Running\nHaruki Muarakami\n3.5/5\nI started running more after reading this, not sure why though.\n\n\nCrown of Midnight\nSarah J Maas\n4.5/5\nGreat character development. Things are just getting started!\n\n\nThrone of Glass\nSarah J Maas\n4.5/5\nYou’ve got to start somewhere, and this story has some essential moments and serves as the foundation for the series.\n\n\nAI For Good\nJuan Ferres and William Weeks\n4.5/5\nAn essential, accessible read for those that want to learn about the bright side of AI."
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html",
    "href": "blog/2023-12-5-envdata-proj/analysis.html",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "",
    "text": "Full repository can be found using this link above!\n (NBC 2019)"
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#about",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#about",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "About",
    "text": "About\n\nPurpose\nThis notebook is designed to perform a comprehensive environmental analysis using two distinct approaches: Air Quality Index (AQI) trend analysis for Santa Barbara County from 2017-2018 and remote sensing data visualization of the 2017 Thomas Fire in California. The primary objectives include demonstrating data manipulation and visualization techniques, time-series analysis for AQI, and the application of true and false color imagery in assessing wildfire impacts. Furthermore, the two parts of the analysis are complementary as the AQI trend analysis allows for the temporal scope of the fire, and the remote sensing segment reveals the spatial scope of the event.\n\n\nHighlights of Analysis\n\nAQI Trend Analysis (2017-2018): Retrieval and preparation of AQI data from the EPA, focusing on data cleaning, concatenation, and column modification. Implementation of a 5-day rolling average for AQI to smooth daily variations and a detailed visualization of these trends over time.\nRemote Sensing Data Visualization: Utilization of Landsat 8 satellite imagery for creating true and false color images of Santa Barbara. Integration of California fire perimeter data to assess the spatial impact of the Thomas Fire, enhancing the understanding of wildfire effects through geospatial analysis.\nData Concatenation and Cleaning: Merging AQI datasets for two consecutive years, followed by data cleaning processes such as modifying column names for consistency and dropping unnecessary columns.\nVisualization Techniques: Development of plots and maps to compare daily AQI values with the 5-day average and to overlay wildfire perimeters on satellite imagery, providing a clear visual representation of both air quality trends and the extent of wildfire damage.\n\n\n\nDataset Description\nThe analysis leverages two primary datasets:\n\nAir Quality Index (AQI) Data: Daily AQI measurements by county for 2017 and 2018, sourced from the EPA, providing insights into air quality trends over the two-year period.\nLandsat 8 Satellite Imagery and Fire Perimeter Data: High-resolution imagery capturing various spectral bands, combined with shapefile data of the 2017 California fire perimeters, to visualize and analyze the impact of wildfires. The data is pulled from the Microsoft PLanetary Computer where it has been pre-processed to remove data outside land and improve the spatial resolution.\n\n\nReferences to Datasets\n\nEPA Air Quality Data: “Daily AQI.” EPA, October 16, 2023. https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI.\nNASA EarthData: “Microsoft Planetary Computer.” Planetary Computer. Accessed December 11, 2023. https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2.\nCalifornia Fire Perimeters 2017: “California Fire Perimeters” California State Geoportal. Accessed December 11, 2023. https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about."
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#air-quality-index-data",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#air-quality-index-data",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "Air Quality Index Data",
    "text": "Air Quality Index Data\n\nImporting Data\nThis section involves importing necessary libraries like pandas and matplotlib for data manipulation and visualization. The AQI data for 2017 and 2018 is fetched from online sources, ensuring access to the most relevant and up-to-date air quality information.\n\n# Import necessary libraries for data manipulation and visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read in AQI data for the years 2017 and 2018 from online sources\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\n\n\nAnalysis\nData Cleaning and Preparation\nThe datasets from different years are concatenated for a comprehensive analysis. Column names are cleaned for consistency, and irrelevant columns are removed, streamlining the data for effective analysis. The conversion of the ‘date’ column to a datetime object and setting it as an index is verified to ensure proper time-series analysis.\n\n# Concatenate the two data frames for combined analysis\naqi = pd.concat([aqi_17, aqi_18])\n\n# Cleaning column names for ease of use\naqi.columns = aqi.columns.str.lower().str.replace(' ', '_')\n\n# Filtering data for Santa Barbara County\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Removing unnecessary columns\nremove = ['state_name', 'county_name', 'state_code', 'county_code']\naqi_sb = aqi_sb.drop(columns=remove)\n\n# Convert 'date' column to datetime object and set as index\naqi_sb['date'] = pd.to_datetime(aqi_sb['date'])\naqi_sb = aqi_sb.set_index('date')\n\nTime-Series Analysis\nA 5-day rolling average for AQI is calculated and checked by displaying the first few entries. This step is critical for smoothing out daily fluctuations and observing longer-term trends in air quality.\n\n# Create a 5-day rolling average for AQI\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n\n\nFinal Output\nThe output visualizes the Air Quality Index (AQI) in Santa Barbara County across two years: 2017 and 2018. The blue line represents the daily AQI values, showing considerable variability with several peaks indicating days of poor air quality. The orange line depicts the 5-day rolling average of AQI, which smooths out the daily fluctuations to reveal the underlying trends more clearly. Notably, there is a significant peak at the start of December 2017, marked by the dashed vertical line, which correlates to a big fire in the area. Overall, the visualization effectively communicates the temporal changes in air quality and the utility of using a rolling average to understand longer-term trends.\n\n# Plotting daily AQI and 5-day average AQI\naqi_sb['aqi'].plot(label='Daily AQI', color='blue')\naqi_sb['five_day_average'].plot(label='5-Day Average AQI', color='orange', linewidth=2)\nplt.axvline(pd.Timestamp('2017-12-01'), color='black', linestyle='--', label='Start of December 2017')\nplt.title('Daily AQI vs. 5-Day Average AQI')\nplt.xlabel('Date')\nplt.ylabel('AQI Value')\nplt.legend()\nplt.savefig('images/aqi_averages.png')\nplt.show()\n\n\n\n\n&lt;Figure size 1728x1152 with 0 Axes&gt;\n\n\nThis visual shows the exterme impact that the Thomas Fire in December 2017 had on air quality index levels in Santa Barbara. The dotted line represents the start of December 2017 with the fire starting just a few days into the month. and the AQI levels proceed to spike significantly as a result. This is seen in both the daily and 5-day rolling average."
  },
  {
    "objectID": "blog/2023-12-5-envdata-proj/analysis.html#false-color-image",
    "href": "blog/2023-12-5-envdata-proj/analysis.html#false-color-image",
    "title": "Thomas Fire and Air Quality Inspection",
    "section": "False Color Image",
    "text": "False Color Image\n\nImporting Data\nIn this initial step, libraries essential for processing geospatial data, such as NumPy, Pandas, GeoPandas, and xarray, are imported. These tools enable the handling of complex raster and vector data formats necessary for environmental and geographical analyses.\n\n# Import necessary libraries for handling geospatial and raster data\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize\nfrom rasterio.crs import CRS\n\n# File path for raster data\ndata_path = os.path.join(os.getcwd(), \"data\", \"landsat8-2018-01-26-sb-simplified.nc\")\n\n# Open raster file\nlandsat = rioxr.open_rasterio(data_path)\n\n# Read fire data\nfire = gpd.read_file(\"data/California_Fire_Perimeters_2017/California_Fire_Perimeters_2017.shp\")\n\n/Users/maxwellpatterson/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of \"\n\n\n\n\nGeographical Context\nThe geographical context is established by loading Landsat 8 satellite imagery for the Santa Barbara region and fire perimeter data from 2017. This step situates the analysis within the specific area affected by the Thomas Fire, setting the stage for a targeted examination of the landscape.\nData Preparation and Alignment\nThis section involves transforming the CRS of the fire perimeter data to match the Landsat data. The successful alignment of these datasets is confirmed, which is imperative for precise spatial overlay in the analysis.\n\n# Reduce dimensions\nlandsat_new = landsat.squeeze(['band'])\n\nlandsat_new = landsat_new.drop(['band'])\n\n# Display updated landsat\nlandsat_new.values\n\n&lt;bound method Mapping.values of &lt;xarray.Dataset&gt;\nDimensions:      (x: 870, y: 731)\nCoordinates:\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0\nData variables:\n    red          (y, x) float64 ...\n    green        (y, x) float64 ...\n    blue         (y, x) float64 ...\n    nir08        (y, x) float64 ...\n    swir22       (y, x) float64 ...&gt;\n\n\n\n\nAnalysis\nCreating True and False Color Images\nThe creation of true and false color images utilizes specific bands from the Landsat data. These images are normalized to enhance visual contrast, aiding in the identification of different land features. Checks confirm the normalization process has occurred correctly.\nTrue Color Image\n\n# Select R, G, B bands\nred_band = landsat_new['red']\ngreen_band = landsat_new['green']\nblue_band = landsat_new['blue']\n\n# Stack the bands along the 'color' dimension\nrgb_image = xr.concat([red_band, green_band, blue_band], dim='color')\nrgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())\n\n# Plot the RGB image \nplt.imshow(rgb_image.transpose('y', 'x', 'color')) \n\n# Visualize map\nplt.show()\n\n\n\n\nFalse Color Image\n\n# Plot the RGB image using imshow\nlandsat_new[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust= True)\n\n&lt;matplotlib.image.AxesImage at 0x7fd870c1f220&gt;\n\n\n\n\n\n\n\nFinal Output\nThe final output is a composite image that illustrates the affected area during the Thomas Fire. This output serves as a potent visual tool for understanding the spatial extent of wildfires and highlights the value of remote sensing in environmental monitoring and disaster assessment.\n\n# Filter for Thomas fire\nthomas_fire = fire[fire['FIRE_NAME']==\"THOMAS\"]\n\n# Convert thomas_fire to GeoDataFrame to same crs as landsat\nthomas_fire = thomas_fire.to_crs(landsat_new.rio.crs)\n\n# Store false color map\nfalse_color = landsat_new[['swir22', 'nir08', 'red']].to_array()\n\n\n# Initiate figure\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Plot outline of California and create key for legend\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# Plot the outline of the Thomas Fire bounding box and create key for legend\n# Set facecolor to 'none' to only show the border\nthomas_fire.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=0.5)\nthomas_patch = mpatches.Patch(edgecolor='red', facecolor='none', label='Thomas Fire')\n\n# Create legend\nax.legend(handles=[thomas_patch], frameon=True, loc='upper right', bbox_to_anchor=(1.4, 1))\nax.set_title('False Color Image of Santa Barbara with Thomas Fire Boundary')\n\n# Save the figure\nplt.savefig('images/thomas_fire_boundary.png')\n\n\n\n\nThis visual provides context on the area that was impacted by the Thomas Fire. Paired with the air quality index temporal data, these visual outputs give us a better understanding of the scope of this catastrophic event in Santa Barbara."
  },
  {
    "objectID": "blog/2024-5-17-math-ai/index.html",
    "href": "blog/2024-5-17-math-ai/index.html",
    "title": "My Journey Through Math",
    "section": "",
    "text": "Generated with DALL-E"
  },
  {
    "objectID": "blog/2024-5-17-math-ai/index.html#agriculture",
    "href": "blog/2024-5-17-math-ai/index.html#agriculture",
    "title": "My Journey Through Math",
    "section": "Agriculture",
    "text": "Agriculture\nLet’s start with food. In What Is Life, Schrodinger poses the question ”How does the living organism avoid decay?” and he follows with the reasons being ”eating, drinking, breathing, and (in the case of plants) assimilating” (pp 70). Every natural system on the planet is connected in one way or another. Consider agriculture: the sun, soil, water, plants and animals all work in harmony with one another and there are many co-benefits that each piece provides for the entire system. This is like the concept of the invisible hand in markets. If one of these players is out of balance, the entire system can be thrown off. Regenerative agriculture provides a solution to addressing the extractive nature of traditional farming methods through holistic management techniques that lead to compounding benefits for the farmland being worked on. Neural networks and the Bayesian paradigm can both help to improve and optimize these strategies. Each of these respective methods can serve a very similar role in the management of regenerative agricultural strategies, but there are a few distinctions to make that differentiate the two. There are a wide range of ways that these solution methods can be implemented, such as predicting crop yield year over year, analyzing soil nutrients, pest detection and control, energy optimization, livestock management, animal health modeling and climate modeling.\nFirst, let’s start with Markov Chain Monte Carlo. MCMC is a powerful statistical tool that cab be used to analyze massive data sets and generate probability distributions for uncertain parameters. In the context of regenerative agriculture, MCMC can help aid some of these modeling systems mentioned in the end of the previous paragraph. Let’s look into a few of these.\nSoil lies at the heart of of regenerative agriculture since it’s health is the most direct player in the growing of produce and feed needed for animals. Let’s consider nitrogen levels, which is one of the most important nutrients in driving healthy soil. Say we want to model the amount of nitrogen in soil over time as crop tilling and cattle grazing strategies are implemented on a farm. We can set up the following formula to model the change in the amount of nitrogen in the soil over time:\n\\[N_{t+1} = N_{t} + f(N_{t},P_{t}) - L(N_{t})\\] In this equation, \\(Nt\\) is the amount of nitrogen in the soil at time t, \\(F(Nt, Pt)\\) is the amount of nitrogen added to the soil through the regenerative techniques that are implemented, and \\(L(Nt)\\) is the amount of nitrogen lost from the soil due to various processes, such as leaching or seasonal changes.\nIn order to estimate the parameters of this model, or in other words the function \\(f\\) and the rate of nitrogen loss \\(L\\), we can implement the MCMC method. First, we need to define prior distributions based on historical data and knowledge of the system we are working in. Once this data has been collected, we can use MCMC to sample from the posterior distribution of the parameters. For example, we could set up the following prior distributions\n\\[f(N_{t}, P_{t}) = N(\\mu_{f}, \\sigma_{f})\\] \\[L(N_t) = e^{\\lambda}\\]\nwhere \\(\\mu_{f}\\) and \\(\\sigma_{f}\\) are the mean and standard deviation of the distribution of nitrogen inputs, and lambda is the rate for the exponential distribution of nitrogen loss. Then, we can use MCMC to sample from the joint posterior distribution of \\(f\\) and \\(L\\), given the data we have on soil nitrogen at different times. By sampling from this distribution, we could estimate the parameters of the model and make predictions about future soil nitrogen levels based on the implemented regenerative management practices. It is important to note that this model is certainly an oversimplification of the model that would be used in practice. While this can certainly be an effective way to determine the effects of regenerative practices, let’s now consider how neural networks can be used to analyze this same problem.\nThere are several advantages to implementing a neural network in this scenario than using MCMC. A neural network may be better for a few reasons: efficiency, non-linearity and generalization befits. Neural networks tend to operate more efficiently than MCMC algorithms since they can parallelize computation over a large data set. Secondly, neural networks are better equipped to model complex, non-linear relationships between inputs and outputs. It is often the case in agriculture where many factors influence soil nutrient levels. Thirdly, neural networks can be trained on a subset of data and then used to make predictions on new data, while MCMC usually requires fitting a specific model to all data points each use. In terms of setting up the neural network, an input, hidden, and output layer will be established, then the network must be trained and tested based on accessible historical data. The input layer consists of nodes that represent the environmental factors that affect soil nitrogen levels, such as soil pH, rainfall, temperature, the regenerative strategies applied and more. Next, the hidden layer is made up of nodes that do calculations based on the input data to generate new features that are relevant for predicting the output. For example, the hidden layer could include nodes that determine the rate of nitrogen fixation or the amount of nitrogen leached from the soil. Next, the output layer consists of nodes that give the predicted nitrogen levels in the soil after implementing the determined regenerative practices. Using these layers as the foundation, the neural network will be trained using a set of input-output pairs that represent the relationship between the environmental factors in play and the soil nitrogen levels. The weights of the nodes in the network are adjusted during the training process in order to minimize the disparity between predicted and actual nitrogen levels according to historical data. After the network is trained, it can be tested using new input data to predict nitrogen levels of soil. The key to the strength of the neural network is the non-linearity of soil health and how all the different variables work together in unique ways, as things do in a living, breathing system. A neural network is more likely to capture these complex interactions and make accurate predictions about the impact of regenerative agriculture on nitrogen levels. Neural networks are more practical than MCMC in this case due to how computationally intensive MCMC can be and how it requires fine tuning of the algorithm to generate strong results in practice.\nIt is critical to the accuracy of the neural network that the network has a proper depth in order to avoid the overfitting of data. As the depth of a neural network increases, the number of parameters also increases. As a result, the network learns how to create more complex representations of the data, or the network is able to capture more nuanced relationships in the data. Deeper networks can be more difficult to train since they naturally require more data and longer amounts of training time to produce results. Deeper networks can be faulty in that they can suffer from vanishing gradients which will lead to unusable information. If the network is too deep, then the network can become too specialized to the training data and perform poorly when analyzing new data. To minimize the potential damage of overfitting, it is imperative to implement strategies such as regularization and early stopping of the program.\nIn What Is Life, Schrodinger conveys that living organisms appear to be able to decrease their entropy locally and maintain a higher degree of order than the things around them. In terms of regenerative agriculture, the goal is to maintain and improve soil health through practices such as cover cropping, crop rotation, and reduced tillage. These practices have positive impacts on the surrounding environment, such as increased biodiversity, better soil structure, and improved carbon sequestration abilities. All of these can be seen as forms of negative entropy in the system. Using this framework, regenerative agriculture can be seen as having a positive impact on entropy through decreasing disorder in these ecosystems. Schrodinger would almost certainly think of regenerative agriculture as a positive example of how living systems can use energy and information to decrease local entropy and maintain a higher order degree when considering the laws of thermodynamics. Everything is connected."
  },
  {
    "objectID": "blog/2024-5-17-math-ai/index.html#language-and-understanding",
    "href": "blog/2024-5-17-math-ai/index.html#language-and-understanding",
    "title": "My Journey Through Math",
    "section": "Language and Understanding",
    "text": "Language and Understanding\nIn addition to food systems, language, and how people perceives and under- stands language, is another topic that I have a new perspective on based on what we have covered this semester. A previous decoding project was a very interesting exercise in that it made me think about how we as humans understand and articulate language. I have a new-found perspective on the way in which babies are influenced by the speech around them and convert this information into their knowledge foundation.\nLet’s consider how a baby begins to absorb sounds, and eventually words, phrases, and sentences, into its memory. In this project, the ’mother text’ that was used to train the program and assign probabilities to letter locations was Moby Dick. While this text is lengthy as far as books go, with there being over 200,000 words, the amount of words children are exposed to is in a whole different ballpark. According to a study done by Betty Hart and Todd Risley, babies hear about 30 million words in their first year alive. As babies become able to truly understand words and associate these words with objects or thoughts, the amount of words that small children register explodes with an increase in brain capacity as they get older. When a child is three years old, the brain is already about 80 percent developed, and reaches 90 percent by year 5.\nUnlike the brain, computers are rigid in structure and do not undergo a growth period like a babies brain does. In the case of text decrypting, the computer already has the adequate storage and processing capabilities to generate probability distributions of a letter or word being before or after another letter. The reality that the training which the program undergoes only takes in letter and/or distributions, and not the manner in which things are said or the way in which the words are delivered, make the way in which computers understand language objectively inferior to that of a human when the information being given to it is in text form. The amount of information that the way in which something is said carries significant weight in terms of truly understanding the purpose or intention of the thing being said.\nThere are many things we can control in life. However, it takes a certain level of knowledge and ability to be able to have control over something. In this light, it is natural that small children do not have control over much since their parents, or an equivalent, are meant to take care of them and provide them with the things they need to survive. Babies cannot control the things that are being said around them that they register in their brain. Essentially, the ’mother text’ that the baby is using to train it’s brain is not chosen under its own jurisdiction. I find it fascinating that for the period of time when the brain is developing the most, the person carrying the brain has no control over the factors most strongly influencing this development. So what does this mean?\nHumans are social creatures. Furthermore, we are a product of our environment. Despite this statement in the last sentence being something that everyone has heard at some point in their life, it is difficult to fully internalize what it means and just how significant it is, especially as it pertains to the perception of others. My big takeaway from thinking about the amount of things that influence who we are which ultimately one has no control over is that people need to be more sympathetic and understanding that everyone is unique in the way in which their brain has developed. Everyone has their own ‘mother text’, and some people get dealt a poor hand in this regard due to factors completely out of their control, such as the family they are born into, where they live growing up, and the communicative nature of the people they are around during the early stages of brain development. Essentially, we can only control so much of who we are: a large part is scripted based on where we happen to show up in the world. Let us not forget this fact that we are truly a product of our environment, and that it is impossible to truly understand the environments which others have been in. Let us be more understanding of one another, listen more deeply, and realize that there is only so much we can know about someone. Do not assume too much. Give others the benefit of the doubt. A certain amount of all of us is literally our environment."
  },
  {
    "objectID": "blog/2024-5-18-nba-api/analysis.html",
    "href": "blog/2024-5-18-nba-api/analysis.html",
    "title": "Fantasy Basketball 9 Category Simulations",
    "section": "",
    "text": "Introduction\nIn this blog post, I will be exploring the NBA API and combining it with data pulled from my fantasy league. Our league is a dynasty format, where the same players are kept each season and only rookies are drafted. See The Dynamics of Dynasty blog post for a deep dive on the league!\nFirst, let’s start by loading in the fantasy league data to get the fantasy team each player is on. Then this data will be merged with data from the NBA API to pull players stats from last season.\n\n\nData Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\nfrom itertools import combinations\nfrom IPython.display import display, HTML\n\nfrom nba_api.stats.endpoints import leaguedashteamstats\nfrom nba_api.stats.static import players, teams\nfrom nba_api.stats.endpoints import leaguedashplayerstats\n\n\n# Load in fantasy data\nfantrax_data = pd.read_csv(\"fantrax_05_18_2024.csv\")\n\n# Keep only 'Player' and 'Status' columns\nfantrax_data = fantrax_data[['Player', 'Status', 'FPts', 'FP/G']]\n\n# Removing free agent players\nfantrax_data = fantrax_data[fantrax_data['Status'] != 'FA']\n\n# Updating Status column name\nfantrax_data = fantrax_data.rename(columns = {'Status': 'Fantasy Team'})\n\n# Create dictionary to redefine team names \nstatus_mapping = {\n    'CCC': 'Cream City',\n    'STARKS': 'Winterfell',\n    'BBB': 'Bikini Bottom',\n    'HBC': 'Helsinki',\n    'Jmarr237': 'Malibu',\n    'maxpat01': 'Santa Barbara',\n    'GBRAYERS': 'Scottsdale',\n    'BIGFOOTS': 'Beaverton',\n    '$?$': 'Las Vegas',\n    'SDP': 'San Diego',\n    'SERP': 'Slytherin',\n    'Orcas': 'Anacortes'\n}\n\n# Apply team name mapping\nfantrax_data['Fantasy Team'] = fantrax_data['Fantasy Team'].map(status_mapping)\n\nNow that the Fantrax data is cleaned, we can join it with each players stats pulled from the NBA API.\n\n# Extract player names\nplayer_names = fantrax_data['Player'].tolist()\n\n# Get player information from NBA API\nnba_players = players.get_players()\n\nplayer_id_map = {player['full_name']: player['id'] for player in nba_players if player['full_name'] in player_names}\n\nfantrax_data['Player_ID'] = fantrax_data['Player'].map(player_id_map)\n\nplayer_stats = leaguedashplayerstats.LeagueDashPlayerStats(season='2023-24').get_data_frames()[0]\n\ndf = pd.merge(fantrax_data, player_stats, left_on='Player_ID', right_on='PLAYER_ID', how='left')\n\nLet’s check out the data to make sure it looks good.\n\ndf.columns\n\nIndex(['Player', 'Fantasy Team', 'FPts', 'FP/G', 'Player_ID', 'PLAYER_ID',\n       'PLAYER_NAME', 'NICKNAME', 'TEAM_ID', 'TEAM_ABBREVIATION', 'AGE', 'GP',\n       'W', 'L', 'W_PCT', 'MIN', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A',\n       'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'TOV',\n       'STL', 'BLK', 'BLKA', 'PF', 'PFD', 'PTS', 'PLUS_MINUS',\n       'NBA_FANTASY_PTS', 'DD2', 'TD3', 'WNBA_FANTASY_PTS', 'GP_RANK',\n       'W_RANK', 'L_RANK', 'W_PCT_RANK', 'MIN_RANK', 'FGM_RANK', 'FGA_RANK',\n       'FG_PCT_RANK', 'FG3M_RANK', 'FG3A_RANK', 'FG3_PCT_RANK', 'FTM_RANK',\n       'FTA_RANK', 'FT_PCT_RANK', 'OREB_RANK', 'DREB_RANK', 'REB_RANK',\n       'AST_RANK', 'TOV_RANK', 'STL_RANK', 'BLK_RANK', 'BLKA_RANK', 'PF_RANK',\n       'PFD_RANK', 'PTS_RANK', 'PLUS_MINUS_RANK', 'NBA_FANTASY_PTS_RANK',\n       'DD2_RANK', 'TD3_RANK', 'WNBA_FANTASY_PTS_RANK'],\n      dtype='object')\n\n\n\ndf\n\n\n\n\n\n\n\n\nPlayer\nFantasy Team\nFPts\nFP/G\nPlayer_ID\nPLAYER_ID\nPLAYER_NAME\nNICKNAME\nTEAM_ID\nTEAM_ABBREVIATION\n...\nBLK_RANK\nBLKA_RANK\nPF_RANK\nPFD_RANK\nPTS_RANK\nPLUS_MINUS_RANK\nNBA_FANTASY_PTS_RANK\nDD2_RANK\nTD3_RANK\nWNBA_FANTASY_PTS_RANK\n\n\n\n\n0\nNikola Jokic\nCream City\n5154\n65.24\n203999.0\n203999.0\nNikola Jokic\nNikola\n1.610613e+09\nDEN\n...\n32.0\n541.0\n542.0\n6.0\n5.0\n1.0\n1.0\n2.0\n2.0\n2.0\n\n\n1\nLuka Doncic\nWinterfell\n4644\n66.34\n1629029.0\n1629029.0\nLuka Doncic\nLuka\n1.610613e+09\nDAL\n...\n108.0\n497.0\n475.0\n4.0\n1.0\n32.0\n2.0\n6.0\n3.0\n1.0\n\n\n2\nGiannis Antetokounmpo\nBikini Bottom\n4394\n60.19\n203507.0\n203507.0\nGiannis Antetokounmpo\nGiannis\n1.610613e+09\nMIL\n...\n22.0\n565.0\n555.0\n1.0\n3.0\n29.0\n3.0\n4.0\n4.0\n3.0\n\n\n3\nDomantas Sabonis\nBikini Bottom\n4345\n52.99\n1627734.0\n1627734.0\nDomantas Sabonis\nDomantas\n1.610613e+09\nSAC\n...\n64.0\n561.0\n571.0\n12.0\n26.0\n130.0\n5.0\n1.0\n1.0\n5.0\n\n\n4\nAnthony Davis\nBikini Bottom\n4338\n57.08\n203076.0\n203076.0\nAnthony Davis\nAnthony\n1.610613e+09\nLAL\n...\n4.0\n559.0\n524.0\n7.0\n12.0\n85.0\n4.0\n3.0\n10.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n314\nSteven Adams\nScottsdale\n0\n0.00\n203500.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n315\nLonzo Ball\nLas Vegas\n0\n0.00\n1628366.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n316\nKai Jones\nScottsdale\n0\n0.00\n1630539.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n317\nChristian Koloko\nCream City\n0\n0.00\n1631132.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n318\nKevin Porter\nWinterfell\n0\n0.00\n77876.0\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n319 rows × 71 columns\n\n\n\nThe data has been merged successfully! Now, let’s perform some data cleaning to filter out players that were not active last season.\n\ndf = df.dropna()\n\nWhile our league is a points league, where fantasy points are the scoring metric, some leagues are set up as a 9 category format where teams try to win each week based on the 9 main statistical categories.\nThe categories are: - Points - Rebounds - Assists - Steals - Blocks - Field goal percentage - Free throw percentage - Three pointers made - Turnovers\nLet’s see where each fantasy teams strengths would be in this format!\n\npd.options.mode.chained_assignment = None\n\n# Columns to be used for Z-score calculations\ncategories = ['PTS', 'REB', 'AST', 'STL', 'BLK', 'FG_PCT', 'FT_PCT', 'FG3M', 'TOV']\n\n# Calculate per-game stats for each player in each category\nfor category in categories:\n    if category not in ['FG_PCT', 'FT_PCT']:\n        df[category + '_PG'] = df[category] / df['GP']\n    else:\n        df[category + '_PG'] = df[category]  # Percentages are already per-game\n\n# Calculate Z-scores for each player in each per-game category\nfor category in categories:\n    if category not in ['FG_PCT', 'FT_PCT']:\n        z_score_column = category + '_PG_Z'\n        df[z_score_column] = (df[category + '_PG'] - df[category + '_PG'].mean()) / df[category + '_PG'].std()\n    else:\n        z_score_column = category + '_Z'\n        df[z_score_column] = (df[category] - df[category].mean()) / df[category].std()\n\n# Aggregate the Z-scores by team\nteam_z_scores = df.groupby('Fantasy Team')[[col for col in df.columns if col.endswith('_Z')]].mean()\n\n# Display the Z-scores table in HTML format\ndisplay(HTML(team_z_scores.to_html(float_format=\"%.2f\", border=0, justify='center')))\n\n\n\n\n\nPTS_PG_Z\nREB_PG_Z\nAST_PG_Z\nSTL_PG_Z\nBLK_PG_Z\nFG_PCT_Z\nFT_PCT_Z\nFG3M_PG_Z\nTOV_PG_Z\n\n\nFantasy Team\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnacortes\n-0.13\n-0.17\n-0.12\n0.00\n-0.08\n0.07\n-0.36\n-0.23\n-0.02\n\n\nBeaverton\n0.09\n-0.36\n-0.03\n-0.04\n-0.41\n-0.33\n0.26\n0.49\n0.07\n\n\nBikini Bottom\n0.35\n0.45\n0.38\n0.13\n0.24\n0.28\n0.23\n-0.05\n0.33\n\n\nCream City\n0.08\n0.30\n-0.05\n0.16\n0.34\n0.29\n0.27\n0.19\n-0.10\n\n\nHelsinki\n-0.09\n-0.19\n-0.12\n-0.19\n-0.05\n0.09\n-0.24\n-0.19\n-0.22\n\n\nLas Vegas\n-0.22\n-0.06\n-0.39\n-0.00\n-0.04\n-0.21\n-0.20\n-0.09\n-0.21\n\n\nMalibu\n0.32\n0.31\n0.37\n0.30\n0.26\n0.07\n0.25\n0.28\n0.28\n\n\nSan Diego\n0.16\n-0.05\n0.06\n0.30\n-0.03\n0.06\n0.20\n0.20\n0.18\n\n\nSanta Barbara\n-0.30\n-0.31\n-0.28\n-0.16\n-0.02\n-0.02\n-0.24\n-0.02\n-0.15\n\n\nScottsdale\n-0.24\n0.01\n-0.02\n-0.20\n0.02\n-0.13\n-0.57\n-0.38\n-0.25\n\n\nSlytherin\n-0.33\n-0.14\n-0.27\n-0.41\n-0.10\n-0.13\n0.06\n-0.38\n-0.17\n\n\nWinterfell\n0.23\n0.11\n0.35\n0.06\n-0.21\n-0.09\n0.22\n0.12\n0.20\n\n\n\n\n\nThis table shows us which teams are strongest in each categories. As we saw in the previous post, BBB (Bikini Bottom Ballers) are by far the strongest team. JMarr237 (Malibu Mamas) also has a deep roster with lots of good players.\n\n\nMonte Carlo Simulation\nWhen it comes to predicting the outcome of a fantasy basketball season, there are many factors to consider. Each team’s performance can vary from week to week based on the number of games their players have, potential injuries, and the inherent randomness of sports. This is where Monte Carlo simulations come in handy.\nMonte Carlo simulations are a powerful tool that allows us to model complex systems and account for randomness. In the context of our fantasy basketball league, we can use these simulations to generate many possible outcomes for the season, taking into account the variability in each team’s performance.\nThink of it like a game of tug-of-war. Each team’s Z score represents their baseline strength in a particular category. The higher the Z score, the stronger the team is in that category. However, just like in a real game of tug-of-war, there are external factors that can influence the outcome, such as the condition of the rope, the terrain, or even a momentary lapse in concentration from one of the participants. In our simulation, we’ll be using three levels of randomness: low, medium, and high. These levels represent how much each team’s performance can deviate from their expected Z scores each week. A low level of randomness means that the results will be closer to what we expect based on the Z scores, while a high level of randomness introduces more unpredictability.\nBy running the simulation 250 times, we can generate a wide range of plausible scenarios. Each simulation will give us a different set of results, and by analyzing these results, we can gain insights into the likelihood of various outcomes.\nFor example, let’s say Team A has a higher Z score than Team B in the “points” category. In a single simulation, Team B might win that category due to the randomness factor. However, across 250 simulations, we would expect Team A to win the “points” category more often than Team B.\nMonte Carlo simulations help us account for the complexities and uncertainties in a fantasy basketball season. By embracing randomness and running many simulations, we can better understand the range of possible outcomes and make more informed decisions when managing our fantasy teams.\n\n # Parameters\nteams = team_z_scores['Fantasy Team'].unique()\nrandomness_levels = {'low': 0.3, 'medium': 0.6, 'high': 0.9}\nnum_weeks = 22\nnum_simulations = 250\n\n\n# Initialize results df to track wins, losses, and ties\nresults_template = pd.DataFrame(0, index=teams, columns=['Wins', 'Losses'])\n\n\ndef simulate_season(randomness):\n    weekly_results = results_template.copy()\n\n    # Create a schedule where each team plays every other team twice\n    matchups = list(combinations(teams, 2)) * 2\n    random.shuffle(matchups)  # Randomize the order of matchups\n\n    # Distribute matchups across 22 weeks\n    weeks = [matchups[i:i + len(teams)//2] for i in range(0, len(matchups), len(teams)//2)]\n\n    for week in weeks:\n        for team1, team2 in week:\n            team1_score = 0\n            team2_score = 0\n\n            for category in categories:\n                z_score_column = category + '_PG_Z' if category not in ['FG_PCT', 'FT_PCT'] else category + '_Z'\n                team1_z = team_z_scores.loc[team_z_scores['Fantasy Team'] == team1, z_score_column].values[0]\n                team2_z = team_z_scores.loc[team_z_scores['Fantasy Team'] == team2, z_score_column].values[0]\n\n                team1_z += np.random.normal(0, randomness * abs(team1_z))\n                team2_z += np.random.normal(0, randomness * abs(team2_z))\n\n                if category == 'TOV':\n                    if team1_z &lt; team2_z:  # Fewer turnovers is better\n                        team1_score += 1\n                    else:\n                        team2_score += 1\n                else:\n                    if team1_z &gt; team2_z:  # More is better for other categories\n                        team1_score += 1\n                    else:\n                        team2_score += 1\n\n            if team1_score &gt; team2_score:\n                weekly_results.at[team1, 'Wins'] += 1\n                weekly_results.at[team2, 'Losses'] += 1\n            elif team1_score &lt; team2_score:\n                weekly_results.at[team1, 'Losses'] += 1\n                weekly_results.at[team2, 'Wins'] += 1\n\n    return weekly_results\n\n\ndef simulate_multiple_seasons(randomness, num_simulations):\n    cumulative_results = pd.DataFrame(0, index=teams, columns=['Wins', 'Losses'])\n    all_simulation_results = []\n\n    for _ in range(num_simulations):\n        season_results = simulate_season(randomness)\n        cumulative_results += season_results\n        all_simulation_results.append(season_results['Wins'].copy())\n\n    # Calculate average results\n    average_results = cumulative_results / num_simulations\n    return average_results, pd.concat(all_simulation_results, axis=1)\n\nNow that the Monte Carlo simulation functions have been set up, we can run the simulation 250 times.\n\n# Perform the simulations\nfinal_results = {}\nsimulation_results = {}\n\nfor level, randomness in randomness_levels.items():\n    avg_results, all_results_df = simulate_multiple_seasons(randomness, num_simulations)\n    final_results[level] = avg_results\n    simulation_results[level] = all_results_df\n\nKeyError: 'San Diego'\n\n\n\n# Display the final averaged results for each randomness level\nresults_table = pd.concat(final_results, axis=1)\nresults_table.columns = [f'{level.capitalize()} Randomness' for level in results_table.columns.get_level_values(0)]\n\n# Sort the table by 'Low Randomness' wins in descending order\nresults_table_sorted = results_table.sort_values(by='Low Randomness', ascending=False)\n\ndisplay(HTML(results_table_sorted.to_html(float_format=\"%.2f\", border=0, justify='center')))\n\nValueError: The column label 'Low Randomness' is not unique.\n\n\nReminder that this is for a 22 week regular season where each team plays each other twice. Under a 9-cat format, the Bikini Bottom Ballers are still the best team in the league. SDP, the San Diego Pilots, would seem to fare much better in a 9-cat format than the current points format.\n\n\nPlotting Team Outcomes\nNow let’s visualize the results by showing the win distribution for each team under each randomness scenario.\n\nfor team in teams:\n    fig, axes = plt.subplots(1, len(randomness_levels), figsize=(20, 5), sharex=True, sharey=True)\n    fig.suptitle(f'Number of Wins Distribution for {team} (250 Simulations)', fontsize=20)\n\n    for col, (level, results_df) in enumerate(simulation_results.items()):\n        axes[col].hist(results_df.loc[team], bins=10, alpha=0.7, edgecolor='black')\n        axes[col].set_title(f'{level.capitalize()} Randomness', fontsize=18)\n        axes[col].set_xlabel('Number of Wins', fontsize=16)\n        axes[col].tick_params(axis='both', which='major', labelsize=14)\n        if col == 0:\n            axes[col].set_ylabel('Frequency', fontsize=16)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nIn our analysis, we utilized Monte Carlo simulations to project the outcomes of a fantasy basketball season under different levels of randomness. By running 250 simulations for each level of randomness — low, medium, and high — we were able to model the inherent variability in team performances. This approach not only allowed us to see which teams are likely to excel consistently but also highlighted how randomness can impact the overall season. The histograms generated for each team showed the distribution of their wins across simulations, providing a visual representation of their performance stability."
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html",
    "href": "blog/2024-4-26-topics/index.html",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "",
    "text": "In this blog post, we will explore the topic of biodiversity loss using text analysis techniques in R. We will use the quanteda, tm, topicmodels, ldatuning, and other relevant packages to process and analyze a set of articles related to biodiversity loss. The goal is to identify the main topics discussed in these articles and visualize the results.\nFirst, let’s load the necessary libraries and set the working directory to the location of our data files.\n\nlibrary(quanteda)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(ldatuning)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(reshape2)\nlibrary(LexisNexisTools)\n\n\nsetwd(\"/Users/maxwellpatterson/Desktop/personal/maxwellpatt.github.io/blog/2024-4-26-topics/data/bio-1\")\n\n# Reading in docx files\npost_files &lt;- list.files(pattern = \".docx\",\n                         path = getwd(),\n                         full.names = TRUE,\n                         recursive = TRUE,\n                         ignore.case = TRUE)\n\n# Use LNT to handle docs\ndat &lt;- lnt_read(post_files)\n\nmeta_df &lt;- dat@meta\narticles_df &lt;- dat@articles\nparagraphs_df &lt;- dat@paragraphs\n\ndata &lt;- tibble(Date=meta_df$Date, Headline = meta_df$Headline, id = articles_df$ID, text = articles_df$Article)"
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#introduction",
    "href": "blog/2024-4-26-topics/index.html#introduction",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "",
    "text": "In this blog post, we will explore the topic of biodiversity loss using text analysis techniques in R. We will use the quanteda, tm, topicmodels, ldatuning, and other relevant packages to process and analyze a set of articles related to biodiversity loss. The goal is to identify the main topics discussed in these articles and visualize the results.\nFirst, let’s load the necessary libraries and set the working directory to the location of our data files.\n\nlibrary(quanteda)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(ldatuning)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(reshape2)\nlibrary(LexisNexisTools)\n\n\nsetwd(\"/Users/maxwellpatterson/Desktop/personal/maxwellpatt.github.io/blog/2024-4-26-topics/data/bio-1\")\n\n# Reading in docx files\npost_files &lt;- list.files(pattern = \".docx\",\n                         path = getwd(),\n                         full.names = TRUE,\n                         recursive = TRUE,\n                         ignore.case = TRUE)\n\n# Use LNT to handle docs\ndat &lt;- lnt_read(post_files)\n\nmeta_df &lt;- dat@meta\narticles_df &lt;- dat@articles\nparagraphs_df &lt;- dat@paragraphs\n\ndata &lt;- tibble(Date=meta_df$Date, Headline = meta_df$Headline, id = articles_df$ID, text = articles_df$Article)"
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#data-cleaning",
    "href": "blog/2024-4-26-topics/index.html#data-cleaning",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nNow, let’s clean up the data by removing duplicates, creating a corpus, and examining the tokens. A corpus is just a collection of all the words across all of the articles we are feeding in, with tokens being each of the words in the corpus. We’re working with 161 different news articles on biodiversity loss!\n\n# Cleaning up data\narticles &lt;- data %&gt;% \n  dplyr::filter(!is.na(Headline)) %&gt;%\n  distinct(Headline, .keep_all = TRUE)\n\n# Create a corpus text from the text in the articles\narticles_corpus &lt;- corpus(articles$text)\n\n# Add stop words\nadd_stops &lt;- stopwords(kind = quanteda_options(\"language_stopwords\"))\n\n# Examine tokens\ntokens(articles_corpus)\n\nTokens consisting of 161 documents.\ntext1 :\n [1] \"The\"           \"topic\"         \"of\"            \"this\"         \n [5] \"year's\"        \"International\" \"Day\"           \"for\"          \n [9] \"Biological\"    \"Diversity\"     \"is\"            \"'\"            \n[ ... and 1,186 more ]\n\ntext2 :\n [1] \"November\"      \"17\"            \",\"             \"2022\"         \n [5] \"Release\"       \"date\"          \"-\"             \"16112022\"     \n [9] \"-\"             \"While\"         \"carbon-driven\" \"climate\"      \n[ ... and 1,125 more ]\n\ntext3 :\n [1] \"November\" \"23\"       \",\"        \"2023\"     \"Release\"  \"date\"    \n [7] \"-\"        \"22112023\" \"-\"        \"A\"        \"research\" \"group\"   \n[ ... and 1,577 more ]\n\ntext4 :\n [1] \"March\"    \"17\"       \",\"        \"2023\"     \"Release\"  \"date\"    \n [7] \"-\"        \"16032023\" \"-\"        \"A\"        \"new\"      \"research\"\n[ ... and 341 more ]\n\ntext5 :\n [1] \"Government\"   \"has\"          \"failed\"       \"to\"           \"adequately\"  \n [6] \"protect\"      \"biodiversity\" \"and\"          \"\\\"\"           \"urgent\"      \n[11] \"action\"       \"\\\"\"          \n[ ... and 457 more ]\n\ntext6 :\n [1] \"New\"          \"Delhi\"        \",\"            \"May\"          \"21\"          \n [6] \"(\"            \"IANS\"         \")\"            \"With\"         \"global\"      \n[11] \"biodiversity\" \"loss\"        \n[ ... and 633 more ]\n\n[ reached max_ndoc ... 155 more documents ]\n\n\nWhat we see above is all of the tokens split up across the entire corpus.\nNext, we’ll perform some preprocessing steps on the tokens, such as removing punctuation, numbers, URLs, and stop words, converting to lowercase, and trimming the document-feature matrix.\n\n# Remove punctuation, numbers and url\ntoks &lt;- tokens(articles_corpus, remove_punct = T, remove_numbers = T, remove_url = T)\n\n# Remove stop words\ntok1 &lt;- tokens_select(toks, pattern = add_stops, selection = \"remove\")\n\n# Convert to lower case\ndfm1 &lt;- dfm(tok1, tolower = T)\n\n# Remove words that are included only 1 or 2 times\ndfm2 &lt;- dfm_trim(dfm1, min_docfreq = 2)\n\nsel_idx &lt;- slam::row_sums(dfm2) &gt; 0\ndfm &lt;- dfm2[sel_idx, ]"
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#finding-the-optimal-number-of-topics",
    "href": "blog/2024-4-26-topics/index.html#finding-the-optimal-number-of-topics",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Finding the Optimal Number of Topics",
    "text": "Finding the Optimal Number of Topics\nTo determine the optimal number of topics for our analysis, we’ll use the FindTopicsNumber function from the ldatuning package.\n\nset.seed(123)\n\nresults &lt;- FindTopicsNumber(dfm,\n                            topics = seq(from = 2,\n                                         to = 20,\n                                         by = 1),\n                            metrics = c(\"CaoJuan2009\", \"Deveaud2014\"),\n                            method = \"Gibbs\",\n                            verbose = T)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(results)\n\n\n\n\nBased on the results when I first ran this model, k = 7 appeared to be the optimal number of topics. Note that the graph is looking different on this outputted blog post, but the topics below are identical. So let’s just pretend like this graph is a placeholder!"
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#running-the-topic-model",
    "href": "blog/2024-4-26-topics/index.html#running-the-topic-model",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Running the Topic Model",
    "text": "Running the Topic Model\nNow that we have determined the optimal number of topics, let’s run the Latent Dirichlet Allocation (LDA) topic model and show the most prevalent topics.\n\nset.seed(123)\n\n# set k value based \nk &lt;- 7\n\n# run model\ntopicModel_k7 &lt;- LDA(dfm,\n                     k,\n                     method = \"Gibbs\",\n                     control = list(iter = 1000),\n                     verbose = 25\n)\n\n# get results\nresults &lt;- posterior(topicModel_k7)\nattributes(results)\n\n$names\n[1] \"terms\"  \"topics\"\n\n# define matrices for interpretation\nbeta &lt;- results$terms\ntheta &lt;- results$topics\n\ntopics &lt;- tidy(topicModel_k7, matrix = \"beta\")\n\n# pull top terms\ntop_terms &lt;- topics %&gt;% \n  group_by(topic) %&gt;% \n  top_n(10, beta) %&gt;% \n  ungroup() %&gt;% \n  arrange(topic, -beta)\n\ntop_terms\n\n# A tibble: 73 × 3\n   topic term            beta\n   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     1 research      0.0309\n 2     1 biodiversity  0.0232\n 3     1 news          0.0192\n 4     1 loss          0.0187\n 5     1 science       0.0132\n 6     1 species       0.0120\n 7     1 environmental 0.0118\n 8     1 pollution     0.0108\n 9     1 information   0.0102\n10     1 university    0.0102\n# ℹ 63 more rows"
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#visualizing-top-terms-by-topic",
    "href": "blog/2024-4-26-topics/index.html#visualizing-top-terms-by-topic",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Visualizing Top Terms by Topic",
    "text": "Visualizing Top Terms by Topic\nLet’s visualize the top terms for each topic using a bar plot.\n\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic, sep = \"\")) %&gt;% \n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = F) +\n  facet_wrap(~topic, scales = \"free_y\") +\n  scale_x_reordered() +\n  coord_flip()\n\n\n\n\nThis plot breaks down the most frequent words in each of the topic groups. The first topic group seems to be composed of articles related to academia with research, university, and science being some of the most frequent terms in the group. The fourth topic group has higher frequency of finance related terms like finance, risk, economic, and banks. These articles are likely related to the financial implications of biodiversity loss, which is one of the most underrated aspects of climate change in the general public in my opinion. The fifth topic group has key terms related to the energy sector and pollutants, and the sixth group has key words like food and water which signal a potential focus on agriculture for articles in this group."
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#examining-topic-proportions",
    "href": "blog/2024-4-26-topics/index.html#examining-topic-proportions",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Examining Topic Proportions",
    "text": "Examining Topic Proportions\nNext, let’s examine the topic proportions for a few example documents.\n\n# assign names to topics\ntopic_words &lt;- terms(topicModel_k7, 5)\ntopic_names &lt;- apply(topic_words, 2, paste, collapse = \" \")\n\n#specify # of examples to inspect\nexample_ids &lt;- c(1:5)\nn &lt;- length(example_ids)\n\n# get topic proportions from example documents\nexample_props &lt;- theta[example_ids,]\ncolnames(example_props) &lt;- topic_names\n\n#combine example topics with identifiers and melt to plotting form\nviz_df &lt;- melt(cbind(data.frame(example_props),\n                     document = factor(1:n),\n                     variable.name = \"topic\",\n                     id.vars = \"document\"))\n\nggplot(data = viz_df, aes(variable, value, fill = document),\n       ylab = \"proportion\") +\n  geom_bar(stat = \"identity\") +\n  coord_flip() + \n  facet_wrap(~ document, ncol = n)\n\n\n\n\nThis plot shows the importance of each topic group in each respective article. For example, the first article has the highest frequency of terms found in group two, while the second article has the most terms that define the third grouping. It’s important to note that the order of the groups is irrelevant, it is the most frequent terms that construct each of the topic groups that is useful."
  },
  {
    "objectID": "blog/2024-4-26-topics/index.html#takeaways",
    "href": "blog/2024-4-26-topics/index.html#takeaways",
    "title": "Topic Analysis: Biodiversity Loss",
    "section": "Takeaways",
    "text": "Takeaways\nThe topic analysis done in this blog post demonstrates the power of using text mining techniques to gain insights from large collections of textual data on complex issues like biodiversity loss. By applying methods like LDA topic modeling, we can extract the dominant themes and concepts discussed across numerous documents in an unsupervised manner.\nFor researchers, policymakers, and practitioners working on biodiversity conservation, having the ability to quickly identify and visualize the key topics can save valuable time and resources in their efforts. Some potential applications are:\n\nLiterature Review and Knowledge Synthesis: Topic modeling can help synthesize the vast amount of literature and reports on biodiversity loss by automatically surfacing the main themes, trends, and focus areas. This can accelerate literature reviews and help researchers quickly understand the outlook of knowledge on the topic.\nMonitoring Public Discourse: By analyzing news articles, social media posts, and other public textual data, topic models can reveal how different aspects of biodiversity loss are being discussed in public discourse. This information can help guide communication strategies and public outreach efforts by conservation organizations.\nPolicy Analysis: Policymakers can use topic modeling to analyze policy documents, stakeholder submissions, and public comments related to biodiversity policies and regulations. Understanding the dominant topics and concerns can inform evidence-based policymaking and support effective implementation of conservation measures.\nIdentifying Research Gaps: The topics extracted from existing literature can highlight potential gaps or understudied areas in biodiversity conservation. This information can aid future research agendas and funding priorities.\nCross-Disciplinary Collaboration: Topic modeling can reveal connections and overlaps between different disciplines studying biodiversity loss, such as ecology, environmental economics, and policy studies. This can promote interdisciplinary collaborations and knowledge exchange, which is critical in the fight against climate change.\n\nWhile topic analysis is an exploratory strategy and is best coupled with domain expertise and further analysis, it provides a powerful lens for obtaining data-driven insights from large text datasets. As the volume of text data continues to grow, leveraging techniques like topic modeling will become increasingly valuable for understanding complex environmental issues and informing decision-making processes related to biodiversity conservation efforts."
  },
  {
    "objectID": "blog/2023-12-1-ethics-proj/index.html",
    "href": "blog/2023-12-1-ethics-proj/index.html",
    "title": "The AI Paradox in Environmental Research - Balancing Innovation with Ecological Impact",
    "section": "",
    "text": "Introduction\nIn this epoch of technological evolution, artificial intelligence stands at the forefront, bringing forth a paradigm shift in all domains, including environmental research. This shift, however, is not without its dichotomies. AI, while being a potent instrument in environmental conservation efforts that can help address the biggest issues in the field, contributes to environmental issues in its own right, primarily through its significant carbon emissions. In addition to high carbon output, AI systems have a high human cost, as the technology is deeply rooted in the exploitation of human capital. Training AI models has been a revolution in the sense that it has essentially broken Moore’s Law, which states that computational capacity doubles roughly every 18 months. An analysis of the OpenAI research lab found that its AI training models had been doubling in capacity every 3.4 months from 2012 to 2018.(OpenAI 2023) This is roughly a 300,000x increase from Moore’s Law over this six year period, a shocking result that speaks to the speed and scale at which AI is being integrated, scaled and refined. However, this incredible increase in the capacity of AI comes with a price that is paid by the environment since the energy outputs from training these models are quite high and leave behind a significant carbon footprint. The aviation industry will be used as a parallel case study to demonstrate the potential transformation the field of AI could undergo. This discussion aims to delve deeper into this paradox of the benefits that AI brings to solving environmental problems while also realizing the negative environmental impact of these systems, exploring AI’s dual role in environmental research and its broader implications. Understanding and addressing AI’s environmental footprint is not just a technological imperative but a moral and ecological one that is only to get more nuanced as AI continues its sweep over modern civilization.\n\n\n\nThe centrality of technology\n\n\n(Communications 2020)\n\n\nParadox of AI in Environmental Research\nAI’s multifaceted nature in environmental research is a blend of promise and peril. It offers groundbreaking capabilities in analyzing and predicting environmental changes, such as climate variations and pollution trends. However, the carbon footprint associated with training sophisticated AI models is substantial, thereby contributing to the very environmental challenges it seeks to mitigate.\nFirst, consider the promise. There are many incredible AI systems being built that aid in the advancement of carbon neutrality. The International Methane Emissions Observatory leverages AI to revolutionize approaches of monitoring and mitigating methane emissions. The platform is an open source, public database that connects methane emissions data with action on science, transparency and policy to inform the best possible data driven decisions.(UNEP 2022) CO2 AI is another powerful industry player that helps corporations measure, track, simulate and optimize their emissions at scale. The company’s AI tools implement deep learning techniques and graph theory to increase the accuracy of emission measurements. (AI 2023) A third example of a company using AI to address the climate crisis is Earth Insights, a collaborative effort between Hewlett Packard and Conservation International, which uses AI to monitor biodiversity loss of tropical forests worldwide with the goal of protecting these ecosystems through science and policy initiatives. (Moussa 2023) These are only three of the many companies and initiatives that are utilizing AI to reduce the effects of the impending climate crisis through a wide array of strategies. Improvements in energy grid efficiency, vehicle carbon output, building and city emissions, industry-related efficiency improvements in design, sourcing, manufacturing, and distribution, and farming are some other examples of the benefits AI brings to the table.\nIn the swiftly evolving landscape of AI development, a proliferating concern has emerged regarding the environmental impact of these systems. The University of Massachusetts Amherst conducted a pivotal study, focusing on the energy consumption and consequent carbon emissions of training Natural Language Processing (NLP) models. Their findings are stark, revealing that the carbon footprint of training a single large language model is roughly an astonishing 300,000 kg of CO2 emissions. (Strubell 2019) To put this into a more tangible perspective, consider someone driving a car or flying in an airplane. The average car, for instance, emits about 4.6 metric tons of carbon dioxide each year. In this context, the emissions from training one of these large language models equates to the yearly emissions of nearly 65 cars. Similarly, if we consider air travel, a single flight from New York to San Francisco generates about 1 ton of CO2 per passenger. Thus, the emissions from training a large language model are roughly equivalent to 300 such flights. These comparisons shed light on the environmental footprint of AI development, underscoring the need for more sustainable practices in this rapidly advancing field. The energy required for these systems will increase as these AI systems get more powerful and robust, so this problem will only become more pressing over time unless there is a significant shift in where AI systems receive their energy from. (Visuals 2015)\n\n\n\nOne metric ton of C02 for scale\n\n\n(Visuals 2015)\n\n\nQuantifying the Carbon Footprint\n\nChallenges in Measurement\nThe endeavor to accurately quantify AI’s carbon footprint is riddled with complexities. The muddied nature of energy consumption in data centers, coupled with the diverse methodologies used in AI operations, makes it challenging to pinpoint the exact environmental impact. Crawford and Joler’s insightful analysis sheds light on these hidden costs, revealing the extensive energy consumption behind AI’s operations. (Crawford 2018) Their work, published by the SHARE Lab SHARE Foundation and the AI Now Institute NYU, offers a profound insight into the often-overlooked environmental consequences of AI development​​. This intricate web of energy use, stretching from the vast data centers to the minutiae of algorithmic calculations, uncovers a distinct reality. The environmental footprint of AI is not merely a byproduct of its computational processes but a deeply embedded aspect of its very existence. As Crawford and Joler illustrate, every facet of AI, from its design to deployment, is intertwined with significant energy demands. (Crawford 2018) This revelation calls for a recalibration in the approach to building AI tools, urging a shift towards more sustainable practices that consider the long-term ecological impacts. To make matters worse, there is a lack of incentives for companies to share data and publicly display their emissions output. The incredible pace at which the AI and computation industry has evolved and globalized has led to a few players holding the majority of the control of this infrastructure and policy adaptation.\n\n\nTools and Methods\nIn the face of these challenges, the field has witnessed the advent of innovative tools aimed at more accurately measuring the energy consumption and emissions of AI processes. A noteworthy contribution is the emissions calculator developed by Alexandre Lacoste and his team. This tool represents a significant step forward in our ability to pragmatically estimate the carbon footprint associated with AI operations. The underlying research in creating this calculator underscores that emissions are intricately linked to several factors: the geographical location of the training server, the characteristics of the energy grid powering it, the duration of the training process, and the specific hardware utilized for the training. (Lacoste, n.d.) This issue transcends mere technological hurdles, veering into the realms of political will and consumer awareness. \nThere is a pressing need for increased transparency in the AI sector. Contrary to a lack of knowledge, companies are quite cognizant of the extent of training conducted on their hardware. They possess detailed insights into the computational demands of various algorithms, akin to the aviation industry’s awareness of the energy efficiency of air travel. In aviation, there are established standards and detailed reports outlining the hardware used in planes, their flight duration, and distance travele. Similarly, in the AI industry, adopting such standardized reporting and transparency could lead to more informed choices and practices from people actually using the AI. It is vital to draw parallels from sectors like aviation to instill a culture of accountability and sustainability in AI development. Just as the aviation industry has evolved with a focus on energy efficiency and transparency, the AI sector too must embrace these values. This shift not only demands technological innovation but also a concerted effort from policymakers, industry leaders, and consumers to foster an environment where sustainable AI development is not just a choice but an expectation.\n\n\n\nImpact and Implications\nAI’s carbon footprint undeniably casts a long shadow over environmental ecosystems, influencing them at both granular and broader scales. This paradoxical situation, where AI’s immediate benefits in environmental research are contrasted against the more protracted environmental impacts of its carbon emissions, forms a complex ethical tableau. Data centers, pivotal to AI operations, are now outpacing the aviation industry in greenhouse gas emissions. (Cho 2023)\nVenturing into renewable energy solutions for AI systems uncovers additional ecological concerns. Consider lithium, a critical component in the creation of rechargeable batteries. The extraction of this element is a water-intensive process; every ton of lithium extraction requires about 500,000 gallons of water.(IER 2020) This level of water consumption has profound environmental repercussions. In Chile, the world’s second-largest lithium producer, the indigenous Copiapó communities find themselves in a struggle with mining companies over vital land and water rights.(Greenfield 2022) These mining activities in regions like Salar de Atacama, Chile are so water-intensive that, according to the Institute for Energy Research, they account for 65% of the area’s water usage.(IER 2020) The resultant water loss inflicts severe damage on the local ecosystems, leading to the depletion of wetlands and water sources. Such environmental degradation has far-reaching effects, endangering native flora and fauna and severely impacting the lives and livelihoods of local populations. This situation presents a nuanced challenge: while strides in AI technology are heralded for their potential to address environmental issues, their underlying infrastructure and energy sources inadvertently contribute to ecological degradation. The pursuit of technological advancement in AI, therefore, necessitates a careful consideration of its environmental trade-offs, urging a thoughtful and sustainable approach to innovation. The ethical implications of using high-carbon-footprint AI in environmental research revolve around a fundamental conflict. This conflict lies in balancing the immediate utility of AI in research endeavors against the long-term environmental costs, raising questions about the ethical responsibilities of researchers and developers.\n\n\nMitigation Practices and Future Directions\n\nSocietal Adaptation\nThe responsibility of steering AI towards greener practices lies significantly with how societies will adapt and unlock the powers of AI in the environmental space. Collective efforts from researchers, developers, and other civilians are essential in pioneering sustainable AI development. Furthermore, understanding the realities and consequences of climate change can allow communities to have better practices when it comes to climate awareness and outcomes by prioritizing less destructive AI systems. These are the most critical avenues in which society can adapt AI in an environmentally-conscious manner:\n\nRaising ecological awareness about AI’s benefits is crucial. Enhanced data collection, through citizen science initiatives, advanced sensors, and remote monitoring, enriches our understanding and application of AI in environmental contexts. This wealth of data aids in crafting more accurate and responsive solutions to ecological challenges. In high-risk areas, spreading knowledge on crisis management becomes imperative. Gathering data through surveys and community engagement can provide invaluable insights into local needs and vulnerabilities.\nDeveloping disaster maps and emergency plans, bolstered by AI analytics, empowers communities to better safeguard themselves during crises. Proactive measures, such as timely alerts via text and email, must seamlessly integrate into our daily routines, ensuring preparedness becomes a norm rather than an exception.\nBeyond immediate responses, AI’s role in enhancing societal systems is significant. Optimizing food distribution and growth, minimizing waste, and ensuring equitable access to resources are areas where AI can make a substantial difference. The public health sector also stands to gain, with AI-driven solutions potentially improving healthcare delivery and outcomes, particularly in environmentally vulnerable communities.\nCrucially, AI can play a transformative role in building resilient infrastructures that are attuned to the demands of a changing climate. Implementing eco-conscious solutions like wetlands, seawalls, and stormwater ponds can significantly bolster defenses in susceptible regions. This requires a communal shift in perspective, embracing and supporting such infrastructures for their long-term benefits.\nIn addition, AI’s capability in detecting and addressing issues in energy grids and water systems marks a leap forward in infrastructure management. Modern systems and buildings, equipped with AI technologies, are more adept at preemptively identifying and rectifying environmental and operational challenges. Therefore, the integration of AI into our societal fabric, from data gathering to infrastructure development, heralds a new era of eco-conscious and efficient environmental management.\n\n\n\n\nUnlocking Nature through AI\n\n\n(BCG 2023)\n\n\nPolicy and Regulation\nEffective policies, legal frameworks, and comprehensive governmental backing are fundamental in steering AI to be more sustainable. These elements can guide the tech industry in adopting environmentally responsible practices, thereby achieving a crucial balance between technological advancement and ecological conservation. To realize this, global cooperation and standardized AI policies, akin to international climate change agreements, are essential. Such policies can harmonize and mitigate the varying environmental impacts of AI technologies across diverse regions. Mandating transparency and reporting standards is another key step. By requiring companies to disclose their energy consumption and carbon emissions related to AI activities, we can draw on the aviation industry’s approach to transparency, underscoring the potential benefits of such practices in the realm of AI.\nTax incentives also play a vital role. Tax credits, grants, and subsidies can motivate companies, researchers, and institutions to invest in AI solutions that address environmental challenges. Moreover, a regulatory framework focused on the energy consumption of data centers and AI operations is necessary. Setting energy efficiency benchmarks and enforcing penalties for non-compliance could significantly expedite addressing AI’s environmental footprint. Public awareness is an equally important facet. Educating the public about AI’s environmental impact can shift consumer demand towards sustainable AI products and services, thus nudging the market towards greener practices. This comprehensive approach, encompassing policy, regulation, incentives, and awareness, is imperative to shape AI’s future in a way that is both technologically innovative and environmentally responsible.\n\n\n\nConclusion\nAs we stand at the crossroads of technological advancement and environmental preservation, the role of artificial intelligence in environmental research embodies a profound paradox. AI’s potential to address some of the most pressing environmental challenges is indisputable, yet its substantial carbon footprint and ecological implications present a legitimate counterbalance. This juxtaposition demands a concerted effort from all sectors of society – particularly policymakers, financial institutions, researchers, and the tech community – to forge a path that harmonizes technological innovation with ecological responsibility.\nFor policymakers, the imperative is clear: to enact comprehensive, forward-thinking legislation that not only promotes sustainable AI practices but also holds the industry accountable for its environmental impact. This involves creating regulatory frameworks that encourage green innovation, enforce transparency in energy consumption and emissions, and support the development of environmentally friendly AI applications. Financial institutions have a pivotal role to play. By directing investments towards sustainable AI ventures and research, they can accelerate the shift towards environmentally conscious technologies. This shift not only aligns with global environmental goals but also opens avenues for sustainable economic growth and long-term profitability in the green technology sector. Researchers and the tech community are tasked with the continuous innovation of AI technologies, ensuring they align with environmental objectives. This includes improving the energy efficiency of AI models, exploring alternative, less carbon-intensive computing methods, and advancing AI applications in environmental monitoring, conservation, and sustainable resource management.\nAbove all, the journey towards an eco-friendly AI future is a collective one. It requires a paradigm shift in how we perceive and utilize technology – not as an end in itself but as a means to a greater goal of ecological sustainability. By integrating ethical considerations into AI development and harnessing its power to serve environmental needs, we can ensure that the AI-driven era ahead is not only technologically advanced but also environmentally conscious and sustainable. Let this be a call to action for everyone involved: to balance the scales between the immense potential of AI and the urgent need to protect and preserve our environment.\n\n\n\nWho Shapes AI?\n\n\n(State 2023)\n\n\n\n\n\nReferences\n\nAI, CO2. 2023. “Overview.” https://co2ai.com/.\n\n\nBCG. 2023. “How AI Can Speed Climate Action.” https://www.bcg.com/publications/2023/how-ai-can-speedup-climate-action.\n\n\nCho, Renee. 2023. “AI’s Growing Carbon Footprint.” https://news.climate.columbia.edu/2023/06/09/ais-growing-carbon-footprint/.\n\n\nCommunications, Nature. 2020. “The Role of Artificial Intelligence in Achieving the Sustainable Development Goals.” https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-019-14108-y/MediaObjects/41467_2019_14108_Fig5_HTML.png.\n\n\nCrawford, Kate et al. 2018. “Anatomy of AI Systems.” https://anatomyof.ai/img/ai-anatomy-publication.pdf.\n\n\nGreenfield, Nicole. 2022. “Lithium Mining Is Leaving Chile’s Indigenous Communities High and Dry (Literally).” https://www.nrdc.org/stories/lithium-mining-leaving-chiles-indigenous-communities-high-and-dry-literally#:~:text=Chile%20is%20the%20second%2Dlargest,transition%20away%20from%20fossil%20fuels.\n\n\nIER. 2020. “The Environmental Impact of Lithium Batteries.” https://www.instituteforenergyresearch.org/renewable/the-environmental-impact-of-lithium-batteries/.\n\n\nLacoste, Alexandre et al. n.d. “Quantifying the Carbon Emissions of Machine Learning.” https://arxiv.org/abs/1910.09700.\n\n\nMoussa, Raed. 2023. “HP - Earth Insights.” https://raedmoussa.com/hp-earth-insights.\n\n\nOpenAI. 2023. https://openai.com/research/ai-and-compute.\n\n\nState, US Department of. 2023.\n\n\nStrubell, Emma. 2019. “Machine Learning Models for Efficient and Robust Natural Language Processing.” https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=2805&context=dissertations_2.\n\n\nUNEP. 2022. “How Artificial Intelligence Is Helping Tackle Environmental Challenges.” https://www.unep.org/news-and-stories/story/how-artificial-intelligence-helping-tackle-environmental-challenges.\n\n\nVisuals, Carbon. 2015. “One Metric Ton of Carbon.” https://www.carbonvisuals.com/projects/tag/carbon."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html",
    "href": "blog/2023-12-1-stats-proj/index.html",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "",
    "text": "When we scrutinize the relationship between capitalism and environmental impacts, we’re engaging in a complex dialogue about efficiency, freedom, and sustainability. Markets act as a powerful mechanism for efficient resource allocation. Yet market-based economies, with the intrinsic drive for growth, often neglect the environmental dimension and impact. This oversight manifests in a key shortcoming: the failure to incorporate environmental costs into the pricing of goods and services.\nThe role of freedom in global economies brings about a dual impact on the environment. The positive aspects are rooted in the innovative potential and efficiency that economic freedom encourages. In a capitalist system, the competitive market can be a catalyst for discovering sustainable and economical production methods. Furthermore, the wealth generated in such systems can boost both private and public investments in environmental initiatives. Regulatory efficiency also stands out as an advantage, offering the ability to quickly adapt and respond to emerging environmental challenges. Unfortunately, there are many reasons why this idealized version falls short. At the end of the day, addressing the climate crisis is a collective action dilemma, since the actions of one person are negligible in the grand scheme of things. This makes it difficult for consumers to support climate-focused goods and services in exchange for relatively cheaper alternatives.\nNow, the ugly. Capitalism often leads to aggressive resource use, resulting in issues like deforestation, loss of biodiversity, and water pollution. The tendency of capitalism to prioritize short-term profit over long-term sustainability creates a blind spot for environmental considerations. This short-sighted focus on immediate financial returns often overshadows the broader, more enduring impacts on the environment. In fact, the wealthiest 10 percent of the global population are responsible for half of global emissions. (Reid 2023) Additionally, the burden of environmental degradation in capitalist systems is not evenly distributed. Often, it’s the less affluent communities that bear the worst of this degradation, leading to a disparity in environmental impact and quality of life.\nWhile capitalism has the potential to foster innovation and generate funds that could benefit environmental conservation, resource exploitation and prioritization of short-term profits present substantial challenges to achieving true environmental sustainability.\n\n\nThere is certainly a level of subjectivity, or at least a certain amount of uncertainty, when scoring the freedom of an economy. As a result, it seems impossible to create a perfect score for economic freedom. However, this doesn’t mean people haven’t tried to create a spectrum to measure how free and open different economies are.\nThe Economic Freedom of the World: 2022 Annual Report serves as the backbone of the analysis in this exploration. The data set has a multitude of columns, the most important of which is gives each country an economic freedom index score on a scale from 1 to 10. According to the Fraser Institute, the pillars of their scoring of economic freedom depend on “personal choice, voluntary exchange, freedom to enter markets and compete, and security of the person and privately owned property.” (Gwartney 2022) This economic freedom score is measured in five areas: size of government, legal system and property rights, sound money, freedom to trade internationally, and regulation.\nIn addition to the economic freedom index column, there are a plethora of interesting variables that can be analyzed in this data set.\nThe report discusses how countries that are have higher levels of economic freedom outperform less free countries in indicators of well being. Countries in the top quartile of economic freedom saw an average per-capita GDP of $48,251 in 2020, while countries in the bottom quartile for economic freedom had an average of $6,542. Furthermore, life expectancy in the top quartile was 80.4 years and 66 years in the bottom quartile in 2020 (Gwartney 2022) However, do these positive impacts of higher economic freedom also lead to better environmental outcomes? This analysis will put this question to the test.\n\n\n\nThis analysis will consider several environmental outcomes pulled from the World Bank website. These data include freshwater withdrawal as a proportion of available freshwater resources (water stress), net forest depletion as a percentage of GNI, renewable energy output as a percentage of total energy consumption, renewable energy consumption as a percentage of total energy consumption, and methane emissions in metric tons of CO2 per capita (WorldBank 2023) Combined with the economic freedom data, this will allow for the analysis to look at the relationship between economic freedom scores and related variables to environmental outcomes over time in different countries."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#introduction",
    "href": "blog/2023-12-1-stats-proj/index.html#introduction",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "",
    "text": "When we scrutinize the relationship between capitalism and environmental impacts, we’re engaging in a complex dialogue about efficiency, freedom, and sustainability. Markets act as a powerful mechanism for efficient resource allocation. Yet market-based economies, with the intrinsic drive for growth, often neglect the environmental dimension and impact. This oversight manifests in a key shortcoming: the failure to incorporate environmental costs into the pricing of goods and services.\nThe role of freedom in global economies brings about a dual impact on the environment. The positive aspects are rooted in the innovative potential and efficiency that economic freedom encourages. In a capitalist system, the competitive market can be a catalyst for discovering sustainable and economical production methods. Furthermore, the wealth generated in such systems can boost both private and public investments in environmental initiatives. Regulatory efficiency also stands out as an advantage, offering the ability to quickly adapt and respond to emerging environmental challenges. Unfortunately, there are many reasons why this idealized version falls short. At the end of the day, addressing the climate crisis is a collective action dilemma, since the actions of one person are negligible in the grand scheme of things. This makes it difficult for consumers to support climate-focused goods and services in exchange for relatively cheaper alternatives.\nNow, the ugly. Capitalism often leads to aggressive resource use, resulting in issues like deforestation, loss of biodiversity, and water pollution. The tendency of capitalism to prioritize short-term profit over long-term sustainability creates a blind spot for environmental considerations. This short-sighted focus on immediate financial returns often overshadows the broader, more enduring impacts on the environment. In fact, the wealthiest 10 percent of the global population are responsible for half of global emissions. (Reid 2023) Additionally, the burden of environmental degradation in capitalist systems is not evenly distributed. Often, it’s the less affluent communities that bear the worst of this degradation, leading to a disparity in environmental impact and quality of life.\nWhile capitalism has the potential to foster innovation and generate funds that could benefit environmental conservation, resource exploitation and prioritization of short-term profits present substantial challenges to achieving true environmental sustainability.\n\n\nThere is certainly a level of subjectivity, or at least a certain amount of uncertainty, when scoring the freedom of an economy. As a result, it seems impossible to create a perfect score for economic freedom. However, this doesn’t mean people haven’t tried to create a spectrum to measure how free and open different economies are.\nThe Economic Freedom of the World: 2022 Annual Report serves as the backbone of the analysis in this exploration. The data set has a multitude of columns, the most important of which is gives each country an economic freedom index score on a scale from 1 to 10. According to the Fraser Institute, the pillars of their scoring of economic freedom depend on “personal choice, voluntary exchange, freedom to enter markets and compete, and security of the person and privately owned property.” (Gwartney 2022) This economic freedom score is measured in five areas: size of government, legal system and property rights, sound money, freedom to trade internationally, and regulation.\nIn addition to the economic freedom index column, there are a plethora of interesting variables that can be analyzed in this data set.\nThe report discusses how countries that are have higher levels of economic freedom outperform less free countries in indicators of well being. Countries in the top quartile of economic freedom saw an average per-capita GDP of $48,251 in 2020, while countries in the bottom quartile for economic freedom had an average of $6,542. Furthermore, life expectancy in the top quartile was 80.4 years and 66 years in the bottom quartile in 2020 (Gwartney 2022) However, do these positive impacts of higher economic freedom also lead to better environmental outcomes? This analysis will put this question to the test.\n\n\n\nThis analysis will consider several environmental outcomes pulled from the World Bank website. These data include freshwater withdrawal as a proportion of available freshwater resources (water stress), net forest depletion as a percentage of GNI, renewable energy output as a percentage of total energy consumption, renewable energy consumption as a percentage of total energy consumption, and methane emissions in metric tons of CO2 per capita (WorldBank 2023) Combined with the economic freedom data, this will allow for the analysis to look at the relationship between economic freedom scores and related variables to environmental outcomes over time in different countries."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#data-wrangling",
    "href": "blog/2023-12-1-stats-proj/index.html#data-wrangling",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nFirst, let’s import the libraries we will need to conduct this analysis.\n\n# import libraries\nlibrary(here)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(tidyr)\nlibrary(forecast)\nlibrary(randomForest)\nlibrary(gplots)\nlibrary(ggforce)\n\n# clear environment for sanity\nrm(list = ls())\n\n\nData Cleaning\nNow, let’s import and clean the data containing the freedom index and other scores by country.\n\n# read in freedom data\nfreedom_raw &lt;- read.csv('data/efw_ratings.csv', header = FALSE)\n\nThis data needs a good bit of clean up. This next code chunk adjusts the column headers and converts column types appropriately.\n\n# set the 5th row as the column names\ncolnames(freedom_raw) &lt;- freedom_raw[5,]\n\n# remove the first 4 rows since they are now empty and clean names, remove columns \nfreedom &lt;- freedom_raw[-c(1:5), ] %&gt;% \n  clean_names() %&gt;% \n  subset(select = -na) %&gt;% \n  subset(select = c(-na_2, -na_3, -na_4, -na_5)) %&gt;% \n  subset(select = -world_bank_region) %&gt;% \n  subset(select = -world_bank_current_income_classification_1990_present)\n\n# convert year columns from char to num\nfreedom &lt;- freedom %&gt;%\n  mutate(across(6:ncol(freedom), as.numeric)) %&gt;% \n  mutate(economic_freedom_summary_index = as.numeric(as.character(economic_freedom_summary_index)))\n\nNow, lets move on to reading in the next dataset. The freedom data serves as the policy side of the data – now we want to append and compare environmental outcomes based on different political and economic factors.\n\n# read in esg data\nesg_wb &lt;- read.csv('data/esg_wb.csv') %&gt;% \n  clean_names()\n\nThis data also needs to be cleaned up a bit. Let’s get to work.\n\ncolumn_names &lt;- c(\"x1998_yr1998\", \"x1999_yr1999\", \"x2000_yr2000\", \n                  \"x2001_yr2001\", \"x2002_yr2002\", \"x2003_yr2003\", \n                  \"x2004_yr2004\", \"x2005_yr2005\", \"x2006_yr2006\", \n                  \"x2007_yr2007\", \"x2008_yr2008\", \"x2009_yr2009\", \n                  \"x2010_yr2010\", \"x2011_yr2011\", \"x2012_yr2012\", \n                  \"x2013_yr2013\", \"x2014_yr2014\", \"x2015_yr2015\", \n                  \"x2016_yr2016\", \"x2017_yr2017\", \"x2018_yr2018\", \n                  \"x2019_yr2019\", \"x2020_yr2020\", \"x2021_yr2021\", \n                  \"x2022_yr2022\")\n\n# function to extract and convert the year part of a column name to numeric\nextract_year &lt;- function(column_name) {\n  year_str &lt;- substr(column_name, 2, 5)\n  as.numeric(year_str)\n}\n\nfirst &lt;- names(esg_wb)[1:4]\n\n# apply the function to each column name\nnumeric_years &lt;- sapply(column_names, extract_year)\n\nnew_cols &lt;- c(first, numeric_years)\n\nnames(esg_wb) &lt;- new_cols\n\n# update esg data \nesg_wb &lt;- esg_wb %&gt;% \n  mutate(across(5:ncol(.), ~ as.numeric(as.character(.))))\n\n\n# make longer so it is compatible to join with freedom data\nesg_wb_long &lt;- esg_wb %&gt;%\n  pivot_longer(\n    cols = '1998':'2022', \n    names_to = \"Year\", \n    values_to = \"Value\" \n  ) \n\nFinally, let’s merge the data sets together by year and country name.\n\n# rename the country column in freedom dataset to match esg_wb_long\nnames(freedom)[names(freedom) == \"countries\"] &lt;- \"country_name\"\n\n# rename the year column in freedom dataset to match esg_wb_long\nnames(freedom)[names(freedom) == \"year\"] &lt;- \"Year\"\n\n# perform the join\nfreedom_esg &lt;- merge(freedom, esg_wb_long, by = c(\"Year\", \"country_name\"))\n\nfreedom_esg &lt;- freedom_esg %&gt;% \n  mutate(Year = as.numeric(as.character(Year)),\n         Value = as.numeric(as.character(Value)))\n\n# save dataset as a csv\nwrite.csv(freedom_esg, \"freedom_esg.csv\", row.names = FALSE)\n\nSweet! Now we have the dataset we will be working with in the analysis.\n\n\nData Filtering\nFor convenience, I have created a data frame for each of the environmental indicators that will be analyzed.\n\nwater_stress &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Level of water stress: freshwater withdrawal as a proportion of available freshwater resources\")\n\nforest_depletion &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Adjusted savings: net forest depletion (% of GNI)\")\n\nrenewable_output &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Renewable electricity output (% of total electricity output)\")\n\nrenewable_consumption &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Renewable energy consumption (% of total final energy consumption)\")\n\nmethane_emissions &lt;- freedom_esg %&gt;% \n  filter(series_name == \"Methane emissions (metric tons of CO2 equivalent per capita)\")"
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#analysis",
    "href": "blog/2023-12-1-stats-proj/index.html#analysis",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "Analysis",
    "text": "Analysis\n\nEmissions\nIn the first piece of analysis, let’s look at the methane emissions data. The units for which the emissions are recorded in the data set are metric tons of CO2 equivalent per capita.\nFirst, let’s compare the average emission by economic freedom quartile from 2000 to 2020.\n\n# create summary table for mean and stdev of methane emissions for each economic freedom quartile\nmethane_emissions_quartile &lt;- methane_emissions %&gt;% \n  group_by(Year, quartile) %&gt;% \n  summarize(avg_methane = mean(Value, na.rm = TRUE),\n            std_methane = sd(Value, na.rm = TRUE)) %&gt;% \n  na.omit()\n\n# create bar chart of the above summary table\nmethane_emissions_plot_with_error &lt;- ggplot(methane_emissions_quartile, aes(x = Year, y = avg_methane, color = as.factor(quartile))) +\n  geom_smooth(se = TRUE) +\n  labs(title = \"Methane Emissions Over Time by Economic Freedom Quartile\",\n       x = \"Year\",\n       y = \"Avg Methane Emissions (metric tons of CO2 per capita)\",\n       color = \"Economic Freedom Quartile\") +\n  theme_minimal()\n\n# display the plot\nmethane_emissions_plot_with_error\n\n\n\n\nInteresting, so the second quartile of economically free countries has the highest methane emissions by a significant amount across the entire time period. Overall, the emissions levels of all quartiles decreased from 2000 to 2020, a positive sign in the hopes of becoming a carbon-neutral planet.\nNext, let’s perform a linear regression on the economic freedom index and methane emissions to understand the effect of economic freedom on methane emissions.\n\n# run linear regression\nmethane_lm &lt;- lm(Value ~ economic_freedom_summary_index, data = methane_emissions)\n\nsummary(methane_lm)\n\n\nCall:\nlm(formula = Value ~ economic_freedom_summary_index, data = methane_emissions)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6361 -1.0029 -0.7013 -0.0672 14.2312 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     1.725557   0.233174   7.400 1.75e-13 ***\neconomic_freedom_summary_index -0.004037   0.034373  -0.117    0.907    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.028 on 3029 degrees of freedom\n  (511 observations deleted due to missingness)\nMultiple R-squared:  4.554e-06, Adjusted R-squared:  -0.0003256 \nF-statistic: 0.01379 on 1 and 3029 DF,  p-value: 0.9065\n\n\nThis regression reveals that there is essentially no relationship between methane emissions and economic freedom, as shown by the exceptionally high p-value. When the freedom score is increased by 1, the economic freedom decreases by a measely .004037. Let’s consider the analysis on a single year to see if this is any more significant (which shouldn’t be that hard to achieve!).\n\n# filter methane data for 2019\nmethane_emissions_2019 &lt;- methane_emissions %&gt;% \n  filter(Year == 2019) %&gt;% \n  na.omit()\n\n# run linear regression on 2019 data\nmethane_lm_2019 &lt;- lm(Value ~ economic_freedom_summary_index, data = methane_emissions_2019)\n\nsummary(methane_lm_2019)\n\n\nCall:\nlm(formula = Value ~ economic_freedom_summary_index, data = methane_emissions_2019)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3527 -0.7840 -0.6130 -0.1845 10.2183 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                      0.6073     1.3714   0.443    0.659\neconomic_freedom_summary_index   0.1196     0.1932   0.619    0.537\n\nResidual standard error: 1.736 on 94 degrees of freedom\nMultiple R-squared:  0.004061,  Adjusted R-squared:  -0.006534 \nF-statistic: 0.3833 on 1 and 94 DF,  p-value: 0.5373\n\n\nYikes! Not any better - a negative adjusted r-squared value. This is a sign that the model is a poor fit for the data and is less informative than the mean of the dependent variable. It can also be due to overfitting, but this is not the case since we are only using one predictor variable. Maybe there is a combination of variables from the multitude of columns coming from the freedom data set that could be useful and informative in predicting methane emissions. Let’s take a look.\nThe code below finds the 10 variables that have the strongest correlation to methane emissions.\n\n# make sure 'Value' is numeric\nmethane_emissions$Value &lt;- as.numeric(methane_emissions$Value)\n\n# select only numeric columns (excluding 'Value' for now)\nmethane_numeric &lt;- methane_emissions %&gt;% \n  select_if(is.numeric) %&gt;% \n  dplyr::select(-Value)\n\n# calculate correlation of each numeric column with the 'Value' column\nmethane_correlations &lt;- sapply(methane_numeric, function(x) {\n  if(is.numeric(x)) {\n    return(cor(x, methane_emissions$Value, use = \"complete.obs\"))\n  } else {\n    return(NA)\n  }\n})\n\n# convert to a dataframe \nmethane_corr_results &lt;- as.data.frame(methane_correlations)\n\n# sort by the absolute value to find the strongest correlations\nmethane_sorted_correlations &lt;- methane_corr_results %&gt;% \n  rownames_to_column(\"series\") %&gt;% \n  arrange(desc(abs(methane_corr_results)))\n\n# display results\nhead(methane_sorted_correlations, 10)\n\n                                           series methane_correlations\n1                              ie_state_ownership           -0.3134906\n2                                            data            0.3073977\n3                                          data_4           -0.3000107\n4             x3b_standard_deviation_of_inflation           -0.2831654\n5                      x1a_government_consumption           -0.2736432\n6                                          data_5           -0.2680756\n7                          gender_disparity_index           -0.2157412\n8  x1dii_top_marginal_income_and_payroll_tax_rate            0.2132415\n9                           x1_size_of_government           -0.2067589\n10                      x1d_top_marginal_tax_rate            0.2024452\n\n\nThese are the 10 variables that have the strongest correlation with methane emission levels. For context: the data variable represents government consumption, data_4 represents the top marginal income tax rate, and data_5 represents top marginal tax rate. It is essential to consider whether there is a causal relationship between the variables and the relationships with methane emissions must be investigated on a deeper level to get a sense of the true influence.\nLet’s consider the hypothesis that a strong government with stable currency and lower regulatory influence will have higher methane emissions due to the amount of profitability that can be gained through traditional methane production of goods.\n\nmethane_emission_2020_selected &lt;- methane_emissions %&gt;% \n  filter(Year == 2020) %&gt;% \n  dplyr::select(country_name, x1_size_of_government, x3_sound_money, x5_regulation, Value, quartile) %&gt;% \n  na.omit()\n\nemissions_lm_govt &lt;- lm(Value ~ x1_size_of_government + x3_sound_money + x5_regulation, data = methane_emission_2020_selected)\n\nsummary(emissions_lm_govt)\n\n\nCall:\nlm(formula = Value ~ x1_size_of_government + x3_sound_money + \n    x5_regulation, data = methane_emission_2020_selected)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7025 -0.9243 -0.5214  0.1214 10.3849 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)            2.00245    1.22304   1.637   0.1036  \nx1_size_of_government -0.20041    0.11974  -1.674   0.0962 .\nx3_sound_money         0.04109    0.11744   0.350   0.7269  \nx5_regulation          0.07798    0.16736   0.466   0.6419  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.772 on 157 degrees of freedom\nMultiple R-squared:  0.02424,   Adjusted R-squared:  0.005599 \nF-statistic:   1.3 on 3 and 157 DF,  p-value: 0.2764\n\n\nThe results of the regression show that the strong government variable set I put together do not strongly predict the methane emission levels. Interpreting the coefficients: a one unit increase in the size of government is associated with a .20041 metric ton decrease in methane emission levels holding all else constant, a one unit increase in sound money is associated with an increase in a .04109 metric ton increase in emission levels holding all else constant, and a one unit increase in regulation leads to a .07798 metric ton increase in emission levels holding all else constant. The adjusted R-squared value is very low, suggesting that the model explains very little of the variability in methane emission levels. The p-value of .274 speaks to the overall statistical insignificance of the model.\nThere is not evidence that the variables in the freedom data set correlate strongly with methane emission outputs. Intuitively, this makes sense because essentially all nations generate some amount of their energy using methane since it is the traditional method of forming energy. Capitalist, socialist and communist countries alike are more likely to produce the amount of energy needed through whatever means necessary. Energy use is a complex subject and there are many more factors that go into methane emission output levels for each country. Unfortunately, the indices and scores in the freedom dataset are not able to capture all of these nuances and it would take a much larger data with a wider range of relevant columns to accurately predict methane emissions.\n\n\nRenewable Energy Consumption\nIncreasing the rate of renewable energy consumption is imperative in the fight against climate change. While it would be logical to assume that private and public ensurers alike would react to the impending climate crisis by increasing demand in renewable energy, this is not really the case. To make matters worse, oil consumption is actually increasing with ambiguous signs of slowing down. (Lawler 2023)\nPlotting renewable energy percentage levels as a histogram can give us a better understanding of the distribution across all countries in 2020.\n\n# filter renewable energy consumption for 2020\nrenewable_consumption_2020 &lt;- renewable_consumption %&gt;% \n  filter(Year == 2020)\n\n# plot renewable energy consumption histogram.\nggplot(renewable_consumption_2020, aes(x = Value)) + \n  geom_histogram(binwidth = 3, fill = \"blue\", color = \"black\") + \n  labs(title = \"Histogram of Renewable Energy Consumption Percentage\",\n       x = \"Percentage of Renewable Energy Consumption (of total energy use)\",\n       y = \"Frequency\") +\n  theme_minimal() \n\n\n\n\nThe distribution is strongly right-skewed with a long right tail. There is the highest density of values from zero to about 25 percent, with a strong tail for values above 75% as well.\nNow, I want to investigate whether there is a significant difference in renewable energy consumption between the first and fourth economic freedom quartiles. We can utilize a Welch’s two-sample t-test and create new dataframes of each quartile for the analysis.\n\n# subset renewable energy consumption for Q1 of economic freedom \nquartile_1 &lt;- renewable_consumption %&gt;% \n  filter(quartile == 1) %&gt;%\n  select(Value)\n\n# subset renewable energy consumption for Q4 of economic freedom \nquartile_4 &lt;- renewable_consumption %&gt;% \n  filter(quartile == 4) %&gt;%\n  select(Value)\n\n# Perform Welch's t-test\nt_test_result &lt;- t.test(quartile_1$Value, quartile_4$Value)\n\n# View the results\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  quartile_1$Value and quartile_4$Value\nt = -24.969, df = 1102.2, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -36.61378 -31.27871\nsample estimates:\nmean of x mean of y \n 20.18034  54.12658 \n\n\nThis very small p-value indicates a statistically significant difference in renewable energy consumption rates between the first and fourth economic freedom quartiles. The mean of the fourth quartile is significantly higher than the first quartile. It’s important to remember that this result does not imply causation. The differences could be influenced by a multitude of factors, such as natural resources availability, technological advancements, and public awareness about renewable energy in different countries.\nFinally, let’s look at renewable energy consumption rates across each economic freedom quartile from 2000 to 2020.\n\n# group renewable consumption data by year and quartile\nrenewable_consumption_summary &lt;- renewable_consumption %&gt;% \n  filter(Year &lt;= 2020) %&gt;% \n  group_by(Year, quartile) %&gt;% \n  summarize(avg_consumption = mean(Value, na.rm = TRUE)) %&gt;% \n  na.omit()\n\n# plot average renewable consumption rate by quartile\nrenewable_consumption_quartile_plot &lt;- ggplot(renewable_consumption_summary, aes(x = Year, y = avg_consumption, color = as.factor(quartile))) +\n  geom_line(size = 1.2) +\n  labs(title = \"Renewable Energy Consumption Rates Across Economic Freedom Quartiles\",\n       x = \"Year\",\n       y = \"Average Renewable Energy Consumption (%)\",\n       color = \"Economic Freedom Quartile\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# display plot\nrenewable_consumption_quartile_plot\n\n\n\n\nThe graph shows that renewable energy consumption rate average of the lowest quartile is the highest among all quartiles, with the 3rd quartile being the second highest by a significant margin from 2000 all the way up until 2020. This is interesting as it reveals that countries with less economic freedom tend to have higher renewable energy consumption rates. It’s possible this is because of government regulation that mandates a certain level of consumption be renewable. However, there are other alternative hypotheses as well. For instance, it could be that since we know countries that have less freedom tend to have lower GDP and overall economic output, that they simply need less energy overall and can therefore rely more on renewable energy consumption, which might be cheaper and more accessible today that the alternative of coal or oil fueled. Additionally, fostering a preference for renewable energy in poorer, less free countries can decrease the reliance on oil and natural gas, leading to energy portfolios that are more resilient to fluctuations and spikes in prices. This dynamic will be investigated later in the omitted variable bias section.\n\n\nRenewable Energy Output\nRenewable energy output is valuable in determining a country’s economic impact since energy that is consumed can be exported to other countries for consumption. Furthermore, certain countries are better set up to capitalize on the economic gains of producing renewable energy due to regional climate conditions. For example, Costa Rica can produced so much renewable energy that it has enough to export it to countries in the Central American Regional Electricity Market. (Council 2022) Having renewable output rates closer to 100 bodes well for a countries long-term resilience and preparedness for increases in oil prices and the impending need for a total shift to renewable energy. These countries with higher rates are “ahead of the curve” but there are lots of variables that may still make energy demands difficult to meet due to variables outside of that country’s control. Let’s investigate how economic freedom and renewable energy output are related. Are countries with less economic freedom more likely to have higher renewable energy outputs since the government has more control over the market and regional demand for energy? Or are more free economies more likely to report higher renewable energy outputs due to more natural market mechanisms?\n\n# run linear regression of economic freedon on renewable energy output\nrenewable_output_lm &lt;- lm(Value ~ economic_freedom_summary_index, data = renewable_output)\n\nsummary(renewable_output_lm)\n\n\nCall:\nlm(formula = Value ~ economic_freedom_summary_index, data = renewable_output)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.717 -29.203  -8.951  26.743  72.335 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     66.6093     4.3933  15.162  &lt; 2e-16 ***\neconomic_freedom_summary_index  -4.6307     0.6492  -7.133 1.32e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.3 on 2237 degrees of freedom\n  (1303 observations deleted due to missingness)\nMultiple R-squared:  0.02224,   Adjusted R-squared:  0.0218 \nF-statistic: 50.88 on 1 and 2237 DF,  p-value: 1.321e-12\n\n\nThe resulting p-value shows that there is a significant relationship between economic freedom and renewable energy output. Countries with higher economic freedom scores tend to have lower rates of renewable energy output. However, the adjusted r-squared value of .0218 says that the model descibes very little of the distribution of renewable energy output.\nNow, let’s investigate the renewable energy output across each freedom quartile over time.\n\n# group renewable consumption data by year and quartile\nrenewable_output_summary &lt;- renewable_output %&gt;% \n  filter(Year &lt;= 2015) %&gt;% \n  group_by(Year, quartile) %&gt;% \n  summarize(avg_output = mean(Value, na.rm = TRUE)) %&gt;% \n  na.omit()\n\n# plot average renewable consumption rate by quartile\nrenewable_output_quartile_plot &lt;- ggplot(renewable_output_summary, aes(x = Year, y = avg_output, color = as.factor(quartile))) +\n  geom_line(size = 1.2) +\n  labs(title = \"Renewable Energy Output Rates Across Economic Freedom Quartiles\",\n       x = \"Year\",\n       y = \"Average Renewable Energy Output Rate (%)\",\n       color = \"Economic Freedom Quartile\") +\n  theme_minimal()\n\n# display plot\nrenewable_output_quartile_plot\n\n\n\n\nWe have to filter the years from 2000 to 2015 since the data set does not contain many values for renewable energy output after 2015. The results are similar to the renewable energy consumption plot: the fourth quartile has the highest output rate, followed by the third quartile. Notably, the most free quartile has seen a strong increase in renewable energy output from 2010 to 2015, which bodes well for future outlook of renewable energy in more open countries.\nNext, let’s see confirm the statement that there is a significant difference in renewable energy output across each quartile of economic freedom with the help of an ANOVA test.\n\nrenewable_output$quartile &lt;- factor(renewable_output$quartile)\n\nanova_ro &lt;- aov(Value ~ quartile, data = renewable_output)\n\nsummary(anova_ro)\n\n              Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nquartile       3   62414   20805   18.79 4.93e-12 ***\nResiduals   2235 2475053    1107                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1303 observations deleted due to missingness\n\n\nThe p-value of 4.93e-12 shows a significant difference across economic freedom quartiles for renewable energy output. This checks out with the time series graph shown above, especially when considering the stark differences between the fourth versus the first and second quartiles.\n\n\nForest Depletion\nDeforestation is a global issue that exacerbates the climate crisis. Countries that cut down trees at higher rates for timber and clearing land for agriculture come at the expense of the health of ecosystems around the world. Are countries with more freedom cutting down more forests to exploit the profit it has to offer? Or are less free countries that are economically limited, and likely poorer, more likely to do so?\n\n# generate summary table for forest depletion for 2020\nforest_depletion_2020_quartile_summary &lt;- forest_depletion %&gt;% \n  group_by(quartile) %&gt;% \n  filter(Year == 2020) %&gt;% \n  summarize(avg_depletion = mean(Value, na.rm = TRUE),\n            std_depletion = sd(Value, na.rm = TRUE))\n\n\n# plotting above summary table\nggplot(forest_depletion_2020_quartile_summary, aes(x = factor(quartile), y = avg_depletion)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), fill = \"skyblue\") +\n  geom_errorbar(aes(ymin = avg_depletion - std_depletion, ymax = avg_depletion + std_depletion), \n                width = 0.2, position = position_dodge(0.9)) +\n  labs(title = \"Average Forest Depletion Index by Quartile in 2020\",\n       x = \"Economic Freedom Quartile\",\n       y = \"Average Forest Depletion Index\") +\n  theme_minimal()\n\n\n\n\nThis visualization provides some strong takeaways. The least economically free quartile has significantly higher rates of deforestation than the other three quartiles, as the mean for the 4th quartile is roughly 3.5 while the next highest quartile index is less than 1. Furthermore, the most economically free quartile has seen almost no deforestation based on the index score that is very close to zero.\nNow let’s look at deforestation indices over time to see if there is a temporal element to the relationship between economicfreedom and deforestation.\n\n# aggregate the forest depletion data by year and quartile\nforest_depletion_time_series &lt;- forest_depletion %&gt;%\n  group_by(Year, quartile) %&gt;%\n  summarize(avg_depletion = mean(Value, na.rm = TRUE),\n            std_depletion = sd(Value, na.rm = TRUE)) %&gt;%\n  na.omit()\n\n# plot a line graph \nggplot(forest_depletion_time_series, aes(x = Year, y = avg_depletion, color = as.factor(quartile))) +\n  geom_line(size = 1.2) +  # Increase line thickness\n  labs(title = \"Average Forest Depletion Score by Quartile (2000-2020)\",\n       x = \"Year\",\n       y = \"Average Forest Depletion Score\",\n       color = \"Economic Freedom Quartile\") +\n  theme_minimal()\n\n\n\n\nThe average forest depletion in the least economically free countries appears to be significantly higher than the other quartiles. Notably, the most free quartile has almost no forestation, with the second and third follow behind in order.\nLet’s run an ANOVA test to determine the exact statistical significance here, since there are multiple groups that we want to test for. Using an ANOVA test is just a t-test for multiple groups rather than just two, so for instance this process could be done in a similar fashion by comparing the first and fourth quartiles through a t-test\n\nforest_depletion_2020 &lt;- forest_depletion %&gt;% \n  filter(Year == 2020) %&gt;% \n  na.omit()\n\nanova_fd &lt;- aov(Value ~ quartile, data = forest_depletion_2020)\n\nsummary(anova_fd)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nquartile     1  34.33   34.33   14.73 0.000228 ***\nResiduals   91 211.99    2.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value of 0.000228 reveals that there is a significant relationship between the economic freedom quartile and forest depletion across quartiles.\n\n\nWater Stress\nWater stress is significantly exacerbated by climate change, leading to more frequent and severe droughts and water shortages. As global temperatures rise, altered precipitation patterns and increased evaporation rates strain water resources, impacting agriculture, ecosystems, and human populations in a degrading manner. The escalating water stress due to climate change also heightens the risk of regional and global conflicts over water resources and poses challenges for existing water management systems. Furthermore, climate change-induced water stress is likely to have massive effects on food security, public health, and economic stability, especially in vulnerable regions already facing the harsh realities of water scarcity. This section of analysis looks into the dynamic between economic freedom and water stress.\nFirst, let’s determine the variables that have the strongest correlation with water stress in the data sets.\n\n# make sure `Value` is numeric\nwater_stress$Value &lt;- as.numeric(water_stress$Value)\n\n# filter for numeric columns\nwater_stress_numeric &lt;- water_stress %&gt;% \n  select_if(is.numeric) %&gt;% \n  dplyr::select(-Value)\n\n# calculate correlation \nwater_stress_correlations &lt;- sapply(water_stress_numeric, function(x) {\n  if(is.numeric(x)) {\n    return(cor(x, water_stress$Value, use = \"complete.obs\"))\n  } else {\n    return(NA)\n  }\n})\n\n# convert to a dataframe for simplicity\nwater_stress_correlation_results &lt;- as.data.frame(water_stress_correlations)\n\n# sort by the absolute value of correlation\nwater_stress_sorted_correlations &lt;- water_stress_correlation_results %&gt;% \n  rownames_to_column(\"series\") %&gt;% \n  arrange(desc(abs(water_stress_correlation_results)))\n\n# display results\nhead(water_stress_sorted_correlations, 10)\n\n                                           series water_stress_correlations\n1                                          data_4                -0.3370701\n2                                          data_5                -0.3331397\n3                          gender_disparity_index                -0.3300405\n4  x1dii_top_marginal_income_and_payroll_tax_rate                 0.2724939\n5                              ie_state_ownership                -0.2573339\n6                       x1d_top_marginal_tax_rate                 0.2517648\n7             x3b_standard_deviation_of_inflation                -0.2345262\n8                                            data                 0.2053606\n9                      x1a_government_consumption                -0.2029093\n10                           x2h_police_and_crime                 0.1872722\n\n\nFor reference, the following ambiguous columns represent the following: data_4 is the top marginal income tax rate value, data_5 is top marginal income and payroll tax rate value, and data is the government consumption value.\nUsing these values, let’s see what happens when we put together a multi-linear regression where the independent variables are strong fiscal and government related variables\n\nwater_stress_lm &lt;- lm(Value ~ data + data_4 + ie_state_ownership, data = water_stress)\n\nsummary(water_stress_lm)\n\n\nCall:\nlm(formula = Value ~ data + data_4 + ie_state_ownership, data = water_stress)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-529.5  -99.8  -16.2   49.0 3378.4 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        331.9590    32.5042  10.213   &lt;2e-16 ***\ndata                 8.0161     0.7034  11.396   &lt;2e-16 ***\ndata_4              -7.4694     0.4453 -16.774   &lt;2e-16 ***\nie_state_ownership -28.5481     3.9658  -7.199    8e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 288.1 on 2514 degrees of freedom\n  (1024 observations deleted due to missingness)\nMultiple R-squared:  0.1861,    Adjusted R-squared:  0.1851 \nF-statistic: 191.6 on 3 and 2514 DF,  p-value: &lt; 2.2e-16\n\n\nInterpreting the key results of the regression: a one unit increase in government consumption value leads to an 8.0161 increase in water stress, holding all else constant. A one unit increase in the top marginal income tax leads to a 7.4694 decrease in water stress levels, holding all else constant. Finally, a one unit increase in state ownership leads to a 28.5481 decrease in water stress levels, holding all else constant. The adjusted r-squared value of .1851 tells us that 18.51 percent of the variation in water stress is described by the independent variables used in the model. While the p-value states that the model is statistically significant, more investigation should be done to understand the nuances of the relationship. Water stress is largely a geographical issue as well since some areas are inherently more likely to face water shortages because of their climatic conditions, such as arid regions, or due to geographical features that limit access to sustainable freshwater sources.\n\n\nSimilarity Function\nThe Euclidean distance calculation is a mathematical tool to measure the degree of similarity between countries. It calculates the ‘distance’ between points in a multi-dimensional space, where each dimension corresponds to a specific attribute of economic freedom. This method is particularly effective in identifying countries with similar economic profiles, as it provides a straightforward, yet comprehensive, quantitative measure of similarity. By considering multiple economic factors simultaneously, the Euclidean distance offers a nuanced understanding of how closely aligned countries are in terms of their economic policies and practices.\nBelow is a function that determines the Euclidean distance for a certain inputted country. The similarity is determined by the five groups that make up the economic freedom index score: government size, property rights, sound money, freedom to trade internationally, and government regulation.\n\nfind_similar_countries_w_methane &lt;- function(data, target_country, top_n) {\n  # filter the dataset for the year 2020\n  data_2020 &lt;- filter(data, Year == 2020)\n\n  # select relevant columns (5 areas of economic freedom score and score itself)\n  relevant_data &lt;- data_2020 %&gt;%\n    dplyr::select(country_name, x1_size_of_government, x2_legal_system_property_rights_with_gender_adjustment, x3_sound_money, x4_freedom_to_trade_internationally, x5_regulation) %&gt;%\n    na.omit()\n\n  # normalize the data\n  normalized_data &lt;- as.data.frame(scale(relevant_data[,-1]))\n  normalized_data$country_name &lt;- relevant_data$country_name\n\n  # calculate Euclidean distances\n  target_country_data &lt;- normalized_data %&gt;% filter(country_name == target_country)\n  distances &lt;- apply(normalized_data[,-ncol(normalized_data)], 1, function(x) {\n    sqrt(sum((x - unlist(target_country_data[,-ncol(target_country_data)]))^2))\n  })\n\n  # combine distances with country names\n  distance_data &lt;- data.frame(country_name = normalized_data$country_name, \n                              eucl_distance = distances)\n\n  # rank countries by distance\n  similar_countries &lt;- distance_data %&gt;% \n    arrange(eucl_distance) %&gt;% \n    filter(country_name != target_country)\n  \n  # filter methane emissions data for 2020\n  emissions_2020 &lt;- methane_emissions %&gt;% \n    filter(Year == 2020)\n\n  # Join the methane emissions data\n  similar_countries_with_emissions &lt;- merge(similar_countries, emissions_2020, by = \"country_name\") %&gt;% \n    dplyr::select(country_name, eucl_distance, Value) %&gt;% \n    arrange(eucl_distance)\n  \n  top_countries &lt;- head(similar_countries_with_emissions, top_n)\n\n  return(top_countries)\n\n}\n\nNow, let’s test the function. Denmark is renowned for having the most climate-friendly policies and overall perspective on environmental issues. This next step of analysis investigates whether a countries Euclidean distance, or similarity, to Denmark using the economic freedom categories can relate to methane emissions output. Simply put: is being more similar to Denmark a good predictor of methane emission level per country?\n\nfind_similar_countries_w_methane(freedom, \"Denmark\", 10)\n\n   country_name eucl_distance     Value\n1       Finland     0.6396362 0.8149538\n2    Luxembourg     0.6477164 0.8815772\n3   Netherlands     0.8955070 0.8715177\n4     Australia     0.9625143 5.1250548\n5        Sweden     0.9671643 0.4628503\n6        Norway     0.9784877 0.8072835\n7         Japan     1.0060645 0.2042065\n8        Canada     1.0504405 2.6348554\n9   New Zealand     1.0715922 6.3341199\n10      Germany     1.0751399 0.5641788\n\n\nThe 10 countries display are the countries that are the most similar to Denmark based on the five economic indices analyzed. Off first glance it seems like many of the countries in this list have friendly environmental policies, so that’s a good sign!\nWe can implement a linear regression to determine the significance between this calculated Euclidean distance and methane emissions. This will determine if there is a significant relationship between the similarity to Denmark and methane emissions.\n\ndenmark_similarity &lt;- find_similar_countries_w_methane(freedom, \"Denmark\", 200)\n\neucl_model &lt;- lm(Value ~ eucl_distance, data = denmark_similarity)\n\nsummary(eucl_model)\n\n\nCall:\nlm(formula = Value ~ eucl_distance, data = denmark_similarity)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6035 -0.9326 -0.6405  0.0336 10.3393 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.90809    0.34503   5.530  1.3e-07 ***\neucl_distance -0.09982    0.08339  -1.197    0.233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.78 on 158 degrees of freedom\nMultiple R-squared:  0.008987,  Adjusted R-squared:  0.002715 \nF-statistic: 1.433 on 1 and 158 DF,  p-value: 0.2331\n\n\nThe p-value of .2331 shows that there is not a significant difference in the methane emissions for countries that are have more similar economic freedom profiles. In fact, the eucl_distance coefficient of -.09982 tells us that countries that are one unit of distance further actually have lower methane emissions. The adjusted r-squared value of .002715 says that essentially none of the variance in the model can be described by the Euclidean distance.\nThe results tell us that more research is needed to explore the relationship between economic freedom and methane emissions more comprehensively. Future investigations could involve expanding the dataset to include more years or different economic indicators, potentially revealing unique trends or stronger correlations. Qualitative research, including case studies of specific countries, could provide deeper insights into the underlying mechanisms driving the relationship between economic freedom and water stress."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#omitted-variable-bias",
    "href": "blog/2023-12-1-stats-proj/index.html#omitted-variable-bias",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nAs mentioned earlier in the blog post, the freedom data description mentions that the average GDP in the most economically free countries was much higher than that in the least free countries. Omitted variable bias arises when a relevant variable that significantly influences the dependent variable is not included in the model. In this case, GDP is almost certainly influencing both methane emissions and the economic freedom score. To investigate this potential omitted variable bias, I will read in some more data from the WorldBank to try to better understand how incorporating GDP can aid in the analysis on the relationship between economic freedom and methane emission levels.\nFirst, let’s read in the GDP data from WorldBank. The data set contains total GDP values across the globe from 1960 to 2022. It is important to note that there is a fair amount of missing data, mostly from years before 2000.\n\ngdp &lt;- read.csv(here(\"blog/2023-12-1-stats-proj/data/gdp/gdp_data_clean.csv\"), fill = TRUE)\n\nNext up is manipulating the data so it can be joined with freedom_esg.\n\n# reshape GDP data\ngdp_long &lt;- gdp %&gt;%\n  pivot_longer(cols = starts_with(\"X\"), names_to = \"Year\", values_to = \"GDP\") %&gt;%\n  mutate(Year = as.numeric(sub(\"X\", \"\", Year)))\n\n# make Year column numeric\nfreedom_esg &lt;- freedom_esg %&gt;%\n  mutate(Year = as.numeric(Year))\n\n# join the freedom_esg data with gdp_long\ncombined_data &lt;- left_join(gdp_long, freedom_esg, by = c(\"Country.Name\" = \"country_name\", \"Year\")) %&gt;% \n  drop_na()\n\n\ncountries_per_quartile &lt;- combined_data %&gt;% \n  group_by(quartile) %&gt;%\n  summarise(num_countries = n_distinct(Country.Name))\n\n# Print the results\nprint(countries_per_quartile)\n\n# A tibble: 4 × 2\n  quartile num_countries\n     &lt;dbl&gt;         &lt;int&gt;\n1        1            34\n2        2            33\n3        3            26\n4        4            24\n\n\nUnfortunately, some countries were not successful in the join, so we will roll with this new, shorter merged data set. The distribution across each economic freedom quartile is passable with at least 24 countries in each quartile remaining. Furthermore, data is now from 2017 to 2021.\nLet’s look renewable energy consumption versus GDP, as we have discussed this relationship is very likely to be experiencing omitted variable bias when analyzed in the earlier section of the blog post.\n\n# Filter for Renewable Energy Consumption\nrenewable_energy_data &lt;- combined_data %&gt;%\n  filter(series_name == \"Renewable energy consumption (% of total final energy consumption)\") %&gt;% \n  filter(Year == 2020)\n\n# Create the scatter plot\nggplot(renewable_energy_data, aes(x = Value, y = GDP, color = as.factor(quartile))) +\n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"yellow\")) +  # Adjust the colors as needed\n  labs(title = \"Renewable Energy Consumption vs GDP in 2020\",\n       x = \"Renewable Energy Consumption (% of Total Final Energy Consumption)\",\n       y = \"GDP (current US$)\",\n       color = \"Quartile\") +\n  theme_minimal()\n\n\n\n\nLet’s try log scaling the data since its a bit wacky and hard to read because of a few outliers and many countries within a similar region of GDP. Log transformation is a powerful technique that is particularly useful for working with skewed distributions and reducing the impact of outliers. By applying a logarithmic scale to the GDP data, extreme values are compressed to a smaller range, making the dataset more manageable and revealing patterns that may be obscured in the original scale. This transformation can also stabilize the variance across the data.\n\n# calculate the means for each quartile for points\nquartile_means &lt;- renewable_energy_data %&gt;%\n  group_by(quartile) %&gt;%\n  summarize(mean_Value = mean(Value, na.rm = TRUE), \n            mean_GDP = mean(GDP, na.rm = TRUE))\n\n# create the scatter plot\nggplot(renewable_energy_data, aes(x = Value, y = log(GDP), color = as.factor(quartile))) +\n  geom_point(alpha = 0.7) +  # Original data points\n  geom_point(data = quartile_means, aes(x = mean_Value, y = log(mean_GDP), color = as.factor(quartile)),\n             size = 5, shape = 17) +  # add mean points\n  scale_color_manual(values = c(\"#FFB6C1\", \"blue\", \"yellow\", \"red\")) +\n  labs(title = \"Renewable Energy Consumption vs GDP\",\n       x = \"Renewable Energy Consumption (% of Total Energy Consumption)\",\n       y = \"Log of GDP (current US$)\",\n       color = \"Quartile mean\") +\n  theme_minimal()\n\n\n\n\nThe plot shows that countries in the least economically free quartile, as classified by their economic freedom scores, typically have lower GDP levels and higher rates of renewable energy consumption compared to other quartiles. This relationship is visually highlighted by the concentration of red points in the bottom right section of the plot, representing the least economically free countries. These points clearly cluster together, indicating a potential trend where these countries, despite having lower GDP, prioritize or utilize renewable energy sources to a greater extent than their more economically free counterparts. To further understand the significance of this relationship, advanced spatial or statistical analysis techniques could be employed. However, the primary purpose of this plot is to provide a visual representation of the relationship between economic freedom, GDP, and renewable energy consumption, serving as a preliminary step in a more in-depth statistical process.\nWhile this blog will not go further into the likelihood of omitted variable bias, there is evidence to suggest that it is influencing the model in a significant way."
  },
  {
    "objectID": "blog/2023-12-1-stats-proj/index.html#conclusion",
    "href": "blog/2023-12-1-stats-proj/index.html#conclusion",
    "title": "Exploring the Dynamic Between Economic Freedom and Environmental Outcomes",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the relationship between economic freedom and environmental outcomes has yielded a mix of insights and does not provide a definitive answer. The evidence gathered does not conclusively suggest that economic freedom, on a global scale, has a direct and uniform impact on environmental outcomes. This points to the variable nature of these relationships, influenced by a multitude of factors beyond the scope of economic freedom alone. The necessity to look deeper into the potential omitted variable bias, especially in the context of renewable energy use, emerged as a critical aspect of this study. The presence of numerous missing data points in the data sets further complicates the analysis, indicating that a more complete and comprehensive dataset might yield more definitive conclusions. Moving forward, future research could focus on examining national economic situations more closely, identifying region-specific systems and strategies that can effectively address climate outcomes in accordance with their unique economic frameworks. The pursuit of a one-size-fits-all solution to climate change at a global level appears elusive at this stage. The question remains open as to whether free markets or government-led initiatives are more effective in driving environmental solutions.\nIn light of these findings, it becomes clear that addressing the climate crisis requires proactive leadership from governments worldwide. While the role of market mechanisms and economic freedom cannot be discounted, the urgency of the situation calls for governments to lead the way in implementing robust and forward-thinking environmental policies. Furthermore, the responsibility also falls on individuals to overcome the collective action dilemma that is the climate crisis. The challenge lies in placing the well-being of our planet and future generations above immediate personal interests and convenience. The journey towards a more sustainable future is a collective effort, demanding commitment and action from both governmental entities and consumers alike. The results of this investigation underscore the complexity of the climate crisis and the multifaceted approach needed to combat it, blending economic considerations with environmental imperatives for the betterment of humanity."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "🚀\nHello! My name is Maxwell Patterson, and I’m a Master’s student in Environmental Data Science at the Bren School of Environmental Science & Management, University of California, Santa Barbara (UCSB). I’m passionate about using data-driven approaches to tackle environmental challenges and contribute to a more sustainable future.\n\n\n🌲\nMy journey in environmental data science is fueled by my strong interest in climate communication, data visualization, and machine learning. I believe that effective communication is crucial in driving positive change, and I’m dedicated to honing my skills in presenting complex environmental data in a clear, engaging, and impactful way. Through my coursework and projects at UCSB, I’m exploring various techniques in data visualization and learning how to create compelling narratives that resonate with diverse audiences. Moreover, I’m fascinated by the potential of machine learning in solving environmental problems. From predictive modeling to pattern recognition, I’m eager to apply these cutting-edge tools to uncover valuable insights and develop innovative solutions. By combining my expertise in data science with my passion for environmental sustainability, I aim to make a meaningful impact and contribute to a greener future.\n\n\n👨‍🍳\nBeyond my academic pursuits, I have a deep love for cooking. In fact, I took my passion to the next level by attending culinary school at the prestigious Le Cordon Bleu, where I earned a French Cuisine Certificate SCQF Level 6. Through this experience, I honed my culinary skills, learned classic French techniques, and developed a greater appreciation for the art of cooking. I find joy in experimenting with different flavors, creating delightful dishes, and sharing my culinary creations with others.\n\n\n🔢🤽‍♂️\nI hold a Bachelor of Arts in Applied Mathematics-Economics from Brown University, where I also had the privilege of being a member of the varsity water polo team. As a four-time Academic All-American and the 2022 Team Captain, I learned valuable lessons in leadership, teamwork, and perseverance, which I now apply to my academic and professional endeavors.\n\n\n🗺️\nThrough this blog, I aim to share my experiences, insights, and projects related to environmental data science, climate communication, and beyond. I’m excited to connect with like-minded individuals, exchange ideas, and contribute to the growing community of data-driven environmentalists. Join me on this journey as we explore the power of data in shaping a more sustainable world!"
  }
]