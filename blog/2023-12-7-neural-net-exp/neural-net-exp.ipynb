{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "title: Expressivity of Neural Networks\n",
    "author: Maxwell Patterson \n",
    "date: 11 May 2023\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The rise of neural networks as a tool has led to many technological advancements that have added significant value to society. Things such as voice and image recognition, medical diagnoses, and targeted marketing are a few examples of concepts that have seen significant improvements in performance and applica- tion from the application of neural networks. AI and deep learning unlock a whole new realm of what is possible within mathematics, as the computer is able to learn to differentiate among different representations in data that can reveal important trends and observations from some data set. In order to accomplish this, neural networks calculate, with the use of input and output data, some sort of pattern that can be applied to these situations such as voice recognition and medical diagnoses. In this project, we are tasked with exploring the na- ture in which neural networks can be applied to the approximation of functions. Overall goals of the project include developing an understanding of the training process dynamics, the ways in which the depth and width of the neural network influence the approximation, challenges and takeaways from this investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "While modern neural networks can have millions of layers, the first neural net- work had, naturally, only one layer. In July of 1958, Frank Rosenblatt revealed his prized perceptron, which set the stage for which modern day AI was built upon. Rosenblatt coined the perceptron as being the ”first machine which is capable of having an original idea” (Cornell). The perceptron is a type of single-layer neural network that is inspired from the collaborative nature of how neurons work in the brain. It operated by labeling inputs in two ways, such as left or right, or man or woman. In case of an incorrect prediction, the al- gorithm adjusts itself to improve the accuracy of future predictions. After the completion of thousands or millions of iterations, the neural networks gets more precise over time in order to obtain some valuable result. Weights and bias play a critical role in establishing the relationship between inputs and outputs in the context of perceptrons. A perceptron computes the weighted sum of the input features, \n",
    "$$\n",
    "f(x) = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b = 0\n",
    "$$\n",
    "\n",
    "x1, x2, ..., xn represent the inputs, w1, w2, ..., wn represent the weights associated with these inputs, and b is the bias term. The weights and bias are the keys that drive the neural network, as they are flexible controls that establish the model and allow for optimal relationships to be understood during the learning process. Bias allows for the offsetting of any constant in the data, which enables the perceptron to generate more accurate result.\n",
    "\n",
    "\n",
    "The result that the perceptron generates, known as the decision boundary, is calculated using the input data, the associated weights in the network, and the bias term. This is done through the linear combination of these factors. The decision boundary can be defined as the set of points x such that\n",
    "$$\n",
    "f(x) = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b = 0\n",
    "$$\n",
    "\n",
    "In this case, the sign of the function f(x) determines the significance of the output. For example, a positive value could be associated to the left direction and a negative value associated to the right direction. The flexibility of the per- ceptron is dictated by the weights and biases it possesses. The model is able to learn which input features are more important than others by assigning proper weight values to each input. Coupled with the bias term allow for certain con- stant to be offset in the data, the perceptron is able to pick up certain trends in the input data. Ultimately, the perceptron’s power lies in its ability to find an adequate decision boundary through the manipulation of weights and biases, highlighting how important these concepts are in the process of neural networks.\n",
    "\n",
    "### Activation Function In Play\n",
    "\n",
    "Activation functions allow for neural networks to learn and understand compli- cated patterns between inputs and outputs by presenting the possibility of non- linearity into the network. Non-linear networks are much more powerful than linear networks as they are able to capture much deeper and more profound correlations in data. There are many usable activation functions today: the Sigmoid function, the tanh function, Reduced Linear Unit function, or ReLU, and LeakyReLU, and Exponential Linear Unit function, or ELU to name a few. The choice of activation function depends on the specific architecture of the network. It will be looked into later in the project as to how each of these activation functions is best suited for certain types of situations.\n",
    "Sigmoid : a smooth, S-shaped curve that maps input values to values in the range of 0 to 1. Mathematically, the function is defined as\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "While the sigmoid function is sufficiently utilized in the outputs of binary sit- uations, it can struggle when dealing with more hidden layers due to vanishing gradient type of issues. In these erroneous scenarios, the gradient approaches zero or some large, positively or negatively, constant that hinders the learning ability of the network.\n",
    "tanh: smooth like the sigmoid function, but maps input values to the range of -1 to 1 instead of 0 to 1. Mathematically, the function can be defined as\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "The tanh function is effective in that its output is centered around zero, which can help deal with the vanishing gradient issue to a degree but still deals with the issue of blow-up values.\n",
    "\n",
    "*ReLU/LeakyReLU* : a piecewise linear function that returns the input value if it is positive, and zero, or a small negative value associated the input, if the value is negative. Mathematically, the function is defined as\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "ReLU helps deal with the vanishing gradient issue since the gradient is constant for positive input values and cannot spiral in or out. The dying ReLU problem does exist however, in which the neurons deactivate for negative inputs which makes it difficult for the network to learn and adapt over time.\n",
    "\n",
    "*ELU*: variation of the ReLU function that smooths the curve to the left of the x-axis. Mathematically, the function can be defined as\n",
    "$$\n",
    "\\text{ELU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\sigma \\times (e^x - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, σ is some positive constant. ELU helps to address the vanishing gradient issue and upholds a stronger curve.\n",
    "The differences in application and results obtained using these approximation functions will be discussed later.\n",
    "\n",
    "### Backpropagation\n",
    "Backpropagation is an essential tool that is used in the training of neural net- works, especially ones that have a multitude of layers. Backpropagation calcu- lates the loss function’s gradient with respect to the weights and biases of the neural network, which then enables the network to update it’s weights and biases in a productive manner that increases its accuracy. The process of backprop- agation contains a forward and backward direction. In the forward direction, input data is fed into the neural network, which transmits the signal through the network for it to update itself. In the backwards direction, error is calcu- lated by analyzing the difference between predicted and actual outcomes. Then, the error value is used to update the network weights via gradient descent. To accomplish this, a loss-function will be defined that calculates the error value. Since gradient descent is used to minimize the error, its derivative to the weight matrix is obtained and can be multiplied with some positive value, σ, and sub- tracted value to complete one step. σ is also known as the learning rate. It is essential to set an adequate learning rate, as one that is too high will help the model learn faster, but opens the possibility of a failure to converge so that the network does not learn anything. If the learning rate is too low, the train- ing process may take too much time. Mathematically, using W as the weight matrices, this process looks like\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\sigma \\times \\delta E\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
